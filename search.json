[
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Reference",
    "section": "",
    "text": "Python API\n\n\n\n\n\n\n\ninspect_flow\nCore types and functions.\n\n\ninspect_flow.api\nOutput and run a spec.\n\n\n\n\n\nFlow CLI\n\n\n\n\n\n\n\nflow config\nOutput spec as yaml.\n\n\nflow run\nRun a spec.",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "reference/flow_run.html",
    "href": "reference/flow_run.html",
    "title": "flow run",
    "section": "",
    "text": "Run a spec\n\nUsage\nflow run [OPTIONS] CONFIG_FILE\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--dry-run\nboolean\nDo not run spec, but show a count of tasks that would be run.\nFalse\n\n\n--no-venv\nboolean\nIf set run the flow in the current environment without creating a virtual environment.\nFalse\n\n\n--log-dir-create-unique\nboolean\nIf set, create a new log directory by appending an _ and numeric suffix if the specified log_dir already exists. If the directory exists and has a _numeric suffix, that suffix will be incremented. If not set, use the existing log_dir (which must be empty or have log_dir_allow_dirty=True).\nFalse\n\n\n--log-dir\ndirectory\nSet the log directory. Will override the log_dir specified in the config.\nNone\n\n\n--limit\ninteger\nLimit the number of samples to run.\nNone\n\n\n--arg, -A\ntext\nSet arguments that will be passed as kwargs to the function in the flow config. Only used when the last statement in the config file is a function. Examples: --arg task_min_priority=2 If the same key is provided multiple times, later values will override earlier ones.\nNone\n\n\n--set, -s\ntext\nSet config overrides. Examples: --set defaults.solver.args.tool_calls=none --set options.limit=10 --set options.metadata={\"key1\": \"val1\", \"key2\": \"val2\"} The specified value may be a string or json parsable list or dict. If string is provided then it will be appended to existing list values. If json list or dict is provided then it will replace existing values. If the same key is provided multiple times, later values will override earlier ones.\nNone\n\n\n--log-level\nchoice (debug | trace | http | info | warning | error | critical | notset)\nSet the log level (defaults to ‘info’)\ninfo\n\n\n--help\nboolean\nShow this message and exit.\nFalse",
    "crumbs": [
      "Reference",
      "Inspect CLI",
      "flow run"
    ]
  },
  {
    "objectID": "reference/flow_config.html",
    "href": "reference/flow_config.html",
    "title": "flow config",
    "section": "",
    "text": "Output config\n\nUsage\nflow config [OPTIONS] CONFIG_FILE\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--no-venv\nboolean\nIf set run the flow in the current environment without creating a virtual environment.\nFalse\n\n\n--log-dir-create-unique\nboolean\nIf set, create a new log directory by appending an _ and numeric suffix if the specified log_dir already exists. If the directory exists and has a _numeric suffix, that suffix will be incremented. If not set, use the existing log_dir (which must be empty or have log_dir_allow_dirty=True).\nFalse\n\n\n--log-dir\ndirectory\nSet the log directory. Will override the log_dir specified in the config.\nNone\n\n\n--limit\ninteger\nLimit the number of samples to run.\nNone\n\n\n--arg, -A\ntext\nSet arguments that will be passed as kwargs to the function in the flow config. Only used when the last statement in the config file is a function. Examples: --arg task_min_priority=2 If the same key is provided multiple times, later values will override earlier ones.\nNone\n\n\n--set, -s\ntext\nSet config overrides. Examples: --set defaults.solver.args.tool_calls=none --set options.limit=10 --set options.metadata={\"key1\": \"val1\", \"key2\": \"val2\"} The specified value may be a string or json parsable list or dict. If string is provided then it will be appended to existing list values. If json list or dict is provided then it will replace existing values. If the same key is provided multiple times, later values will override earlier ones.\nNone\n\n\n--log-level\nchoice (debug | trace | http | info | warning | error | critical | notset)\nSet the log level (defaults to ‘info’)\ninfo\n\n\n--help\nboolean\nShow this message and exit.\nFalse",
    "crumbs": [
      "Reference",
      "Inspect CLI"
    ]
  },
  {
    "objectID": "advanced.html",
    "href": "advanced.html",
    "title": "Advanced",
    "section": "",
    "text": "Flow supports two types of metadata with distinct purposes: metadata and flow_metadata.\n\n\nThe metadata field in FlowOptions and FlowTask is passed directly to Inspect AI and stored in evaluation logs. Use this for tracking experiment information that should be accessible in Inspect AI’s log viewer and analysis tools.\nExample:\n\n\nconfig.py\n\nfrom inspect_flow import FlowOptions, FlowSpec, FlowTask\n\nFlowSpec(\n    log_dir=\"logs\",\n    options=FlowOptions(\n        metadata={\n            \"experiment\": \"baseline_v1\",\n            \"hypothesis\": \"Higher temperature improves creative tasks\",\n            \"hardware\": \"A100-80GB\",\n        }\n    ),\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n            metadata={\n                \"task_variant\": \"chemistry_subset\",\n                \"note\": \"Testing with reduced context\",\n            },\n        )\n    ],\n)\n\nThe metadata from FlowOptions is applied globally to all tasks in the evaluation run, while task-level metadata is specific to each task. These metadata dictionaries are merged in Inspect AI, with task-level metadata keys overriding the global options.\n\n\n\nThe flow_metadata field is available on FlowSpec, FlowTask, FlowModel, FlowScorer, FlowSolver, and FlowAgent. This metadata is not passed to Inspect AI—it exists only in the Flow configuration and is useful for configuration-time logic and organization.\nUse cases:\n\nFiltering or selecting configurations based on properties\nOrganizing complex configuration generation logic\nDocumenting configuration decisions\nAnnotating configs without polluting Inspect AI logs\n\nExample: Configuration-time filtering\n\n\nconfig.py\n\nfrom inspect_flow import FlowModel, FlowSpec, tasks_matrix\n\n# Define models with metadata about capabilities\nmodels = [\n    FlowModel(name=\"openai/gpt-4o\", flow_metadata={\"context_window\": 128000}),\n    FlowModel(name=\"openai/gpt-4o-mini\", flow_metadata={\"context_window\": 128000}),\n    FlowModel(\n        name=\"anthropic/claude-3-5-sonnet\", flow_metadata={\"context_window\": 200000}\n    ),\n]\n\n# Filter to only long-context models\nlong_context_models = [\n    m for m in models if (m.flow_metadata or {}).get(\"context_window\", 0) &gt;= 128000\n]\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=\"long_context_task\",\n        model=long_context_models,\n    ),\n)",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Advanced"
    ]
  },
  {
    "objectID": "advanced.html#metadata",
    "href": "advanced.html#metadata",
    "title": "Advanced",
    "section": "",
    "text": "Flow supports two types of metadata with distinct purposes: metadata and flow_metadata.\n\n\nThe metadata field in FlowOptions and FlowTask is passed directly to Inspect AI and stored in evaluation logs. Use this for tracking experiment information that should be accessible in Inspect AI’s log viewer and analysis tools.\nExample:\n\n\nconfig.py\n\nfrom inspect_flow import FlowOptions, FlowSpec, FlowTask\n\nFlowSpec(\n    log_dir=\"logs\",\n    options=FlowOptions(\n        metadata={\n            \"experiment\": \"baseline_v1\",\n            \"hypothesis\": \"Higher temperature improves creative tasks\",\n            \"hardware\": \"A100-80GB\",\n        }\n    ),\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n            metadata={\n                \"task_variant\": \"chemistry_subset\",\n                \"note\": \"Testing with reduced context\",\n            },\n        )\n    ],\n)\n\nThe metadata from FlowOptions is applied globally to all tasks in the evaluation run, while task-level metadata is specific to each task. These metadata dictionaries are merged in Inspect AI, with task-level metadata keys overriding the global options.\n\n\n\nThe flow_metadata field is available on FlowSpec, FlowTask, FlowModel, FlowScorer, FlowSolver, and FlowAgent. This metadata is not passed to Inspect AI—it exists only in the Flow configuration and is useful for configuration-time logic and organization.\nUse cases:\n\nFiltering or selecting configurations based on properties\nOrganizing complex configuration generation logic\nDocumenting configuration decisions\nAnnotating configs without polluting Inspect AI logs\n\nExample: Configuration-time filtering\n\n\nconfig.py\n\nfrom inspect_flow import FlowModel, FlowSpec, tasks_matrix\n\n# Define models with metadata about capabilities\nmodels = [\n    FlowModel(name=\"openai/gpt-4o\", flow_metadata={\"context_window\": 128000}),\n    FlowModel(name=\"openai/gpt-4o-mini\", flow_metadata={\"context_window\": 128000}),\n    FlowModel(\n        name=\"anthropic/claude-3-5-sonnet\", flow_metadata={\"context_window\": 200000}\n    ),\n]\n\n# Filter to only long-context models\nlong_context_models = [\n    m for m in models if (m.flow_metadata or {}).get(\"context_window\", 0) &gt;= 128000\n]\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=\"long_context_task\",\n        model=long_context_models,\n    ),\n)",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Advanced"
    ]
  },
  {
    "objectID": "advanced.html#parameterized-jobs",
    "href": "advanced.html#parameterized-jobs",
    "title": "Advanced",
    "section": "Parameterized Jobs",
    "text": "Parameterized Jobs\n\nFlow Args (--arg)\nPass custom variables to Python config files using --arg or the INSPECT_FLOW_ARG environment variable. Use this for dynamic configuration that isn’t available via --set. To access the args the last statement in the config file should be a function that returns a FlowSpec. This function will be called with any provided args:\nflow run config.py --arg task_min_priority=2\n\n\nconfig.py\n\nfrom inspect_flow import FlowSpec, FlowTask\n\nall_tasks = [\n    FlowTask(name=\"task_easy\", flow_metadata={\"priority\": 1}),\n    FlowTask(name=\"task_medium\", flow_metadata={\"priority\": 2}),\n    FlowTask(name=\"task_hard\", flow_metadata={\"priority\": 3}),\n]\n\n\ndef spec(task_min_priority: int = 1) -&gt; FlowSpec:\n    return FlowSpec(\n        log_dir=\"logs\",\n        tasks=[\n            t\n            for t in all_tasks\n            if (t.flow_metadata or {}).get(\"priority\", 0) &gt;= task_min_priority\n        ],\n    )\n\n\n\nTemplate Substitution\nUse {field_name} syntax to reference other FlowSpec configuration values. Substitutions are applied after the config is loaded:\nFlowSpec(\n    log_dir=\"logs/my_eval\",\n    options=FlowOptions(bundle_dir=\"{log_dir}/bundle\"),\n    # Result: bundle_dir=\"logs/my_eval/bundle\"\n)\nFor nested fields, use bracket notation: {options[eval_set_id]} or {flow_metadata[key]}. Substitutions are resolved recursively until no more remain.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Advanced"
    ]
  },
  {
    "objectID": "advanced.html#viewer-bundling",
    "href": "advanced.html#viewer-bundling",
    "title": "Advanced",
    "section": "Viewer Bundling",
    "text": "Viewer Bundling\nViewer bundling works the same way as eval_set() in Inspect AI and is configurable via FlowOptions. An additional feature allows you to print bundle URLs for users running evaluations.\nConvert local bundle paths to public URLs for sharing evaluation results. The bundle_url_mappings in FlowOptions applies string replacements to bundle_dir to generate a shareable URL that’s printed to stdout after the evaluation completes.\n\n\nconfig.py\n\nfrom inspect_flow import FlowOptions, FlowSpec, FlowTask\n\nFlowSpec(\n    log_dir=\"logs/my_eval\",\n    options=FlowOptions(\n        bundle_dir=\"s3://my-bucket/bundles/my_eval\",\n        bundle_url_mappings={\"s3://my-bucket\": \"https://my-bucket.s3.amazonaws.com\"},\n    ),\n    tasks=[FlowTask(name=\"task\", model=\"openai/gpt-4o\")],\n)\n\nAfter running this prints: Bundle URL: https://my-bucket.s3.amazonaws.com/bundles/my_eval\nUse this when storing bundles on cloud storage like S3 or on servers with public HTTP access. Multiple mappings are applied in order.\nUsing Bundle URL maps makes sense along with the spec inheritance feature so you can configure the same bundle mapping for all configs in a repository.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Advanced"
    ]
  },
  {
    "objectID": "advanced.html#config-scripts",
    "href": "advanced.html#config-scripts",
    "title": "Advanced",
    "section": "Config Scripts",
    "text": "Config Scripts\nWhen loading a configuration file, Flow expects the last expression to either be a FlowSpec or a function that returns a FlowSpec. Other than this requirement, the configuration file may execute arbitrary code.\n\nafter_load\nConfiguration scripts are executed while loading the spec. At the time that the script is running the spec is in an intermediate state (includes may not have been processed, overrides not applied, and template substitutions will not have run). To run code after the spec is fully loaded a script can decorate a function with @after_load.\nThe decorated function may optionally implement the following arguments:\n\nspec - the fully loaded FlowSpec\nfiles - the list of configuration files that were loaded\n\nOne example of functionality that can be implemented using this feature is validation code to enforce constraints. Instead of repeating this validation code in every Flow configuration file, the code could be placed in a _flow.py file that is auto included.\n\n\nPrevent Runs with Uncommitted Changes\nPlace a _flow.py file at your repository root to validate that all configs are in clean git repositories. This validation runs automatically for all configs in subdirectories.\n\n\n_flow.py\n\nimport subprocess\nfrom pathlib import Path\n\nfrom inspect_flow import after_load\n\n\ndef check_repo(path: str) -&gt; None:\n    abs_path = Path(path).resolve()\n    check_dir = abs_path if abs_path.is_dir() else abs_path.parent\n\n    result = subprocess.run(\n        [\"git\", \"status\", \"--porcelain\"],\n        cwd=check_dir,\n        capture_output=True,\n        text=True,\n        check=True,\n    )\n\n    if result.stdout.strip():\n        raise RuntimeError(f\"The repository at {check_dir} has uncommitted changes.\")\n\n\n@after_load\ndef validate_no_dirty_repo(files: list[str]) -&gt; None:\n    # Check no config files are in a dirty git repo\n    for path in files:\n        check_repo(path)\n\n\n\nconfig.py\n\n# Automatically inherits _flow.py\nfrom inspect_flow import FlowSpec, FlowTask\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=[FlowTask(name=\"inspect_evals/gpqa_diamond\", model=\"openai/gpt-4o\")],\n)\n# Will fail if uncommitted changes exist in the repository\n\n\n\nLock Config Fields\nA _flow.py file can prevent configs from overriding critical settings:\n\n\n_flow.py\n\nfrom inspect_flow import FlowOptions, FlowSpec, after_load\n\nMAX_SAMPLES = 16\n\n\n@after_load\ndef validate_max_samples(spec: FlowSpec) -&gt; None:\n    if not spec.options or not spec.options.max_samples == MAX_SAMPLES:\n        raise ValueError(\"Do not override max_samples!\")\n\n\nFlowSpec(\n    options=FlowOptions(max_samples=MAX_SAMPLES),\n)\n\n\n\nconfig.py\n\n# Automatically inherits _flow.py\nfrom inspect_flow import FlowOptions, FlowSpec, FlowTask\n\nFlowSpec(\n    log_dir=\"logs\",\n    options=FlowOptions(max_samples=32),  # Will raise ValueError!\n    tasks=[FlowTask(name=\"inspect_evals/gpqa_diamond\", model=\"openai/gpt-4o\")],\n)\n\nThis pattern is useful for enforcing organizational standards (resource limits, safety constraints, etc.) across all evaluation configs in a repository.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Advanced"
    ]
  },
  {
    "objectID": "dependencies.html",
    "href": "dependencies.html",
    "title": "Dependencies",
    "section": "",
    "text": "Inspect Flow automatically creates isolated virtual environments for each workflow run, ensuring repeatability and avoiding dependency conflicts with your system Python environment. Because each run starts with a fresh environment, dependencies must be resolved—either automatically detected from your project files or explicitly specified in your configuration.\nWhen you run flow run config.py, Flow:",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Dependencies"
    ]
  },
  {
    "objectID": "dependencies.html#automatic-dependency-discovery",
    "href": "dependencies.html#automatic-dependency-discovery",
    "title": "Dependencies",
    "section": "Automatic Dependency Discovery",
    "text": "Automatic Dependency Discovery\nBy default, Flow automatically discovers and installs dependencies without requiring explicit configuration:\n\nDependency files: Searches upward from your config file directory for pyproject.toml or requirements.txt. Relative paths will be resolved relative to the config file (when using the CLI) or base_dir arg (when using the API)\nPackage inference: Detects packages from object names in your config:\n\nModels: model=\"openai/gpt-4\" → installs openai\nTasks: name=\"inspect_evals/mmlu\" → installs inspect-evals\n\nConfig inheritance: Automatically includes any _flow.py files in the config directory or parent directories (see Inheritance). These files can specify default dependencies that apply to all including configs.\n\nThis means most workflows require no dependency configuration at all!",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Dependencies"
    ]
  },
  {
    "objectID": "dependencies.html#explicit-dependencies",
    "href": "dependencies.html#explicit-dependencies",
    "title": "Dependencies",
    "section": "Explicit Dependencies",
    "text": "Explicit Dependencies\nWhile automatic dependency discovery works for most cases, you may need explicit dependencies when you require specific package versions for reproducibility e.g. openai==2.8.0, need to specify a non-standard dependency file location, or need to override the automatic detection behavior. Explicit dependencies give you full control over what gets installed in your workflow’s virtual environment.\nThe dependencies field in FlowSpec accepts a FlowDependencies object:\n\n\nconfig.py\n\nfrom inspect_flow import FlowDependencies, FlowSpec, FlowTask\n\nFlowSpec(\n    dependencies=FlowDependencies(\n1        dependency_file=\"../foo/pyproject.toml\",\n2        additional_dependencies=[\"pandas==2.0.0\"],\n3        auto_detect_dependencies=True,\n    ),\n    log_dir=\"logs\",\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-5\",\n        ),\n    ],\n)\n\n\n1\n\nHow to find dependency files: Defaults to \"auto\" which auto-detects a requirements.txt or a pyproject.toml file. May also be set to a path to a dependency file or \"no_file\" to not use a dependency file. When using pyproject.toml, if a uv.lock file exists in the same directory, it will be used automatically for reproducible installs.\n\n2\n\nExtra packages beyond the dependency file. Accepts a string or list of strings. Supports: PyPI packages, Git repositories, local packages.\n\n3\n\nAuto-install packages based on task and model names in the config (default: True). For example, model=\"openai/gpt-4\" installs openai, and name=\"inspect_evals/mmlu\" installs inspect-evals.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Dependencies"
    ]
  },
  {
    "objectID": "dependencies.html#python-version",
    "href": "dependencies.html#python-version",
    "title": "Dependencies",
    "section": "Python Version",
    "text": "Python Version\nSpecify the Python version for your spec’s virtual environment:\n\n\nconfig.py\n\nfrom inspect_flow import FlowSpec, FlowTask\n\nFlowSpec(\n    python_version=\"3.11\",\n    log_dir=\"logs\",\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-5\",\n        ),\n    ],\n)",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Dependencies"
    ]
  },
  {
    "objectID": "dependencies.html#checking-config",
    "href": "dependencies.html#checking-config",
    "title": "Dependencies",
    "section": "Checking Config",
    "text": "Checking Config\nTo verify which dependencies and Python version will be used:\nflow run config.py --dry-run\n\n\n\n\n\n\nTipRepeatability Best Practices\n\n\n\nFor repeatable workflows:\n\nPin PyPI package versions: \"inspect-evals==0.3.15\"\nPin Git commits: \"git+https://github.com/user/repo@commit_hash\"\nSpecify python_version explicitly\nUse uv.lock with pyproject.toml for locked dependencies (automatically detected if present)",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Dependencies"
    ]
  },
  {
    "objectID": "flow_concepts.html",
    "href": "flow_concepts.html",
    "title": "Flow Concepts",
    "section": "",
    "text": "The primary interface for defining a workflow in Flow is a FlowSpec. A FlowSpec defines the log directory, tasks, models, and other options that make up the spec. When Inspect Flow processes the FlowSpec, it resolves the configuration into a list of evaluation tasks to be run. These tasks are provided to Inspect’s eval_set() function along with any additional configuration, and Inspect executes the tasks.\nInspect Flow mirrors Inspect AI’s object model with corresponding Flow types:",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Flow Concepts"
    ]
  },
  {
    "objectID": "flow_concepts.html#config-files",
    "href": "flow_concepts.html#config-files",
    "title": "Flow Concepts",
    "section": "Config Files",
    "text": "Config Files\nFlow configuration files are Python files where the last expression is either:\n\nA FlowSpec object, or\nA function that returns a FlowSpec object\n\nSince config files are standard Python code, you can use all Python features including variables, loops, comprehensions, imports, and comments. Flow configs are just Python!",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Flow Concepts"
    ]
  },
  {
    "objectID": "flow_concepts.html#basic-example",
    "href": "flow_concepts.html#basic-example",
    "title": "Flow Concepts",
    "section": "Basic Example",
    "text": "Basic Example\nIn Welcome - Basic Example, we showed a simple FlowSpec. Let’s break down what’s happening:\n\n\nconfig.py\n\nfrom inspect_flow import FlowSpec, FlowTask\n\nFlowSpec(\n1    log_dir=\"logs\",\n2    tasks=[\n        FlowTask(\n3            name=\"inspect_evals/gpqa_diamond\",\n4            model=\"openai/gpt-5\",\n        ),\n    ],\n)\n\n\n1\n\nSpecify the directory for storing logs\n\n2\n\nList evaluation tasks to run\n\n3\n\nSpecify task from registry by name\n\n4\n\nSpecify model to evaluate by name\n\n\nTo run the task, run the following command in your shell.\nflow run config.py\n\n\n\nProgress bar in terminal\n\n\nWhat happens when you run this?\n\nFlow creates an isolated virtual environment\nInstalls inspect-evals and openai (auto-detected)\nLoads the gpqa_diamond task from the inspect_evals registry\nRuns the evaluation with GPT-5\nStores results in logs/",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Flow Concepts"
    ]
  },
  {
    "objectID": "flow_concepts.html#jobs",
    "href": "flow_concepts.html#jobs",
    "title": "Flow Concepts",
    "section": "Jobs",
    "text": "Jobs\nFlowSpec is the primary interface for defining Flow workflows. All Flow operations—including parameter sweeps and matrix expansions—ultimately produce a list of tasks that FlowSpec executes.\nRequired fields:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nlog_dir\nOutput path for logging results. Supports S3 paths (e.g., s3://bucket/path)\n\n\n\nOptional fields:\n\n\n\n\n\n\n\n\nField\nDescription\nDefault\n\n\n\n\ntasks\nList of tasks to run (FlowTask objects or strings)\nNone\n\n\nincludes\nList of other flow configs to include (paths as strings). Relative paths resolved relative to config file (CLI) or base_dir arg (API). Additionally, any _flow.py files in the same directory or parent directories are automatically included\nNone\n\n\nlog_dir_create_unique\nIf True, append numeric suffix to log_dir if it exists. If False, use existing directory (must only contain logs from tasks in the spec or have options.log_dir_allow_dirty=True)\nFalse\n\n\npython_version\nPython version for the isolated virtual environment (e.g., \"3.11\")\nSame as current environment\n\n\ndependencies\nFlowDependencies object configuring package installation. Controls dependency file detection, additional packages, and auto-detection behavior\nAuto-detect from pyproject.toml or requirements.txt, and config object names\n\n\nenv\nEnvironment variables to set when running tasks\nNone\n\n\noptions\nRuntime options passed to eval_set (see FlowOptions reference)\nNone\n\n\ndefaults\nDefault values applied across tasks, models, solvers, and agents (see Defaults and FlowDefaults reference)\nNone\n\n\nflow_metadata\nMetadata stored in the flow config (not passed to Inspect AI, see flow_metadata)\nNone",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Flow Concepts"
    ]
  },
  {
    "objectID": "flow_concepts.html#tasks",
    "href": "flow_concepts.html#tasks",
    "title": "Flow Concepts",
    "section": "Tasks",
    "text": "Tasks\nFlowTask defines the configuration for a single evaluation. Each FlowTask maps to an Inspect AI Task and accepts the same parameters—including the model to evaluate, solver chain, generation config, sandbox environment, and dataset filters.\n\nSpecification\nIn the example above, we used a registry name (\"inspect_evals/gpqa_diamond\"). Flow supports multiple ways to reference tasks:\nRegistry Name\n# Tasks from installed packages\nFlowTask(\n    name=\"inspect_evals/gpqa_diamond\",\n    model=\"openai/gpt-5\"\n)\nFile Path\n# Auto-discovers `@task` decorated functions in the specified file and creates a task for each of them\nFlowTask(\n    name=\"./my_task.py\",\n    model=\"openai/gpt-5\"\n)\nFile with Function\n# Explicitly selects a specific function from the file\nFlowTask(\n    name=\"./my_task.py@custom_eval\",\n    model=\"openai/gpt-5\"\n)\n\n\nConfiguration\nFlowTask accepts parameters that map to Inspect AI Task fields. The examples below show commonly used fields; see the FlowTask reference documentation for the complete list of available parameters.\nFlowTask(\n1    name=\"inspect_evals/mmlu_0_shot\",\n2    model=\"openai/gpt-5\",\n3    epochs=3,\n4    config=GenerateConfig(\n        temperature=0.7,\n        max_tokens=1000,\n    ),\n5    solver=\"chain_of_thought\",\n6    args={\"subject\": \"physics\"},\n7    sandbox=\"docker\",\n8    sample_id=[0, 1, 2],\n)\n\n1\n\nTask name — Maps to Inspect AI Task.name. Can be a registry name (\"inspect_evals/mmlu\"), file path (\"./task.py\"), or file with function (\"./task.py@eval_fn\").\n\n2\n\nModel — Maps to Inspect AI Task.model. Optional model for this task. If not specified, uses the model from INSPECT_EVAL_MODEL environment variable. Can be a string (\"openai/gpt-5\") or a FlowModel object for advanced configuration.\n\n3\n\nEpochs — Maps to Inspect AI Task.epochs. Number of times to repeat evaluation over the dataset samples. Can be an integer (epochs=3) or a FlowEpochs object to specify custom reducer functions (FlowEpochs(epochs=3, reducer=\"median\")). By default, scores are combined using the \"mean\" reducer across epochs.\n\n4\n\nGeneration config — Maps to Inspect AI Task.config (GenerateConfig). Model generation parameters like temperature, max_tokens, top_p, reasoning_effort, etc. These settings override config on FlowSpec.config but are overridden by settings on FlowModel.config.\n\n5\n\nSolver chain — Maps to Inspect AI Task.solver. The algorithm(s) for solving the task. Can be a string (\"chain_of_thought\"), FlowSolver object, FlowAgent object, or a list of solvers for chaining. Defaults to generate() if not specified.\n\n6\n\nTask arguments — Maps to task function parameters. Dictionary of arguments passed to the task constructor or @task decorated function. Enables parameterization of tasks (e.g., selecting dataset subsets, configuring difficulty levels).\n\n7\n\nSandbox environment — Maps to Inspect AI Task.sandbox. Can be a string (\"docker\", \"local\"), a tuple with additional config, or a SandboxEnvironmentType object.\n\n8\n\nSample selection — Evaluate specific samples from the dataset. Accepts a single ID (sample_id=0), list of IDs (sample_id=[0, 1, 2]), or list of string IDs.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Flow Concepts"
    ]
  },
  {
    "objectID": "flow_concepts.html#models",
    "href": "flow_concepts.html#models",
    "title": "Flow Concepts",
    "section": "Models",
    "text": "Models\nWhen specifying a task, you can provide the model for the task as either a string with a model name or as a FlowModel. When using the simple string, the default values for that model will be used. For example:\nFlowTask(name=\"task\", model=\"openai/gpt-5\")\nUsing FlowModel allows you to specify additional parameters for the model (e.g. GenerateConfig). Use FlowModel when you need to set custom GenerateConfig, API endpoints, custom API keys for a specific model, or perform other custom configuration. For example:\nFlowTask(\n    name=\"task\",\n    model=FlowModel(\n        name=\"openai/gpt-5\",\n        config=GenerateConfig(\n            reasoning_effort=\"medium\",\n            max_connections=10,\n        ),\n        base_url=\"https://custom-endpoint.com\",\n        api_key=\"${CUSTOM_API_KEY}\",\n    )\n)\n\nModel Roles\nFor agent evaluations with multiple model roles, use model_roles to specify which model to use for each role. For example:\n# For agent evaluations with multiple roles\nFlowTask(\n    name=\"multi_agent_task\",\n    model_roles={\n        \"assistant\": \"openai/gpt-5\",\n        \"critic\": \"anthropic/claude-3-5-sonnet\",\n    }\n)",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Flow Concepts"
    ]
  },
  {
    "objectID": "matrix.html",
    "href": "matrix.html",
    "title": "Matrixing",
    "section": "",
    "text": "Matrixing lets you systematically explore evaluation configurations by generating Cartesian products of parameters. Instead of manually writing every combination, Flow provides *_matrix() and *_with() functions to declaratively generate evaluation grids.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Matrixing"
    ]
  },
  {
    "objectID": "matrix.html#matrix-functions",
    "href": "matrix.html#matrix-functions",
    "title": "Matrixing",
    "section": "Matrix Functions",
    "text": "Matrix Functions\nMatrix functions generate all combinations of their parameters using Cartesian products.\n\ntasks_matrix()\nGenerate task configurations by combining tasks with models, configs, solvers, and arguments:\n\n\ntasks_matrix.py\n\nfrom inspect_flow import FlowSpec, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=[\"inspect_evals/gpqa_diamond\", \"inspect_evals/mmlu_0_shot\"],\n        model=[\"openai/gpt-4o\", \"anthropic/claude-3-5-sonnet\"],\n    ),\n)\n\nThis creates 4 tasks (2 tasks × 2 models).\n\n\nmodels_matrix()\nGenerate model configurations with different generation settings:\n\n\nmodels_matrix.py\n\nfrom inspect_ai.model import GenerateConfig\nfrom inspect_flow import FlowSpec, models_matrix, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=models_matrix(\n            model=[\n                \"openai/gpt-5\",\n                \"openai/gpt-5-mini\",\n            ],\n            config=[\n                GenerateConfig(reasoning_effort=\"minimal\"),\n                GenerateConfig(reasoning_effort=\"low\"),\n                GenerateConfig(reasoning_effort=\"medium\"),\n                GenerateConfig(reasoning_effort=\"high\"),\n            ],\n        ),\n    ),\n)\n\nThis creates 16 tasks (2 task × 2 models × 4 resoning_effort).\n\n\nconfigs_matrix()\nGenerate generation config combinations by specifying individual parameters:\n\n\nconfigs_matrix.py\n\nfrom inspect_flow import FlowSpec, configs_matrix, models_matrix, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=models_matrix(\n            model=[\n                \"openai/gpt-5\",\n                \"openai/gpt-5-mini\",\n            ],\n            config=configs_matrix(\n                reasoning_effort=[\"minimal\", \"low\", \"medium\", \"high\"],\n            ),\n        ),\n    ),\n)\n\nThis creates 16 tasks (2 task × 2 models × 4 resoning_effort).\n\n\nsolvers_matrix()\nGenerate solver configurations with different arguments:\n\n\nsolvers_matrix.py\n\nfrom inspect_flow import FlowSpec, solvers_matrix, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=\"my_task\",\n        solver=solvers_matrix(\n            solver=\"chain_of_thought\",\n            args=[\n                {\"max_iterations\": 3},\n                {\"max_iterations\": 5},\n                {\"max_iterations\": 10},\n            ],\n        ),\n    ),\n)\n\nThis creates 3 tasks (1 task × 3 solver configurations).\n\n\nagents_matrix()\nGenerate agent configurations with different arguments:\n\n\nagents_matrix.py\n\nfrom inspect_flow import FlowSpec, agents_matrix, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=\"my_task\",\n        solver=agents_matrix(\n            agent=\"system_message\",\n            args=[\n                {\"message\": \"You are a helpful assistant.\"},\n                {\"message\": \"You are a creative writer.\"},\n                {\"message\": \"You are a technical expert.\"},\n            ],\n        ),\n    ),\n)\n\nThis creates 3 tasks (1 task × 3 agent configurations).",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Matrixing"
    ]
  },
  {
    "objectID": "matrix.html#with-functions-apply-to-all",
    "href": "matrix.html#with-functions-apply-to-all",
    "title": "Matrixing",
    "section": "With Functions (Apply to All)",
    "text": "With Functions (Apply to All)\n“With” functions apply the same setting to all items in a list, without creating a Cartesian product. Unlike matrix functions which multiply combinations, with functions keep the list size the same.\nKey difference:\n\nMatrix functions create all combinations: models_matrix(model=[A, B], temperature=[0.5, 1.0]) → 4 tasks (A at 0.5, A at 1.0, B at 0.5, B at 1.0)\nWith functions apply to each item: models_with(model=[A, B], temperature=0.5) → 2 tasks (A at 0.5, B at 0.5)\n\n\ntasks_with()\nApply common settings to multiple tasks:\n\n\ntasks_with.py\n\nfrom inspect_ai.model import GenerateConfig\nfrom inspect_flow import FlowSpec, tasks_with\n\nFlowSpec(\n    tasks=tasks_with(\n        task=[\"inspect_evals/gpqa_diamond\", \"inspect_evals/mmlu_0_shot\"],\n1        model=\"openai/gpt-4o\",\n2        config=GenerateConfig(temperature=0.7),\n    )\n)\n\n\n1\n\nApply the same model to both tasks\n\n2\n\nApply the same generation config to both tasks\n\n\nThis creates 2 tasks (2 tasks, each with the same model and config).\n\n\nmodels_with()\nApply common settings to multiple models:\n\n\nmodels_with.py\n\nfrom inspect_ai.model import GenerateConfig\nfrom inspect_flow import FlowSpec, models_with, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=\"my_task\",\n        model=models_with(\n            model=[\"openai/gpt-4o\", \"anthropic/claude-3-5-sonnet-20241022\"],\n1            config=GenerateConfig(temperature=0.7),\n        ),\n    ),\n)\n\n\n1\n\nApply the same generation config to both models\n\n\nThis creates 2 tasks (1 task × 2 models, each with the same config).\n\n\nconfigs_with()\nApply common settings to multiple configs:\n\n\nconfigs_with.py\n\nfrom inspect_ai.model import GenerateConfig\nfrom inspect_flow import FlowSpec, configs_with, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=\"my_task\",\n        config=configs_with(\n            config=[\n                GenerateConfig(temperature=0.0),\n                GenerateConfig(temperature=0.5),\n                GenerateConfig(temperature=1.0),\n            ],\n1            max_tokens=1000,\n        ),\n    ),\n)\n\n\n1\n\nApply the same max_tokens to all three temperature configs\n\n\nThis creates 3 tasks (1 task × 3 configs, each with the same max_tokens).\n\n\nsolvers_with()\nApply common settings to multiple solvers:\n\n\nsolvers_with.py\n\nfrom inspect_flow import FlowSpec, solvers_with, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=\"my_task\",\n        solver=solvers_with(\n            solver=[\"chain_of_thought\", \"plan_solve\", \"self_critique\"],\n            args={\"max_steps\": 5},\n        ),\n    ),\n)\n\nThis creates 3 tasks (1 task × 3 solvers, each with the same max_attempts).\n\n\nagents_with()\nApply common settings to multiple agents:\n\n\nagents_with.py\n\nfrom inspect_flow import FlowSpec, agents_with, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=\"my_task\",\n        solver=agents_with(\n            agent=[\"system_message\", \"tool_agent\", \"web_agent\"],\n            args={\"system_message\": \"You are a helpful assistant.\"},\n        ),\n    ),\n)\n\nThis creates 3 tasks (1 task × 3 agents, each with cache enabled).\n\n\nCombining Matrix and With\nMix parameter sweeps with common settings:\n\n\nmatrix_and_with.py\n\nfrom inspect_flow import (\n    FlowSpec,\n    configs_matrix,\n    tasks_matrix,\n    tasks_with,\n)\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_with(\n1        task=tasks_matrix(\n            task=[\"task1\", \"task2\"],\n            config=configs_matrix(\n                temperature=[0.0, 0.5, 1.0],\n            ),\n        ),\n2        model=\"openai/gpt-4o\",\n3        sandbox=\"docker\",\n    ),\n)\n\n\n1\n\nCreate a matrix of 6 tasks (2 tasks × 3 temperature values)\n\n2\n\nApply the same model to all 6 tasks from the matrix\n\n3\n\nApply the same sandbox to all 6 tasks from the matrix",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Matrixing"
    ]
  },
  {
    "objectID": "matrix.html#nested-sweeps",
    "href": "matrix.html#nested-sweeps",
    "title": "Matrixing",
    "section": "Nested Sweeps",
    "text": "Nested Sweeps\nMatrix functions can be nested to create complex parameter grids. Use the unpacking operator * to expand inner matrix results:\nExample: Tasks with nested model sweep\n\n\nnested_model_sweep.py\n\nfrom inspect_ai.model import GenerateConfig\nfrom inspect_flow import FlowSpec, models_matrix, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=[\"inspect_evals/mmlu_0_shot\", \"inspect_evals/gpqa_diamond\"],\n1        model=[\n2            \"anthropic/claude-3-5-sonnet\",\n3            *models_matrix(\n                model=[\"openai/gpt-4o\", \"openai/gpt-4o-mini\"],\n                config=[\n                    GenerateConfig(reasoning_effort=\"low\"),\n                    GenerateConfig(reasoning_effort=\"high\"),\n                ],\n            ),\n        ],\n    ),\n)\n\n\n1\n\nTotal of 5 models: 1 single model + 4 from the matrix (2 models × 2 reasoning_effort values)\n\n2\n\nA single model configuration for Claude\n\n3\n\nUse the unpacking operator * to expand the nested model matrix into the list\n\n\nThis creates 10 tasks (2 tasks × 5 model configurations).\nExample: Tasks with nested task sweep\n\n\nnested_task_sweep.py\n\nfrom inspect_flow import FlowSpec, FlowTask, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=[\n1            FlowTask(name=\"task1\", args={\"subset\": \"test\"}),\n2            *tasks_matrix(\n3                task=\"task2\",\n                args=[\n                    {\"language\": \"en\"},\n                    {\"language\": \"de\"},\n                    {\"language\": \"fr\"},\n                ],\n            ),\n        ],\n        model=[\"model1\", \"model2\"],\n    ),\n)\n\n\n1\n\nA single task configuration with specific arguments\n\n2\n\nUse the unpacking operator * to expand the nested task matrix into the list\n\n3\n\nTotal of 4 tasks: 1 single task + 3 from the matrix (1 task × 3 language variants)\n\n\nThis creates 8 tasks (4 task variants × 2 models).\n\n\n\n\n\n\nWarningWatch Out for Combinatorial Explosion\n\n\n\nParameter sweeps grow multiplicatively. A sweep with:\n\n3 tasks\n4 models\n5 temperature values\n3 solver configurations\n\nResults in 3 × 4 × 5 × 3 = 180 evaluations.\nAlways use --dry-run to check the number of evaluations before running expensive grids.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Matrixing"
    ]
  },
  {
    "objectID": "matrix.html#matrix-merge",
    "href": "matrix.html#matrix-merge",
    "title": "Matrixing",
    "section": "Matrix Merge",
    "text": "Matrix Merge\nWhen base objects already have values, matrix parameters are merged:\ntasks_matrix(\n    task=FlowTask(\n        name=\"task\",\n1        config=GenerateConfig(temperature=0.5)\n    ),\n    config=[\n2        GenerateConfig(max_tokens=1000),\n        GenerateConfig(max_tokens=2000),\n    ]\n)\n\n1\n\nBase value of temperature=0.5\n\n2\n\nAdds max_tokens, keeps temperature=0.5\n\n\nThis creates 2 tasks: one with temperature=0.5, max_tokens=1000 and another with temperature=0.5, max_tokens=2000.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Matrixing"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inspect Flow",
    "section": "",
    "text": "Inspect Flow is a workflow orchestration tool for Inspect AI that enables you to run evaluations at scale with repeatability and maintainability.\nWhy Inspect Flow? As evaluation workflows grow in complexity—running multiple tasks across different models with varying parameters—managing these experiments becomes challenging. Inspect Flow addresses this by providing:\n\nDeclarative Configuration: Define complex evaluations with tasks, models, and parameters in type-safe schemas\nRepeatable & Shareable: Encapsulated definitions of tasks, models, configurations, and Python dependencies ensure experiments can be reliably repeated and shared\nPowerful Defaults: Define defaults once and reuse them everywhere with automatic inheritance\nParameter Sweeping: Matrix patterns for systematic exploration across tasks, models, and hyperparameters\n\nInspect Flow is designed for researchers and engineers running systematic AI evaluations who need to scale beyond ad-hoc scripts.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Inspect Flow",
    "section": "",
    "text": "Inspect Flow is a workflow orchestration tool for Inspect AI that enables you to run evaluations at scale with repeatability and maintainability.\nWhy Inspect Flow? As evaluation workflows grow in complexity—running multiple tasks across different models with varying parameters—managing these experiments becomes challenging. Inspect Flow addresses this by providing:\n\nDeclarative Configuration: Define complex evaluations with tasks, models, and parameters in type-safe schemas\nRepeatable & Shareable: Encapsulated definitions of tasks, models, configurations, and Python dependencies ensure experiments can be reliably repeated and shared\nPowerful Defaults: Define defaults once and reuse them everywhere with automatic inheritance\nParameter Sweeping: Matrix patterns for systematic exploration across tasks, models, and hyperparameters\n\nInspect Flow is designed for researchers and engineers running systematic AI evaluations who need to scale beyond ad-hoc scripts.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Inspect Flow",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\nNotePrerequisites\n\n\n\nBefore using Inspect Flow, you should:\n\nHave familiarity with Inspect AI\nHave an existing Inspect evaluation or use one from inspect-evals\n\n\n\n\nInstallation\nInstall the inspect-flow package from PyPI as follows:\npip install inspect-flow\n\n\nSet up API keys\nYou’ll need API keys for the model providers you want to use. Set the relevant provider API key in your .env file or export it in your shell:\n\nOpenAIAnthropicGoogleGrokMistralHugging Face\n\n\nexport OPENAI_API_KEY=your-openai-api-key\n\n\nexport ANTHROPIC_API_KEY=your-anthropic-api-key\n\n\nexport GOOGLE_API_KEY=your-google-api-key\n\n\nexport GROK_API_KEY=your-grok-api-key\n\n\nexport MISTRAL_API_KEY=your-mistral-api-key\n\n\nexport HF_TOKEN=your-hf-token\n\n\n\n\n\nOptional: VS Code extension\nOptionally install the Inspect AI VS Code Extension which includes features for viewing evaluation log files.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#basic-example",
    "href": "index.html#basic-example",
    "title": "Inspect Flow",
    "section": "Basic Example",
    "text": "Basic Example\nLet’s walk through creating your first Flow configuration. We’ll use FlowSpec (the entrypoint class) and FlowTask to define evaluations.\n\n\n\n\n\n\nTipCore Components Reference\n\n\n\n\n\n\nFlowSpec — Pydantic class that encapsulates the declarative description of a Flow spec.\nFlowTask — Pydantic class abstraction on top of Inspect AI Task.\nFlowModel — Pydantic class abstraction on top of Inspect AI Model.\ntasks_matrix() — Helper function for parameter sweeping to generate a list of tasks with all parameter combinations.\nmodels_matrix() — Helper function for parameter sweeping to generate a list of models with all parameter combinations.\nconfigs_matrix() — Helper function for parameter sweeping to generate a list of GenerateConfig with all parameter combinations.\n\n\n\n\nFlowSpec is the main entrypoint for defining evaluation runs. At its core, it takes a list of tasks to run. Here’s a simple example that runs two evaluations:\n\n\nconfig.py\n\nfrom inspect_flow import FlowSpec, FlowTask\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n        ),\n        FlowTask(\n            name=\"inspect_evals/mmlu_0_shot\",\n            model=\"openai/gpt-4o\",\n        ),\n    ],\n)\n\nTo run the evaluations, execute the following command. It creates a virtual environment for this spec run and installs the dependencies—note that task and model dependencies (like the inspect-evals and openai Python packages) are inferred and installed automatically.\nflow run config.py\nBoth tasks will run with progress displayed in your terminal.\n\n\n\nProgress bar in terminal\n\n\n\nPython API\nYou can run evaluations from Python instead of the command line by calling the run() function with a FlowSpec.\n\n\nconfig.py\n\nfrom inspect_flow import FlowSpec, FlowTask\nfrom inspect_flow.api import run\n\nspec = FlowSpec(\n    log_dir=\"logs\",\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n        ),\n        FlowTask(\n            name=\"inspect_evals/mmlu_0_shot\",\n            model=\"openai/gpt-4o\",\n        ),\n    ],\n)\nrun(spec=spec)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#matrix-functions",
    "href": "index.html#matrix-functions",
    "title": "Inspect Flow",
    "section": "Matrix Functions",
    "text": "Matrix Functions\nOften you’ll want to evaluate multiple tasks across multiple models. Rather than manually defining every combination, use tasks_matrix to generate all task-model pairs:\n\n\nmatrix.py\n\nfrom inspect_flow import FlowSpec, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=[\n            \"openai/gpt-5\",\n            \"openai/gpt-5-mini\",\n        ],\n    ),\n)\n\nTo preview the expanded config before running it, you can run the following command in your shell to ensure the generated config is the one that you intend to run.\nflow config matrix.py\nThis command outputs the expanded configuration showing all 4 task-model combinations (2 tasks × 2 models).\n\n\nmatrix.yml\n\nlog_dir: logs\ntasks:\n- name: inspect_evals/gpqa_diamond\n  model: openai/gpt-5\n- name: inspect_evals/gpqa_diamond\n  model: openai/gpt-5-mini\n- name: inspect_evals/mmlu_0_shot\n  model: openai/gpt-5\n- name: inspect_evals/mmlu_0_shot\n  model: openai/gpt-5-mini\n\ntasks_matrix and models_matrix are powerful functions that can operate on multiple levels of nested matrixes which enable sophisticated parameter sweeping. Let’s say you want to explore different reasoning efforts across models—you can achieve this with the models_matrix function.\n\n\nmodels_matrix.py\n\nfrom inspect_ai.model import GenerateConfig\nfrom inspect_flow import FlowSpec, models_matrix, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=models_matrix(\n            model=[\n                \"openai/gpt-5\",\n                \"openai/gpt-5-mini\",\n            ],\n            config=[\n                GenerateConfig(reasoning_effort=\"minimal\"),\n                GenerateConfig(reasoning_effort=\"low\"),\n                GenerateConfig(reasoning_effort=\"medium\"),\n                GenerateConfig(reasoning_effort=\"high\"),\n            ],\n        ),\n    ),\n)\n\nFor even more concise parameter sweeping, use configs_matrix to generate configuration variants. This produces the same 16 evaluations (2 tasks × 2 models × 4 reasoning levels) as above, but with less boilerplate:\n\n\nconfigs_matrix.py\n\nfrom inspect_flow import FlowSpec, configs_matrix, models_matrix, tasks_matrix\n\nFlowSpec(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=models_matrix(\n            model=[\n                \"openai/gpt-5\",\n                \"openai/gpt-5-mini\",\n            ],\n            config=configs_matrix(\n                reasoning_effort=[\"minimal\", \"low\", \"medium\", \"high\"],\n            ),\n        ),\n    ),\n)\n\n\nRun evaluations\nBefore running evaluations, preview the resolved configuration with --dry-run:\nflow run matrix.py --dry-run\nThis creates the virtual environment, installs all dependencies, imports tasks from the registry, applies all defaults, and expands all matrix functions—everything except actually running the evaluations. Unlike flow config which just parses the config file, --dry-run performs the full setup process.\nTo run the config:\nflow run matrix.py\nThis will run all 16 evaluations (2 tasks × 2 models × 4 reasoning levels). When complete, you’ll find a link to the logs at the bottom of the task results summary.\n\n\n\nLog path printed in terminal\n\n\nTo view logs interactively, run:\ninspect view --log-dir logs\n\n\n\nEval logs rendered by Inspect View",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#learning-more",
    "href": "index.html#learning-more",
    "title": "Inspect Flow",
    "section": "Learning More",
    "text": "Learning More\nSee the following articles to learn more about using Flow:\n\nFlow Concepts: Flow type system, config structure and basics.\nDefaults: Define defaults once and reuse them everywhere with automatic inheritance.\nMatrixing: Systematic parameter exploration with matrix and with functions.\nReference: Detailed documentation on the Flow Python API and CLI commands.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "defaults.html",
    "href": "defaults.html",
    "title": "Defaults, Inheritance and Overrides",
    "section": "",
    "text": "Inspect Flow provides powerful mechanisms to avoid repetition and share configuration across evaluations. This page covers:",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Defaults"
    ]
  },
  {
    "objectID": "defaults.html#flowdefaults",
    "href": "defaults.html#flowdefaults",
    "title": "Defaults, Inheritance and Overrides",
    "section": "FlowDefaults",
    "text": "FlowDefaults\nFlowDefaults supports multiple levels of default configuration:\n\n\nconfig.py\n\nfrom inspect_ai.model import GenerateConfig\nfrom inspect_flow import (\n    FlowAgent,\n    FlowDefaults,\n    FlowModel,\n    FlowSolver,\n    FlowSpec,\n    FlowTask,\n)\n\nFlowSpec(\n    defaults=FlowDefaults(\n1        config=GenerateConfig(\n            max_connections=10,\n        ),\n2        model=FlowModel(\n            model_args={\"arg\": \"foo\"},\n        ),\n3        model_prefix={\n            \"openai/\": FlowModel(\n                config=GenerateConfig(\n                    max_connections=20\n                ),\n            ),\n        },\n4        solver=FlowSolver(name=\"generate\"),\n5        solver_prefix={\"chain_of_thought\": FlowSolver(name=\"chain_of_thought\")},\n6        agent=FlowAgent(name=\"basic\"),\n7        agent_prefix={\"inspect/\": FlowAgent(name=\"inspect/basic\")},\n8        task=FlowTask(model=\"openai/gpt-4o\"),\n9        task_prefix={\"inspect_evals/\": FlowTask(model=\"openai/gpt-4o-mini\")},\n    ),\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n        )\n    ],\n)\n\n\n1\n\nDefault model generation options. Will be overridden by settings on FlowTask and FlowModel.\n\n2\n\nField defaults for models.\n\n3\n\nModel defaults for model name prefixes. Overrides FlowDefaults.config and FlowDefaults.model. If multiple prefixes match, longest prefix wins.\n\n4\n\nField defaults for solvers.\n\n5\n\nSolver defaults for solver name prefixes. Overrides FlowDefaults.solver. If multiple prefixes match, longest prefix wins.\n\n6\n\nField defaults for agents.\n\n7\n\nAgent defaults for agent name prefixes. Overrides FlowDefaults.agent. If multiple prefixes match, longest prefix wins.\n\n8\n\nField defaults for tasks.\n\n9\n\nTask defaults for task name prefixes. Overrides FlowDefaults.config and FlowDefaults.task. If multiple prefixes match, longest prefix wins.\n\n\n\nMerge Priority\nDefaults follow a hierarchy where more specific settings override less specific ones:\nFor GenerateConfig:\n\nGlobal config defaults (defaults.config)\nGlobal model config defaults (defaults.model.config)\nModel prefix config defaults (defaults.model_prefix.config)\nTask-specific config (task.config)\nModel-specific config (model.config) — highest priority\n\nExample hierarchy in action:\n\n\nconfig.py\n\nfrom inspect_ai.model import GenerateConfig\nfrom inspect_flow import FlowDefaults, FlowModel, FlowSpec, FlowTask\n\nFlowSpec(\n    defaults=FlowDefaults(\n        config=GenerateConfig(\n1            temperature=0.0,\n            max_tokens=100,\n        ),\n        model_prefix={\n            \"openai/\": FlowModel(\n2                config=GenerateConfig(temperature=0.5)\n            )\n        },\n    ),\n    tasks=[\n        FlowTask(\n            name=\"task\",\n3            config=GenerateConfig(temperature=0.7),\n            model=FlowModel(\n                name=\"openai/gpt-4o\",\n4                config=GenerateConfig(temperature=1.0),\n            ),\n        )\n    ],\n)\n\n\n1\n\nGlobal defaults: temperature=0.0, max_tokens=100\n\n2\n\nPrefix defaults override: temperature=0.5 (for OpenAI models)\n\n3\n\nTask config overrides: temperature=0.7\n\n4\n\nModel config wins: temperature=1.0, max_tokens=100\n\n\nFinal result: temperature=1.0 (most specific), max_tokens=100 (from global defaults)\n\n\n\n\n\n\nNoteNone Values and Merge Behavior\n\n\n\nFor fields in Flow types: Most fields use a special “not given” default, which means None is a meaningful value that does override:\n\n\nconfig.py\n\nfrom inspect_flow import FlowDefaults, FlowModel, FlowSpec, FlowTask\n\nFlowSpec(\n    defaults=FlowDefaults(\n        model=FlowModel(name=\"openai/gpt-4o\"),\n    ),\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=None,  # Explicitly set to None\n        )\n    ],\n)\n# Result: Task uses model=None (overrides the default \"openai/gpt-4o\")\n\nTo avoid overriding a FlowTask field, omit it entirely rather than setting it to None.\nFor GenerateConfig and other Inspect types: Setting a field to None means “not specified” — it won’t override existing values from defaults:\n\n\nconfig.py\n\nfrom inspect_ai.model import GenerateConfig\nfrom inspect_flow import FlowDefaults, FlowSpec, FlowTask\n\nFlowSpec(\n    defaults=FlowDefaults(config=GenerateConfig(temperature=0.8, max_tokens=1000)),\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n            config=GenerateConfig(temperature=0.5, max_tokens=None),\n        )\n    ],\n)\n# Result: Task runs with temperature=0.5, max_tokens=1000\n# The max_tokens=None didn't override the default 1000\n\nValidating the resolved config: You can always review and validate the final resolved configuration by running flow run config.py --dry-run or flow config config.py which will print the resolved config in YAML format.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Defaults"
    ]
  },
  {
    "objectID": "defaults.html#inheritance",
    "href": "defaults.html#inheritance",
    "title": "Defaults, Inheritance and Overrides",
    "section": "Inheritance",
    "text": "Inheritance\nInspect Flow supports configuration inheritance to share settings across multiple config files. This is particularly useful for defining global defaults at a repository level that apply to all evaluations.\n\nAutomatic Discovery\nInspect Flow automatically discovers and includes files named _flow.py in parent directories. Starting from your config file’s location, it searches upward through the directory tree for _flow.py files and automatically merges them.\nThis allows you to define shared defaults (model settings, dependencies, etc.) at a repository root that apply to all configs in subdirectories without explicit includes.\n\n\n\n\n\n\nNote\n\n\n\nWhen using run() from the Python API to run the FlowSpec directly instead of the command line, the base_dir argument is used as the starting point for searching upward through the directory tree, rather than the config file’s location.\n\n\n\n\nIncludes\nUse the includes field to explicitly merge other config files into your spec:\nFlowSpec(\n    includes=[\"../foo/other_config.py\"],\n    log_dir=\"logs\",\n    tasks=[\"my_task\"]\n)\n\nMerge\nIncluded configs are merged recursively, with the current config’s values taking precedence over included values:\n\nDictionaries: Fields are merged deeply (recursive merge)\nLists: Items are concatenated with duplicates removed\nScalars: Current config values override included values\n\n\n\nOrder\nIncludes are processed sequentially in the order they appear. Each included file is loaded, its includes are recursively processed, and then merged into the current config without overwriting existing values.\nPriority order (highest to lowest):\n\n\nconfig.py\n\nFlowSpec(\n    includes=[\"defaults.py\", \"shared.py\", \"path.py\"],\n    ...\n)\n\n\nMain config file (config.py)\ndefaults.py\nFiles included by defaults.py\nshared.py\nFiles included by shared.py\npath.py\nFiles included by path.py\n\n\n\nRecursion\nIncluded files can themselves have includes fields, which are expanded recursively. This allows you to build hierarchies of configuration.\n\n\nPath Resolution\nRelative paths will be resolved relative to the config file (when using the CLI) or base_dir arg (when using the API):\n\n\nconfig.py\n\nfrom inspect_flow import FlowSpec, FlowTask\n\nFlowSpec(\n    includes=[\n        \"defaults.py\",\n        \"../shared.py\",\n        \"/absolute/path.py\",\n    ],\n    log_dir=\"logs\",\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n        ),\n    ],\n)\n\n\n\n\n\n\n\nNote\n\n\n\nAutomatic discovery does not look for _flow.py files in the parent directories of explicitly included files.\n\n\n\n\n\nInheritance with FlowDefaults\nConfig inheritance is especially powerful when combined with FlowDefaults. You can define global defaults in a _flow.py file at your repository root:\n\n\n_flow.py\n\nfrom inspect_ai.model import GenerateConfig\nfrom inspect_flow import FlowDefaults, FlowSpec\n\nFlowSpec(\n    defaults=FlowDefaults(\n        config=GenerateConfig(\n            max_connections=10,\n            temperature=0.0,\n        ),\n    ),\n)\n\nThen all configs in subdirectories automatically inherit these defaults, which can be selectively overridden:\n\n\nexperiments/config.py\n\nfrom inspect_ai.model import GenerateConfig\nfrom inspect_flow import FlowSpec, FlowTask\n\nFlowSpec(\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n            # Override just temperature\n            config=GenerateConfig(temperature=0.7),\n        ),\n    ],\n)\n# Inherits max_connections=10 from _flow.py\n# Overrides temperature=0.7 for this specific task\n\n\n\n\n\n\n\nTipPreventing Overrides\n\n\n\nWhen using inheritance to define shared defaults, you can enforce critical settings by preventing including configs from overriding them. See Lock Configuration Fields in the Advanced section to learn how to lock inherited or default values and prevent them from being overwritten.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Defaults"
    ]
  },
  {
    "objectID": "defaults.html#cli-overrides",
    "href": "defaults.html#cli-overrides",
    "title": "Defaults, Inheritance and Overrides",
    "section": "CLI Overrides",
    "text": "CLI Overrides\nOverride config values at runtime using the --set flag:\n\nBasic usageNested pathsJSON dictsMultiple overrides\n\n\nflow run config.py --set log_dir=./logs\n\n\nflow run config.py --set options.limit=10\nflow run config.py --set defaults.solver.args.tool_calls=none\n\n\nflow run config.py --set 'options.metadata={\"experiment\": \"baseline\", \"version\": \"v1\"}'\n\n\nflow run config.py \\\n  --set log_dir=./logs/experiment1 \\\n  --set options.limit=100 \\\n  --set defaults.config.temperature=0.5\n\n\n\n\n\n\n\n\n\nNoteOverride Behavior\n\n\n\n\nStrings: Replace existing values\nDicts: Replace existing values\nLists:\n\nString values append to existing list\nLists replace existing list\n\n\nExamples:\n# Appends to list\n--set dependencies=new_package\n\n# Replaces list\n--set 'dependencies=[\"pkg1\", \"pkg2\"]'",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Defaults"
    ]
  },
  {
    "objectID": "defaults.html#env-vars",
    "href": "defaults.html#env-vars",
    "title": "Defaults, Inheritance and Overrides",
    "section": "Env Vars",
    "text": "Env Vars\nSet config values via environment variables:\nexport INSPECT_FLOW_LOG_DIR=./logs/custom\nexport INSPECT_FLOW_LIMIT=50\nexport INSPECT_FLOW_SET=\"options.metadata={\\\"key\\\": \\\"value\\\"}\"\nexport INSPECT_FLOW_ARG=\"task_min_priority=2\"\nflow run config.py\nSupported environment variables:\n\n\n\n\n\n\n\n\nVariable\nEquivalent Flag\nDescription\n\n\n\n\nINSPECT_FLOW_LOG_DIR\n--log-dir\nOverride log directory\n\n\nINSPECT_FLOW_LOG_DIR_CREATE_UNIQUE\n--log-dir-create-unique\nCreate new log directory with numeric suffix if exists\n\n\nINSPECT_FLOW_LOG_LEVEL\n--log-level\nInspect Flow log level. Defaults to Info. Use options.log_level to set the Inspect AI log level.\n\n\nINSPECT_FLOW_LIMIT\n--limit\nLimit number of samples\n\n\nINSPECT_FLOW_SET\n--set\nSet config overrides (can be specified multiple times)\n\n\nINSPECT_FLOW_ARG\n--arg\nArgs to pass to spec functions in the config file (can be multiple)\n\n\nINSPECT_FLOW_NO_VENV\n--no-venv\nDo not create a virtual environment to run the Flow spec\n\n\nINSPECT_FLOW_DRY_RUN\n--dry-run\nCreate virtual environment and print resolved YAML configuration\n\n\n\n\n\n\n\n\n\nTipOverride Priority\n\n\n\nSetting defaults via the command line will override the defaults which in turn might be overridden by anything set explicitly.\nRuntime-only flags: INSPECT_FLOW_LOG_LEVEL, INSPECT_FLOW_ARG, INSPECT_FLOW_NO_VENV, INSPECT_FLOW_DRY_RUN, and INSPECT_FLOW_SET (and their corresponding CLI flags --log-level, --arg, --no-venv, --dry-run, --set) are runtime settings for the flow run command and cannot be set in FlowSpec.\nSettings with multiple override levels:\nPriority order for log-dir, log-dir-create-unique and limit:\n\nFlowSpec defaults\nExplicit setting on the FlowSpec\nINSPECT_FLOW_SET_ environment variables\nCLI --set flags\nINSPECT_FLOW_LOG_DIR, INSPECT_FLOW_LOG_DIR_CREATE_UNIQUE and INSPECT_FLOW_LIMIT environment variables\nExplicit --log-dir, --log-dir-create-unique and --limit CLI flags\n\nPriority order for all other settings:\n\nFlowSpec defaults\nExplicit setting on the FlowSpec\nINSPECT_FLOW_SET_ environment variables\nCLI --set flags",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Defaults"
    ]
  },
  {
    "objectID": "defaults.html#debugging",
    "href": "defaults.html#debugging",
    "title": "Defaults, Inheritance and Overrides",
    "section": "Debugging",
    "text": "Debugging\nTo see the fully resolved configuration with all defaults, inheritance, and overrides applied:\nflow run config.py --dry-run\nThis shows exactly what settings each task will use after applying all defaults and overrides.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Defaults"
    ]
  },
  {
    "objectID": "run.html",
    "href": "run.html",
    "title": "Running Flows",
    "section": "",
    "text": "Once you’ve defined your Flow configuration, you can execute evaluations using the flow run command. Flow also provides tools for previewing configurations and controlling runtime behavior.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Running Flows"
    ]
  },
  {
    "objectID": "run.html#the-flow-run-command",
    "href": "run.html#the-flow-run-command",
    "title": "Running Flows",
    "section": "The flow run Command",
    "text": "The flow run Command\nExecute your evaluation workflow:\nflow run config.py\nWhat happens when you run this:\n\nFlow loads your configuration file\nCreates an isolated virtual environment\nInstalls dependencies\nResolves all defaults and matrix expansions\nExecutes evaluations via Inspect AI’s eval_set()\nStores logs in log_dir\nCleans up the temporary environment\n\n\nCommon CLI Flags\nPreview without running:\nflow run config.py --dry-run\nShows the completely resolved configuration:\n\nCreates virtual environment\nApplies all defaults\nExpands all matrix functions\nInstantiates all Python objects\n\nThis is invaluable for debugging what settings will actually be used in your evaluations.\nOverride log directory:\nflow run config.py --log-dir ./experiments/baseline\nChanges where logs and results are stored.\nRuntime overrides:\nflow run config.py \\\n  --set options.limit=100 \\\n  --set defaults.config.temperature=0.5\nOverride any configuration value at runtime. See CLI Overrides for more details.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Running Flows"
    ]
  },
  {
    "objectID": "run.html#the-flow-config-command",
    "href": "run.html#the-flow-config-command",
    "title": "Running Flows",
    "section": "The flow config Command",
    "text": "The flow config Command\nPreview your configuration before running:\nflow config config.py\nDisplays the parsed configuration as YAML with CLI overrides applied. Does not create a virtual environment or instantiate Python objects.\n\n\n\n\n\n\nTipWhen to Use Each Command\n\n\n\n\nflow config - Quick syntax check, verify overrides\nflow run --dry-run - Debug defaults resolution, inspect final settings\nflow run - Execute evaluations",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Running Flows"
    ]
  },
  {
    "objectID": "run.html#running-from-python",
    "href": "run.html#running-from-python",
    "title": "Running Flows",
    "section": "Running from Python",
    "text": "Running from Python\nYou can run Flow evaluations programmatically using the Python API:\n\n\nrun.py\n\nfrom inspect_flow import FlowSpec, FlowTask\nfrom inspect_flow.api import run\n\nspec = FlowSpec(\n    log_dir=\"logs\",\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n        ),\n        FlowTask(\n            name=\"inspect_evals/mmlu_0_shot\",\n            model=\"openai/gpt-4o\",\n        ),\n    ],\n)\nrun(spec=spec)\n\nThe inspect_flow.api module provides three functions:\n\nrun() - Execute a Flow spec with full environment setup (equivalent to flow run)\nload_spec() - Load a Flow configuration from a Python file into a FlowSpec object\nconfig() - Get the resolved configuration as YAML (equivalent to flow config)\n\nSee the API Reference for detailed parameter documentation.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Running Flows"
    ]
  },
  {
    "objectID": "run.html#results-and-logs",
    "href": "run.html#results-and-logs",
    "title": "Running Flows",
    "section": "Results and Logs",
    "text": "Results and Logs\n\nLogs Directory\nEvaluation results are stored in the log_dir:\nlogs/\n├── 2025-11-21T17-38-20+01-00_gpqa-diamond_KvJBGowidXSCLRhkKQbHYA.eval\n├── 2025-11-21T17-38-20+01-00_mmlu-0-shot_Vnu2A3M2wPet5yobLiCQmZ.eval\n├── .eval-set-id\n├── eval-set.json\n├── flow.yaml\n├── flow-requirements.txt\n└── ...\nDirectory structure:\n\nFlow passes the log_dir directly to Inspect AI eval_set() for evaluation log storage\nInspect AI handles the actual evaluation log file naming and storage\nLog file naming conventions follow Inspect AI’s standards (see Inspect AI logging docs)\nFlow automatically saves the resolved configuration as flow.yaml in the log directory\nFlow saves the exact version of packages installed in the virtual environment as flow-requirements.txt\nThe .eval-set-id file contains the eval set identifier\nThe eval-set.json file contains eval set metadata\n\nLog formats:\n\n.eval - Binary Inspect AI log format (default, high-performance)\n.json - JSON format (if log_format=\"json\" in FlowOptions)\n\n\n\nViewing Results\nUsing Inspect View:\ninspect view\nOpens the Inspect AI viewer to explore evaluation logs interactively. Inspect View can automatically detect Flow config files in the log directory and render them in the UI, making it easier to review the spec for the evaluations.\nClick the Flow icon in the top right hand corner to view the Flow config.\n\n\n\nEval list rendered by Inspect View\n\n\nThe Flow config file is rendered in YAML format.\n\n\n\nFlow config rendered by Inspect View\n\n\n\n\nS3 Support\nStore logs directly to S3:\nFlowSpec(\n    log_dir=\"s3://my-bucket/experiments/baseline\",\n    tasks=[...]\n)\nFor more information on configuring an S3 bucket as a logs directory, refer to the Inspect AI documentation.",
    "crumbs": [
      "Welcome",
      "Using Flow",
      "Running Flows"
    ]
  },
  {
    "objectID": "reference/inspect_flow.api.html",
    "href": "reference/inspect_flow.api.html",
    "title": "inspect_flow.api",
    "section": "",
    "text": "Return the flow spec configuration.\n\nSource\n\ndef config(\n    spec: FlowSpec,\n    base_dir: str | None = None,\n    *,\n    log_level: str | None = None,\n) -&gt; str\n\nspec FlowSpec\n\nThe flow spec configuration.\n\nbase_dir str | None\n\nThe base directory for resolving relative paths. Defaults to the current working directory.\n\nlog_level str | None\n\nThe Inspect Flow log level to use. Use spec.options.log_level to set the Inspect AI log level.\n\n\n\n\n\nLoad a spec from file.\n\nSource\n\ndef load_spec(\n    file: str,\n    *,\n    log_level: str | None = None,\n    args: dict[str, Any] | None = None,\n) -&gt; FlowSpec\n\nfile str\n\nThe path to the spec file.\n\nlog_level str | None\n\nThe Inspect Flow log level to use. Use spec.options.log_level to set the Inspect AI log level.\n\nargs dict[str, Any] | None\n\nA dictionary of arguments to pass as kwargs to the function in the flow config.\n\n\n\n\n\nRun an inspect_flow evaluation.\n\nSource\n\ndef run(\n    spec: FlowSpec,\n    base_dir: str | None = None,\n    *,\n    dry_run: bool = False,\n    log_level: str | None = None,\n    no_venv: bool = False,\n    no_dotenv: bool = False,\n) -&gt; None\n\nspec FlowSpec\n\nThe flow spec configuration.\n\nbase_dir str | None\n\nThe base directory for resolving relative paths. Defaults to the current working directory.\n\ndry_run bool\n\nIf True, do not run eval, but show a count of tasks that would be run.\n\nlog_level str | None\n\nThe Inspect Flow log level to use. Use spec.options.log_level to set the Inspect AI log level.\n\nno_venv bool\n\nIf True, do not create a virtual environment to run the spec.\n\nno_dotenv bool\n\nIf True, do not load environment variables from a .env file.",
    "crumbs": [
      "Reference",
      "Python API",
      "inspect_flow.api"
    ]
  },
  {
    "objectID": "reference/inspect_flow.api.html#python-api",
    "href": "reference/inspect_flow.api.html#python-api",
    "title": "inspect_flow.api",
    "section": "",
    "text": "Return the flow spec configuration.\n\nSource\n\ndef config(\n    spec: FlowSpec,\n    base_dir: str | None = None,\n    *,\n    log_level: str | None = None,\n) -&gt; str\n\nspec FlowSpec\n\nThe flow spec configuration.\n\nbase_dir str | None\n\nThe base directory for resolving relative paths. Defaults to the current working directory.\n\nlog_level str | None\n\nThe Inspect Flow log level to use. Use spec.options.log_level to set the Inspect AI log level.\n\n\n\n\n\nLoad a spec from file.\n\nSource\n\ndef load_spec(\n    file: str,\n    *,\n    log_level: str | None = None,\n    args: dict[str, Any] | None = None,\n) -&gt; FlowSpec\n\nfile str\n\nThe path to the spec file.\n\nlog_level str | None\n\nThe Inspect Flow log level to use. Use spec.options.log_level to set the Inspect AI log level.\n\nargs dict[str, Any] | None\n\nA dictionary of arguments to pass as kwargs to the function in the flow config.\n\n\n\n\n\nRun an inspect_flow evaluation.\n\nSource\n\ndef run(\n    spec: FlowSpec,\n    base_dir: str | None = None,\n    *,\n    dry_run: bool = False,\n    log_level: str | None = None,\n    no_venv: bool = False,\n    no_dotenv: bool = False,\n) -&gt; None\n\nspec FlowSpec\n\nThe flow spec configuration.\n\nbase_dir str | None\n\nThe base directory for resolving relative paths. Defaults to the current working directory.\n\ndry_run bool\n\nIf True, do not run eval, but show a count of tasks that would be run.\n\nlog_level str | None\n\nThe Inspect Flow log level to use. Use spec.options.log_level to set the Inspect AI log level.\n\nno_venv bool\n\nIf True, do not create a virtual environment to run the spec.\n\nno_dotenv bool\n\nIf True, do not load environment variables from a .env file.",
    "crumbs": [
      "Reference",
      "Python API",
      "inspect_flow.api"
    ]
  },
  {
    "objectID": "reference/inspect_flow.html",
    "href": "reference/inspect_flow.html",
    "title": "inspect_flow",
    "section": "",
    "text": "Configuration for an Agent.\n\nSource\n\nclass FlowAgent(BaseModel, extra=\"forbid\")\n\n\n\nname str | None | NotGiven\n\nName of the agent. Required to be set by the time the agent is created.\n\nargs CreateArgs | None | NotGiven\n\nAdditional args to pass to agent constructor.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the agent.\n\ntype Literal['agent'] | None\n\nType needed to differentiated solvers and agents in solver lists.\n\n\n\n\n\n\nDefault field values for Inspect objects. Will be overriden by more specific settings.\n\nSource\n\nclass FlowDefaults(BaseModel, extra=\"forbid\")\n\n\n\nconfig GenerateConfig | None | NotGiven\n\nDefault model generation options. Will be overriden by settings on the FlowModel and FlowTask.\n\nagent FlowAgent | None | NotGiven\n\nField defaults for agents.\n\nagent_prefix dict[str, FlowAgent] | None | NotGiven\n\nAgent defaults for agent name prefixes. E.g. {‘inspect/’: FAgent(…)}\n\nmodel FlowModel | None | NotGiven\n\nField defaults for models.\n\nmodel_prefix dict[str, FlowModel] | None | NotGiven\n\nModel defaults for model name prefixes. E.g. {‘openai/’: FModel(…)}\n\nsolver FlowSolver | None | NotGiven\n\nField defaults for solvers.\n\nsolver_prefix dict[str, FlowSolver] | None | NotGiven\n\nSolver defaults for solver name prefixes. E.g. {‘inspect/’: FSolver(…)}\n\ntask FlowTask | None | NotGiven\n\nField defaults for tasks.\n\ntask_prefix dict[str, FlowTask] | None | NotGiven\n\nTask defaults for task name prefixes. E.g. {‘inspect_evals/’: FTask(…)}\n\n\n\n\n\n\nConfiguration for flow dependencies to install in the venv.\n\nSource\n\nclass FlowDependencies(BaseModel, extra=\"forbid\")\n\n\n\ndependency_file Literal['auto', 'no_file'] | str | None | NotGiven\n\nPath to a dependency file (either requirements.txt or pyproject.toml) to use to create the virtual environment. If ‘auto’, will search the path starting from the same directory as the config file (when using the CLI) or base_dir arg (when using the API) looking for pyproject.toml or requirements.txt files. If ‘no_file’, no dependency file will be used. Defaults to ‘auto’.\n\nadditional_dependencies str | Sequence[str] | None | NotGiven\n\nDependencies to pip install. E.g. PyPI package specifiers or Git repository URLs.\n\nauto_detect_dependencies bool | None | NotGiven\n\nIf True, automatically detect and install dependencies from names of objects in the config (defaults to True). For example, if a model name starts with ‘openai/’, the ‘openai’ package will be installed. If a task name is ‘inspect_evals/mmlu’ then the ‘inspect-evals’ package will be installed.\n\n\n\n\n\n\nConfiguration for task epochs.\nNumber of epochs to repeat samples over and optionally one or more reducers used to combine scores from samples across epochs. If not specified the “mean” score reducer is used.\n\nSource\n\nclass FlowEpochs(BaseModel, extra=\"forbid\")\n\n\n\nepochs int\n\nNumber of epochs.\n\nreducer str | Sequence[str] | None | NotGiven\n\nOne or more reducers used to combine scores from samples across epochs (defaults to “mean”)\n\n\n\n\n\n\nTop-level flow specification.\n\nSource\n\nclass FlowSpec(BaseModel, extra=\"forbid\")\n\n\n\nincludes Sequence[str] | None | NotGiven\n\nList of other flow configs to include. Relative paths will be resolved relative to the config file (when using the CLI) or base_dir arg (when using the API). In addition to this list of explicit files to include, any _flow.py files in the same directory or any parent directory of the config file (when using the CLI) or base_dir arg (when using the API) will also be included automatically.\n\nlog_dir str | None | NotGiven\n\nOutput path for logging results (required to ensure that a unique storage scope is assigned). Must be set before running the flow spec. Relative paths will be resolved relative to the config file (when using the CLI) or base_dir arg (when using the API).\n\nlog_dir_create_unique bool | None | NotGiven\n\nIf True, create a new log directory by appending an _ and numeric suffix if the specified log_dir already exists. If the directory exists and has a _numeric suffix, that suffix will be incremented. If False, use the existing log_dir (which must be empty or have log_dir_allow_dirty=True). Defaults to False.\n\npython_version str | None | NotGiven\n\nPython version to use in the flow virtual environment (e.g. ‘3.11’)\n\noptions FlowOptions | None | NotGiven\n\nArguments for calls to eval_set.\n\ndependencies FlowDependencies | None | NotGiven\n\nDependencies to install in the venv. Defaults to auto-detecting dependencies from pyproject.toml, requirements.txt, and object names in the config.\n\nenv dict[str, str] | None | NotGiven\n\nEnvironment variables to set when running tasks.\n\ndefaults FlowDefaults | None | NotGiven\n\nDefaults values for Inspect objects.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the model.\n\ntasks Sequence[str | FlowTask] | None | NotGiven\n\nTasks to run\n\n\n\n\n\n\nConfiguration for a Model.\n\nSource\n\nclass FlowModel(BaseModel, extra=\"forbid\")\n\n\n\nname str | None | NotGiven\n\nName of the model to use. Required to be set by the time the model is created.\n\nrole str | None | NotGiven\n\nOptional named role for model (e.g. for roles specified at the task or eval level). Provide a default as a fallback in the case where the role hasn’t been externally specified.\n\ndefault str | None | NotGiven\n\nOptional. Fallback model in case the specified model or role is not found. Should be a fully qualified model name (e.g. openai/gpt-4o).\n\nconfig GenerateConfig | None | NotGiven\n\nConfiguration for model. Config values will be override settings on the FlowTask and FlowSpec.\n\nbase_url str | None | NotGiven\n\nOptional. Alternate base URL for model.\n\napi_key str | None | NotGiven\n\nOptional. API key for model.\n\nmemoize bool | None | NotGiven\n\nUse/store a cached version of the model based on the parameters to get_model(). Defaults to True.\n\nmodel_args CreateArgs | None | NotGiven\n\nAdditional args to pass to model constructor.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the model.\n\n\n\n\n\n\nEvaluation options.\n\nSource\n\nclass FlowOptions(BaseModel, extra=\"forbid\")\n\n\n\nretry_attempts int | None | NotGiven\n\nMaximum number of retry attempts before giving up (defaults to 10).\n\nretry_wait float | None | NotGiven\n\nTime to wait between attempts, increased exponentially (defaults to 30, resulting in waits of 30, 60, 120, 240, etc.). Wait time per-retry will in no case be longer than 1 hour.\n\nretry_connections float | None | NotGiven\n\nReduce max_connections at this rate with each retry (defaults to 1.0, which results in no reduction).\n\nretry_cleanup bool | None | NotGiven\n\nCleanup failed log files after retries (defaults to True).\n\nsandbox SandboxEnvironmentType | None | NotGiven\n\nSandbox environment type (or optionally a str or tuple with a shorthand spec).\n\nsandbox_cleanup bool | None | NotGiven\n\nCleanup sandbox environments after task completes (defaults to True).\n\ntags Sequence[str] | None | NotGiven\n\nTags to associate with this evaluation run.\n\nmetadata dict[str, Any] | None | NotGiven\n\nMetadata to associate with this evaluation run.\n\ntrace bool | None | NotGiven\n\nTrace message interactions with evaluated model to terminal.\n\ndisplay DisplayType | None | NotGiven\n\nTask display type (defaults to ‘full’).\n\napproval str | ApprovalPolicyConfig | None | NotGiven\n\nTool use approval policies. Either a path to an approval policy config file or a list of approval policies. Defaults to no approval policy.\n\nscore bool | None | NotGiven\n\nScore output (defaults to True).\n\nlog_level str | None | NotGiven\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”).\n\nlog_level_transcript str | None | NotGiven\n\nLevel for logging to the log file (defaults to “info”).\n\nlog_format Literal['eval', 'json'] | None | NotGiven\n\nFormat for writing log files (defaults to “eval”, the native high-performance format).\n\nlimit int | None | NotGiven\n\nLimit evaluated samples (defaults to all samples).\n\nsample_shuffle bool | int | None | NotGiven\n\nShuffle order of samples (pass a seed to make the order deterministic).\n\nfail_on_error bool | float | None | NotGiven\n\nTrue to fail on first sample error(default); False to never fail on sample errors; Value between 0 and 1 to fail if a proportion of total samples fails. Value greater than 1 to fail eval if a count of samples fails.\n\ncontinue_on_fail bool | None | NotGiven\n\nTrue to continue running and only fail at the end if the fail_on_error condition is met. False to fail eval immediately when the fail_on_error condition is met (default).\n\nretry_on_error int | None | NotGiven\n\nNumber of times to retry samples if they encounter errors (defaults to 3).\n\ndebug_errors bool | None | NotGiven\n\nRaise task errors (rather than logging them) so they can be debugged (defaults to False).\n\nmax_samples int | None | NotGiven\n\nMaximum number of samples to run in parallel (default is max_connections).\n\nmax_tasks int | None | NotGiven\n\nMaximum number of tasks to run in parallel (defaults is 10).\n\nmax_subprocesses int | None | NotGiven\n\nMaximum number of subprocesses to run in parallel (default is os.cpu_count()).\n\nmax_sandboxes int | None | NotGiven\n\nMaximum number of sandboxes (per-provider) to run in parallel.\n\nlog_samples bool | None | NotGiven\n\nLog detailed samples and scores (defaults to True).\n\nlog_realtime bool | None | NotGiven\n\nLog events in realtime (enables live viewing of samples in inspect view) (defaults to True).\n\nlog_images bool | None | NotGiven\n\nLog base64 encoded version of images, even if specified as a filename or URL (defaults to False).\n\nlog_buffer int | None | NotGiven\n\nNumber of samples to buffer before writing log file. If not specified, an appropriate default for the format and filesystem is chosen (10 for most all cases, 100 for JSON logs on remote filesystems).\n\nlog_shared bool | int | None | NotGiven\n\nSync sample events to log directory so that users on other systems can see log updates in realtime (defaults to no syncing). Specify True to sync every 10 seconds, otherwise an integer to sync every n seconds.\n\nbundle_dir str | None | NotGiven\n\nIf specified, the log viewer and logs generated by this eval set will be bundled into this directory. Relative paths will be resolved relative to the config file (when using the CLI) or base_dir arg (when using the API).\n\nbundle_overwrite bool | None | NotGiven\n\nWhether to overwrite files in the bundle_dir. (defaults to False).\n\nlog_dir_allow_dirty bool | None | NotGiven\n\nIf True, allow the log directory to contain unrelated logs. If False, ensure that the log directory only contains logs for tasks in this eval set (defaults to False).\n\neval_set_id str | None | NotGiven\n\nID for the eval set. If not specified, a unique ID will be generated.\n\nbundle_url_mappings dict[str, str] | None | NotGiven\n\nReplacements applied to bundle_dir to generate a URL. If provided and bundle_dir is set, the mapped URL will be written to stdout.\n\n\n\n\n\n\nConfiguration for a Scorer.\n\nSource\n\nclass FlowScorer(BaseModel, extra=\"forbid\")\n\n\n\nname str | None | NotGiven\n\nName of the scorer. Required to be set by the time the scorer is created.\n\nargs CreateArgs | None | NotGiven\n\nAdditional args to pass to scorer constructor.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the scorer.\n\n\n\n\n\n\nConfiguration for a Solver.\n\nSource\n\nclass FlowSolver(BaseModel, extra=\"forbid\")\n\n\n\nname str | None | NotGiven\n\nName of the solver. Required to be set by the time the solver is created.\n\nargs CreateArgs | None | NotGiven\n\nAdditional args to pass to solver constructor.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the solver.\n\n\n\n\n\n\nConfiguration for an evaluation task.\nTasks are the basis for defining and running evaluations.\n\nSource\n\nclass FlowTask(BaseModel, extra=\"forbid\")\n\n\n\nname str | None | NotGiven\n\nTask name. Any of registry name (“inspect_evals/mbpp”), file name (“./my_task.py”), or a file name and attr (“./my_task.py@task_name”). Required to be set by the time the task is created.\n\nargs CreateArgs | None | NotGiven\n\nAdditional args to pass to task constructor\n\nsolver str | FlowSolver | Sequence[str | FlowSolver] | FlowAgent | None | NotGiven\n\nSolver or list of solvers. Defaults to generate(), a normal call to the model.\n\nscorer str | FlowScorer | Sequence[str | FlowScorer] | None | NotGiven\n\nScorer or list of scorers used to evaluate model output.\n\nmodel str | FlowModel | None | NotGiven\n\nDefault model for task (Optional, defaults to eval model).\n\nconfig GenerateConfig | NotGiven\n\nModel generation config for default model (does not apply to model roles). Will override config settings on the FlowSpec. Will be overridden by settings on the FlowModel.\n\nmodel_roles ModelRolesConfig | None | NotGiven\n\nNamed roles for use in get_model().\n\nsandbox SandboxEnvironmentType | None | NotGiven\n\nSandbox environment type (or optionally a str or tuple with a shorthand spec)\n\napproval str | ApprovalPolicyConfig | None | NotGiven\n\nTool use approval policies. Either a path to an approval policy config file or an approval policy config. Defaults to no approval policy.\n\nepochs int | FlowEpochs | None | NotGiven\n\nEpochs to repeat samples for and optional score reducer function(s) used to combine sample scores (defaults to “mean”)\n\nfail_on_error bool | float | None | NotGiven\n\nTrue to fail on first sample error (default); False to never fail on sample errors; Value between 0 and 1 to fail if a proportion of total samples fails. Value greater than 1 to fail eval if a count of samples fails.\n\ncontinue_on_fail bool | None | NotGiven\n\nTrue to continue running and only fail at the end if the fail_on_error condition is met. False to fail eval immediately when the fail_on_error condition is met (default).\n\nmessage_limit int | None | NotGiven\n\nLimit on total messages used for each sample.\n\ntoken_limit int | None | NotGiven\n\nLimit on total tokens used for each sample.\n\ntime_limit int | None | NotGiven\n\nLimit on clock time (in seconds) for samples.\n\nworking_limit int | None | NotGiven\n\nLimit on working time (in seconds) for sample. Working time includes model generation, tool calls, etc. but does not include time spent waiting on retries or shared resources.\n\nversion int | str | NotGiven\n\nVersion of task (to distinguish evolutions of the task spec or breaking changes to it)\n\nmetadata dict[str, Any] | None | NotGiven\n\nAdditional metadata to associate with the task.\n\nsample_id str | int | Sequence[str | int] | None | NotGiven\n\nEvaluate specific sample(s) from the dataset.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the task.\n\nmodel_name str | None | NotGiven\n\nGet the model name from the model field.\nReturns: The model name if set, otherwise None.",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  },
  {
    "objectID": "reference/inspect_flow.html#types",
    "href": "reference/inspect_flow.html#types",
    "title": "inspect_flow",
    "section": "",
    "text": "Configuration for an Agent.\n\nSource\n\nclass FlowAgent(BaseModel, extra=\"forbid\")\n\n\n\nname str | None | NotGiven\n\nName of the agent. Required to be set by the time the agent is created.\n\nargs CreateArgs | None | NotGiven\n\nAdditional args to pass to agent constructor.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the agent.\n\ntype Literal['agent'] | None\n\nType needed to differentiated solvers and agents in solver lists.\n\n\n\n\n\n\nDefault field values for Inspect objects. Will be overriden by more specific settings.\n\nSource\n\nclass FlowDefaults(BaseModel, extra=\"forbid\")\n\n\n\nconfig GenerateConfig | None | NotGiven\n\nDefault model generation options. Will be overriden by settings on the FlowModel and FlowTask.\n\nagent FlowAgent | None | NotGiven\n\nField defaults for agents.\n\nagent_prefix dict[str, FlowAgent] | None | NotGiven\n\nAgent defaults for agent name prefixes. E.g. {‘inspect/’: FAgent(…)}\n\nmodel FlowModel | None | NotGiven\n\nField defaults for models.\n\nmodel_prefix dict[str, FlowModel] | None | NotGiven\n\nModel defaults for model name prefixes. E.g. {‘openai/’: FModel(…)}\n\nsolver FlowSolver | None | NotGiven\n\nField defaults for solvers.\n\nsolver_prefix dict[str, FlowSolver] | None | NotGiven\n\nSolver defaults for solver name prefixes. E.g. {‘inspect/’: FSolver(…)}\n\ntask FlowTask | None | NotGiven\n\nField defaults for tasks.\n\ntask_prefix dict[str, FlowTask] | None | NotGiven\n\nTask defaults for task name prefixes. E.g. {‘inspect_evals/’: FTask(…)}\n\n\n\n\n\n\nConfiguration for flow dependencies to install in the venv.\n\nSource\n\nclass FlowDependencies(BaseModel, extra=\"forbid\")\n\n\n\ndependency_file Literal['auto', 'no_file'] | str | None | NotGiven\n\nPath to a dependency file (either requirements.txt or pyproject.toml) to use to create the virtual environment. If ‘auto’, will search the path starting from the same directory as the config file (when using the CLI) or base_dir arg (when using the API) looking for pyproject.toml or requirements.txt files. If ‘no_file’, no dependency file will be used. Defaults to ‘auto’.\n\nadditional_dependencies str | Sequence[str] | None | NotGiven\n\nDependencies to pip install. E.g. PyPI package specifiers or Git repository URLs.\n\nauto_detect_dependencies bool | None | NotGiven\n\nIf True, automatically detect and install dependencies from names of objects in the config (defaults to True). For example, if a model name starts with ‘openai/’, the ‘openai’ package will be installed. If a task name is ‘inspect_evals/mmlu’ then the ‘inspect-evals’ package will be installed.\n\n\n\n\n\n\nConfiguration for task epochs.\nNumber of epochs to repeat samples over and optionally one or more reducers used to combine scores from samples across epochs. If not specified the “mean” score reducer is used.\n\nSource\n\nclass FlowEpochs(BaseModel, extra=\"forbid\")\n\n\n\nepochs int\n\nNumber of epochs.\n\nreducer str | Sequence[str] | None | NotGiven\n\nOne or more reducers used to combine scores from samples across epochs (defaults to “mean”)\n\n\n\n\n\n\nTop-level flow specification.\n\nSource\n\nclass FlowSpec(BaseModel, extra=\"forbid\")\n\n\n\nincludes Sequence[str] | None | NotGiven\n\nList of other flow configs to include. Relative paths will be resolved relative to the config file (when using the CLI) or base_dir arg (when using the API). In addition to this list of explicit files to include, any _flow.py files in the same directory or any parent directory of the config file (when using the CLI) or base_dir arg (when using the API) will also be included automatically.\n\nlog_dir str | None | NotGiven\n\nOutput path for logging results (required to ensure that a unique storage scope is assigned). Must be set before running the flow spec. Relative paths will be resolved relative to the config file (when using the CLI) or base_dir arg (when using the API).\n\nlog_dir_create_unique bool | None | NotGiven\n\nIf True, create a new log directory by appending an _ and numeric suffix if the specified log_dir already exists. If the directory exists and has a _numeric suffix, that suffix will be incremented. If False, use the existing log_dir (which must be empty or have log_dir_allow_dirty=True). Defaults to False.\n\npython_version str | None | NotGiven\n\nPython version to use in the flow virtual environment (e.g. ‘3.11’)\n\noptions FlowOptions | None | NotGiven\n\nArguments for calls to eval_set.\n\ndependencies FlowDependencies | None | NotGiven\n\nDependencies to install in the venv. Defaults to auto-detecting dependencies from pyproject.toml, requirements.txt, and object names in the config.\n\nenv dict[str, str] | None | NotGiven\n\nEnvironment variables to set when running tasks.\n\ndefaults FlowDefaults | None | NotGiven\n\nDefaults values for Inspect objects.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the model.\n\ntasks Sequence[str | FlowTask] | None | NotGiven\n\nTasks to run\n\n\n\n\n\n\nConfiguration for a Model.\n\nSource\n\nclass FlowModel(BaseModel, extra=\"forbid\")\n\n\n\nname str | None | NotGiven\n\nName of the model to use. Required to be set by the time the model is created.\n\nrole str | None | NotGiven\n\nOptional named role for model (e.g. for roles specified at the task or eval level). Provide a default as a fallback in the case where the role hasn’t been externally specified.\n\ndefault str | None | NotGiven\n\nOptional. Fallback model in case the specified model or role is not found. Should be a fully qualified model name (e.g. openai/gpt-4o).\n\nconfig GenerateConfig | None | NotGiven\n\nConfiguration for model. Config values will be override settings on the FlowTask and FlowSpec.\n\nbase_url str | None | NotGiven\n\nOptional. Alternate base URL for model.\n\napi_key str | None | NotGiven\n\nOptional. API key for model.\n\nmemoize bool | None | NotGiven\n\nUse/store a cached version of the model based on the parameters to get_model(). Defaults to True.\n\nmodel_args CreateArgs | None | NotGiven\n\nAdditional args to pass to model constructor.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the model.\n\n\n\n\n\n\nEvaluation options.\n\nSource\n\nclass FlowOptions(BaseModel, extra=\"forbid\")\n\n\n\nretry_attempts int | None | NotGiven\n\nMaximum number of retry attempts before giving up (defaults to 10).\n\nretry_wait float | None | NotGiven\n\nTime to wait between attempts, increased exponentially (defaults to 30, resulting in waits of 30, 60, 120, 240, etc.). Wait time per-retry will in no case be longer than 1 hour.\n\nretry_connections float | None | NotGiven\n\nReduce max_connections at this rate with each retry (defaults to 1.0, which results in no reduction).\n\nretry_cleanup bool | None | NotGiven\n\nCleanup failed log files after retries (defaults to True).\n\nsandbox SandboxEnvironmentType | None | NotGiven\n\nSandbox environment type (or optionally a str or tuple with a shorthand spec).\n\nsandbox_cleanup bool | None | NotGiven\n\nCleanup sandbox environments after task completes (defaults to True).\n\ntags Sequence[str] | None | NotGiven\n\nTags to associate with this evaluation run.\n\nmetadata dict[str, Any] | None | NotGiven\n\nMetadata to associate with this evaluation run.\n\ntrace bool | None | NotGiven\n\nTrace message interactions with evaluated model to terminal.\n\ndisplay DisplayType | None | NotGiven\n\nTask display type (defaults to ‘full’).\n\napproval str | ApprovalPolicyConfig | None | NotGiven\n\nTool use approval policies. Either a path to an approval policy config file or a list of approval policies. Defaults to no approval policy.\n\nscore bool | None | NotGiven\n\nScore output (defaults to True).\n\nlog_level str | None | NotGiven\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”).\n\nlog_level_transcript str | None | NotGiven\n\nLevel for logging to the log file (defaults to “info”).\n\nlog_format Literal['eval', 'json'] | None | NotGiven\n\nFormat for writing log files (defaults to “eval”, the native high-performance format).\n\nlimit int | None | NotGiven\n\nLimit evaluated samples (defaults to all samples).\n\nsample_shuffle bool | int | None | NotGiven\n\nShuffle order of samples (pass a seed to make the order deterministic).\n\nfail_on_error bool | float | None | NotGiven\n\nTrue to fail on first sample error(default); False to never fail on sample errors; Value between 0 and 1 to fail if a proportion of total samples fails. Value greater than 1 to fail eval if a count of samples fails.\n\ncontinue_on_fail bool | None | NotGiven\n\nTrue to continue running and only fail at the end if the fail_on_error condition is met. False to fail eval immediately when the fail_on_error condition is met (default).\n\nretry_on_error int | None | NotGiven\n\nNumber of times to retry samples if they encounter errors (defaults to 3).\n\ndebug_errors bool | None | NotGiven\n\nRaise task errors (rather than logging them) so they can be debugged (defaults to False).\n\nmax_samples int | None | NotGiven\n\nMaximum number of samples to run in parallel (default is max_connections).\n\nmax_tasks int | None | NotGiven\n\nMaximum number of tasks to run in parallel (defaults is 10).\n\nmax_subprocesses int | None | NotGiven\n\nMaximum number of subprocesses to run in parallel (default is os.cpu_count()).\n\nmax_sandboxes int | None | NotGiven\n\nMaximum number of sandboxes (per-provider) to run in parallel.\n\nlog_samples bool | None | NotGiven\n\nLog detailed samples and scores (defaults to True).\n\nlog_realtime bool | None | NotGiven\n\nLog events in realtime (enables live viewing of samples in inspect view) (defaults to True).\n\nlog_images bool | None | NotGiven\n\nLog base64 encoded version of images, even if specified as a filename or URL (defaults to False).\n\nlog_buffer int | None | NotGiven\n\nNumber of samples to buffer before writing log file. If not specified, an appropriate default for the format and filesystem is chosen (10 for most all cases, 100 for JSON logs on remote filesystems).\n\nlog_shared bool | int | None | NotGiven\n\nSync sample events to log directory so that users on other systems can see log updates in realtime (defaults to no syncing). Specify True to sync every 10 seconds, otherwise an integer to sync every n seconds.\n\nbundle_dir str | None | NotGiven\n\nIf specified, the log viewer and logs generated by this eval set will be bundled into this directory. Relative paths will be resolved relative to the config file (when using the CLI) or base_dir arg (when using the API).\n\nbundle_overwrite bool | None | NotGiven\n\nWhether to overwrite files in the bundle_dir. (defaults to False).\n\nlog_dir_allow_dirty bool | None | NotGiven\n\nIf True, allow the log directory to contain unrelated logs. If False, ensure that the log directory only contains logs for tasks in this eval set (defaults to False).\n\neval_set_id str | None | NotGiven\n\nID for the eval set. If not specified, a unique ID will be generated.\n\nbundle_url_mappings dict[str, str] | None | NotGiven\n\nReplacements applied to bundle_dir to generate a URL. If provided and bundle_dir is set, the mapped URL will be written to stdout.\n\n\n\n\n\n\nConfiguration for a Scorer.\n\nSource\n\nclass FlowScorer(BaseModel, extra=\"forbid\")\n\n\n\nname str | None | NotGiven\n\nName of the scorer. Required to be set by the time the scorer is created.\n\nargs CreateArgs | None | NotGiven\n\nAdditional args to pass to scorer constructor.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the scorer.\n\n\n\n\n\n\nConfiguration for a Solver.\n\nSource\n\nclass FlowSolver(BaseModel, extra=\"forbid\")\n\n\n\nname str | None | NotGiven\n\nName of the solver. Required to be set by the time the solver is created.\n\nargs CreateArgs | None | NotGiven\n\nAdditional args to pass to solver constructor.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the solver.\n\n\n\n\n\n\nConfiguration for an evaluation task.\nTasks are the basis for defining and running evaluations.\n\nSource\n\nclass FlowTask(BaseModel, extra=\"forbid\")\n\n\n\nname str | None | NotGiven\n\nTask name. Any of registry name (“inspect_evals/mbpp”), file name (“./my_task.py”), or a file name and attr (“./my_task.py@task_name”). Required to be set by the time the task is created.\n\nargs CreateArgs | None | NotGiven\n\nAdditional args to pass to task constructor\n\nsolver str | FlowSolver | Sequence[str | FlowSolver] | FlowAgent | None | NotGiven\n\nSolver or list of solvers. Defaults to generate(), a normal call to the model.\n\nscorer str | FlowScorer | Sequence[str | FlowScorer] | None | NotGiven\n\nScorer or list of scorers used to evaluate model output.\n\nmodel str | FlowModel | None | NotGiven\n\nDefault model for task (Optional, defaults to eval model).\n\nconfig GenerateConfig | NotGiven\n\nModel generation config for default model (does not apply to model roles). Will override config settings on the FlowSpec. Will be overridden by settings on the FlowModel.\n\nmodel_roles ModelRolesConfig | None | NotGiven\n\nNamed roles for use in get_model().\n\nsandbox SandboxEnvironmentType | None | NotGiven\n\nSandbox environment type (or optionally a str or tuple with a shorthand spec)\n\napproval str | ApprovalPolicyConfig | None | NotGiven\n\nTool use approval policies. Either a path to an approval policy config file or an approval policy config. Defaults to no approval policy.\n\nepochs int | FlowEpochs | None | NotGiven\n\nEpochs to repeat samples for and optional score reducer function(s) used to combine sample scores (defaults to “mean”)\n\nfail_on_error bool | float | None | NotGiven\n\nTrue to fail on first sample error (default); False to never fail on sample errors; Value between 0 and 1 to fail if a proportion of total samples fails. Value greater than 1 to fail eval if a count of samples fails.\n\ncontinue_on_fail bool | None | NotGiven\n\nTrue to continue running and only fail at the end if the fail_on_error condition is met. False to fail eval immediately when the fail_on_error condition is met (default).\n\nmessage_limit int | None | NotGiven\n\nLimit on total messages used for each sample.\n\ntoken_limit int | None | NotGiven\n\nLimit on total tokens used for each sample.\n\ntime_limit int | None | NotGiven\n\nLimit on clock time (in seconds) for samples.\n\nworking_limit int | None | NotGiven\n\nLimit on working time (in seconds) for sample. Working time includes model generation, tool calls, etc. but does not include time spent waiting on retries or shared resources.\n\nversion int | str | NotGiven\n\nVersion of task (to distinguish evolutions of the task spec or breaking changes to it)\n\nmetadata dict[str, Any] | None | NotGiven\n\nAdditional metadata to associate with the task.\n\nsample_id str | int | Sequence[str | int] | None | NotGiven\n\nEvaluate specific sample(s) from the dataset.\n\nflow_metadata dict[str, Any] | None | NotGiven\n\nOptional. Metadata stored in the flow config. Not passed to the task.\n\nmodel_name str | None | NotGiven\n\nGet the model name from the model field.\nReturns: The model name if set, otherwise None.",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  },
  {
    "objectID": "reference/inspect_flow.html#decorators",
    "href": "reference/inspect_flow.html#decorators",
    "title": "inspect_flow",
    "section": "Decorators",
    "text": "Decorators\n\nafter_load\nDecorator to mark a function to be called after a FlowSpec is loaded.\nThe decorated function should have the signature (args are all optional and may be omitted): def after_flow_spec_loaded( spec: FlowSpec, files: list[str], ) -&gt; None:\nspec: The loaded FlowSpec.\nfiles: List of file paths that were loaded to create the FlowSpec.\n…\n\nSource\n\ndef after_load(func: Callable) -&gt; Callable\n\nfunc Callable\n\nThe function to decorate.",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  },
  {
    "objectID": "reference/inspect_flow.html#functions",
    "href": "reference/inspect_flow.html#functions",
    "title": "inspect_flow",
    "section": "Functions",
    "text": "Functions\n\nagents_matrix\nCreate a list of agents from the product of lists of field values.\n\nSource\n\ndef agents_matrix(\n    *,\n    agent: str | FlowAgent | Sequence[str | FlowAgent],\n    **kwargs: Unpack[FlowAgentMatrixDict],\n) -&gt; list[FlowAgent]\n\nagent str | FlowAgent | Sequence[str | FlowAgent]\n\nThe agent or list of agents to matrix.\n\n**kwargs Unpack[FlowAgentMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nagents_with\nSet fields on a list of agents.\n\nSource\n\ndef agents_with(\n    *,\n    agent: str | FlowAgent | Sequence[str | FlowAgent],\n    **kwargs: Unpack[FlowAgentDict],\n) -&gt; list[FlowAgent]\n\nagent str | FlowAgent | Sequence[str | FlowAgent]\n\nThe agent or list of agents to set fields on.\n\n**kwargs Unpack[FlowAgentDict]\n\nThe fields to set on each agent.\n\n\n\n\nconfigs_matrix\nCreate a list of generate configs from the product of lists of field values.\n\nSource\n\ndef configs_matrix(\n    *,\n    config: GenerateConfig | Sequence[GenerateConfig] | None = None,\n    **kwargs: Unpack[GenerateConfigMatrixDict],\n) -&gt; list[GenerateConfig]\n\nconfig GenerateConfig | Sequence[GenerateConfig] | None\n\nThe config or list of configs to matrix.\n\n**kwargs Unpack[GenerateConfigMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nconfigs_with\nSet fields on a list of generate configs.\n\nSource\n\ndef configs_with(\n    *,\n    config: GenerateConfig | Sequence[GenerateConfig],\n    **kwargs: Unpack[GenerateConfigDict],\n) -&gt; list[GenerateConfig]\n\nconfig GenerateConfig | Sequence[GenerateConfig]\n\nThe config or list of configs to set fields on.\n\n**kwargs Unpack[GenerateConfigDict]\n\nThe fields to set on each config.\n\n\n\n\nmerge\nMerge two flow objects.\n\nSource\n\ndef merge(base: _T, add: _T) -&gt; _T\n\nbase _T\n\nThe base object.\n\nadd _T\n\nThe object to merge into the base. Values in this object will override those in the base.\n\n\n\n\nmodels_matrix\nCreate a list of models from the product of lists of field values.\n\nSource\n\ndef models_matrix(\n    *,\n    model: str | FlowModel | Sequence[str | FlowModel],\n    **kwargs: Unpack[FlowModelMatrixDict],\n) -&gt; list[FlowModel]\n\nmodel str | FlowModel | Sequence[str | FlowModel]\n\nThe model or list of models to matrix.\n\n**kwargs Unpack[FlowModelMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nmodels_with\nSet fields on a list of models.\n\nSource\n\ndef models_with(\n    *,\n    model: str | FlowModel | Sequence[str | FlowModel],\n    **kwargs: Unpack[FlowModelDict],\n) -&gt; list[FlowModel]\n\nmodel str | FlowModel | Sequence[str | FlowModel]\n\nThe model or list of models to set fields on.\n\n**kwargs Unpack[FlowModelDict]\n\nThe fields to set on each model.\n\n\n\n\nsolvers_matrix\nCreate a list of solvers from the product of lists of field values.\n\nSource\n\ndef solvers_matrix(\n    *,\n    solver: str | FlowSolver | Sequence[str | FlowSolver],\n    **kwargs: Unpack[FlowSolverMatrixDict],\n) -&gt; list[FlowSolver]\n\nsolver str | FlowSolver | Sequence[str | FlowSolver]\n\nThe solver or list of solvers to matrix.\n\n**kwargs Unpack[FlowSolverMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nsolvers_with\nSet fields on a list of solvers.\n\nSource\n\ndef solvers_with(\n    *,\n    solver: str | FlowSolver | Sequence[str | FlowSolver],\n    **kwargs: Unpack[FlowSolverDict],\n) -&gt; list[FlowSolver]\n\nsolver str | FlowSolver | Sequence[str | FlowSolver]\n\nThe solver or list of solvers to set fields on.\n\n**kwargs Unpack[FlowSolverDict]\n\nThe fields to set on each solver.\n\n\n\n\ntasks_matrix\nCreate a list of tasks from the product of lists of field values.\n\nSource\n\ndef tasks_matrix(\n    *,\n    task: str | FlowTask | Sequence[str | FlowTask],\n    **kwargs: Unpack[FlowTaskMatrixDict],\n) -&gt; list[FlowTask]\n\ntask str | FlowTask | Sequence[str | FlowTask]\n\nThe task or list of tasks to matrix.\n\n**kwargs Unpack[FlowTaskMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\ntasks_with\nSet fields on a list of tasks.\n\nSource\n\ndef tasks_with(\n    *,\n    task: str | FlowTask | Sequence[str | FlowTask],\n    **kwargs: Unpack[FlowTaskDict],\n) -&gt; list[FlowTask]\n\ntask str | FlowTask | Sequence[str | FlowTask]\n\nThe task or list of tasks to set fields on.\n\n**kwargs Unpack[FlowTaskDict]\n\nThe fields to set on each task.",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  }
]