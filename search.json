[
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Reference",
    "section": "",
    "text": "Python API\n\n\n\n\n\n\n\ninspect_flow\nCore types and functions.\n\n\ninspect_flow.api\nOutput and run a job.\n\n\n\n\n\nFlow CLI\n\n\n\n\n\n\n\nflow config\nOutput job config.\n\n\nflow run\nRun a job.",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "reference/flow_run.html",
    "href": "reference/flow_run.html",
    "title": "flow run",
    "section": "",
    "text": "Run a job\n\nUsage\nflow run [OPTIONS] CONFIG_FILE\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--dry-run\nboolean\nDo not run job, but show a count of tasks that would be run.\nFalse\n\n\n--log-dir-create-unique\nboolean\nIf set, create a new log directory by appending an _ and numeric suffix if the specified log_dir already exists. If the directory exists and has a _numeric suffix, that suffix will be incremented. If not set, use the existing log_dir (which must be empty or have log_dir_allow_dirty=True).\nFalse\n\n\n--log-dir\ndirectory\nSet the log directory. Will override the log_dir specified in the config.\nNone\n\n\n--limit\ninteger\nLimit the number of samples to run.\nNone\n\n\n--var\ntext\nSet variables accessible to code executing in the config file through the variable __flow_vars__: task_min_priority = __flow_vars__.get(\"task_min_priority\", 1) Examples: --var task_min_priority=2 If the same key is provided multiple times, later values will override earlier ones.\nNone\n\n\n--set, -s\ntext\nSet config overrides. Examples: --set defaults.solver.args.tool_calls=none --set options.limit=10 --set options.metadata={\"key1\": \"val1\", \"key2\": \"val2\"} The specified value may be a string or json parsable list or dict. If string is provided then it will be appended to existing list values. If json list or dict is provided then it will replace existing values. If the same key is provided multiple times, later values will override earlier ones.\nNone\n\n\n--help\nboolean\nShow this message and exit.\nFalse",
    "crumbs": [
      "Reference",
      "Inspect CLI",
      "flow run"
    ]
  },
  {
    "objectID": "reference/flow_config.html",
    "href": "reference/flow_config.html",
    "title": "flow config",
    "section": "",
    "text": "Output config\n\nUsage\nflow config [OPTIONS] CONFIG_FILE\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--resolve\nboolean\nFully resolve the config. Will create a venv and create all objects.\nFalse\n\n\n--log-dir-create-unique\nboolean\nIf set, create a new log directory by appending an _ and numeric suffix if the specified log_dir already exists. If the directory exists and has a _numeric suffix, that suffix will be incremented. If not set, use the existing log_dir (which must be empty or have log_dir_allow_dirty=True).\nFalse\n\n\n--log-dir\ndirectory\nSet the log directory. Will override the log_dir specified in the config.\nNone\n\n\n--limit\ninteger\nLimit the number of samples to run.\nNone\n\n\n--var\ntext\nSet variables accessible to code executing in the config file through the variable __flow_vars__: task_min_priority = __flow_vars__.get(\"task_min_priority\", 1) Examples: --var task_min_priority=2 If the same key is provided multiple times, later values will override earlier ones.\nNone\n\n\n--set, -s\ntext\nSet config overrides. Examples: --set defaults.solver.args.tool_calls=none --set options.limit=10 --set options.metadata={\"key1\": \"val1\", \"key2\": \"val2\"} The specified value may be a string or json parsable list or dict. If string is provided then it will be appended to existing list values. If json list or dict is provided then it will replace existing values. If the same key is provided multiple times, later values will override earlier ones.\nNone\n\n\n--help\nboolean\nShow this message and exit.\nFalse",
    "crumbs": [
      "Reference",
      "Inspect CLI"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inspect Flow",
    "section": "",
    "text": "Inspect Flow is a workflow orchestration tool for Inspect AI that enables you to run evaluations at scale with repeatability and maintainability.\nWhy Inspect Flow? As evaluation workflows grow in complexity—running multiple tasks across different models with varying parameters—managing these experiments becomes challenging. Inspect Flow addresses this by providing:\n\nDeclarative Configuration: Define complex evaluations with tasks, models, and parameters in type-safe schemas\nRepeatable & Shareable: Encapsulated definitions of tasks, models, configurations, and Python dependencies ensure experiments can be reliably repeated and shared\nIncremental Execution: Add new models, tasks, or configurations to existing results without re-running completed work\nParameter Sweeping: Matrix patterns for systematic exploration across tasks, models, and hyperparameters\n\nInspect Flow is designed for researchers and engineers running systematic AI evaluations who need to scale beyond ad-hoc scripts.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Inspect Flow",
    "section": "",
    "text": "Inspect Flow is a workflow orchestration tool for Inspect AI that enables you to run evaluations at scale with repeatability and maintainability.\nWhy Inspect Flow? As evaluation workflows grow in complexity—running multiple tasks across different models with varying parameters—managing these experiments becomes challenging. Inspect Flow addresses this by providing:\n\nDeclarative Configuration: Define complex evaluations with tasks, models, and parameters in type-safe schemas\nRepeatable & Shareable: Encapsulated definitions of tasks, models, configurations, and Python dependencies ensure experiments can be reliably repeated and shared\nIncremental Execution: Add new models, tasks, or configurations to existing results without re-running completed work\nParameter Sweeping: Matrix patterns for systematic exploration across tasks, models, and hyperparameters\n\nInspect Flow is designed for researchers and engineers running systematic AI evaluations who need to scale beyond ad-hoc scripts.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Inspect Flow",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\nNotePrerequisites\n\n\n\nBefore using Inspect Flow, you should:\n\nHave familiarity with Inspect AI\nHave an existing Inspect evaluation or use one from inspect-evals\n\n\n\n\nInstallation\nInstall the inspect_flow package from PyPI as follows:\npip install inspect-flow\n\n\nSet up API keys\nYou’ll need API keys for the model providers you want to use. Set the relevant provider API key in your .env file or export it in your shell:\n\nOpenAIAnthropicGoogleGrokMistralHugging Face\n\n\nexport OPENAI_API_KEY=your-openai-api-key\n\n\nexport ANTHROPIC_API_KEY=your-anthropic-api-key\n\n\nexport GOOGLE_API_KEY=your-google-api-key\n\n\nexport GROK_API_KEY=your-grok-api-key\n\n\nexport MISTRAL_API_KEY=your-mistral-api-key\n\n\nexport HF_TOKEN=your-hf-token\n\n\n\n\n\nOptional: VS Code extension\nOptionally install the Inspect AI VS Code Extension which includes features for viewing evaluation log files.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#basic-examples",
    "href": "index.html#basic-examples",
    "title": "Inspect Flow",
    "section": "Basic Examples",
    "text": "Basic Examples\nLet’s walk through creating your first Flow configuration. We’ll use FlowJob (the entrypoint class) and FlowTask to define evaluations.\n\n\n\n\n\n\nTipCore Components Reference\n\n\n\n\n\n\ntypes.FlowJob — Pydantic class that encapsulates the declarative description of a Flow job.\ntypes.FlowTask — Pydantic class abstraction on top of Inspect AI Task.\ntypes.FlowModel — Pydantic class abstraction on top of Inspect AI Model.\ntypes.FlowGenerateConfig — Pydantic class abstraction on top of Inspect AI GenerateConfig.\ntasks_matrix — Helper function for parameter sweeping to generate a list of tasks with all parameter combinations.\nmodels_matrix — Helper function for parameter sweeping to generate a list of models with all parameter combinations.\nconfigs_matrix — Helper function for parameter sweeping to generate a list of GenerateConfig with all parameter combinations.\n\n\n\n\nFlowJob is the main entrypoint for defining evaluation runs. At its core, it takes a list of tasks to run. Here’s a simple example that runs two evaluations:\n\n\nconfig.py\n\nfrom inspect_flow import FlowJob, FlowTask\n\nFlowJob(\n    log_dir=\"logs\",\n    dependencies=[\"inspect-evals\"],\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n        ),\n        FlowTask(\n            name=\"inspect_evals/mmlu_0_shot\",\n            model=\"openai/gpt-4o\",\n        ),\n    ],\n)\n\nTo run the evaluations, run the following command in your shell. This will create a virtual environment for this job run and install the dependencies. Note that model dependencies (like the openai Python package) are inferred and installed automatically.\nflow run config.py\nThis will run both tasks and display progress in your terminal.\n\n\n\nProgress bar in terminal\n\n\n\nMatrix Functions\nOften you’ll want to evaluate multiple tasks across multiple models. Rather than manually defining every combination, use tasks_matrix to generate all task-model pairs:\n\n\nmatrix.py\n\nfrom inspect_flow import FlowJob, tasks_matrix\n\nFlowJob(\n    log_dir=\"logs\",\n    dependencies=[\"inspect-evals\"],\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=[\n            \"openai/gpt-5\",\n            \"openai/gpt-5-mini\",\n        ],\n    ),\n)\n\nTo preview the expanded config before running it, you can run the following command in your shell to ensure the generated config is the one that you intend to run.\nflow config matrix.py\nThis command outputs the expanded configuration showing all 4 task-model combinations (2 tasks × 2 models).\n\n\nmatrix.yml\n\nlog_dir: logs\ndependencies:\n- inspect-evals\ntasks:\n- name: inspect_evals/gpqa_diamond\n  model:\n    name: openai/gpt-5\n- name: inspect_evals/gpqa_diamond\n  model:\n    name: openai/gpt-5-mini\n- name: inspect_evals/mmlu_0_shot\n  model:\n    name: openai/gpt-5\n- name: inspect_evals/mmlu_0_shot\n  model:\n    name: openai/gpt-5-mini\n\ntasks_matrix and models_matrix are powerful functions that can operate on multiple levels of nested matrixes which enable sophisticated parameter sweeping. Let’s say you want to explore different reasoning efforts across models—you can achieve this with the models_matrix function.\n\n\nmodels_matrix.py\n\nfrom inspect_flow import FlowGenerateConfig, FlowJob, models_matrix, tasks_matrix\n\nFlowJob(\n    log_dir=\"logs\",\n    dependencies=[\"inspect-evals\"],\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=models_matrix(\n            model=[\n                \"openai/gpt-5\",\n                \"openai/gpt-5-mini\",\n            ],\n            config=[\n                FlowGenerateConfig(reasoning_effort=\"minimal\"),\n                FlowGenerateConfig(reasoning_effort=\"low\"),\n                FlowGenerateConfig(reasoning_effort=\"medium\"),\n                FlowGenerateConfig(reasoning_effort=\"high\"),\n            ],\n        ),\n    ),\n)\n\nFor even more concise parameter sweeping, use configs_matrix to generate configuration variants. This produces the same 16 evaluations (2 tasks × 2 models × 4 reasoning levels) as above, but with less boilerplate:\n\n\nconfigs_matrix.py\n\nfrom inspect_flow import FlowJob, configs_matrix, models_matrix, tasks_matrix\n\nFlowJob(\n    log_dir=\"logs\",\n    dependencies=[\"inspect-evals\"],\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=models_matrix(\n            model=[\n                \"openai/gpt-5\",\n                \"openai/gpt-5-mini\",\n            ],\n            config=configs_matrix(\n                reasoning_effort=[\"minimal\", \"low\", \"medium\", \"high\"],\n            ),\n        ),\n    ),\n)\n\nTo run the config:\nflow run matrix.py\nThis will run all 16 evaluations (2 tasks × 2 models × 4 reasoning levels). When complete, you’ll find a link to the logs at the bottom of the task results summary.\n\n\n\nLog path printed in terminal\n\n\nTo view logs interactively, run:\ninspect view\n\n\n\nInspect View",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#learning-more",
    "href": "index.html#learning-more",
    "title": "Inspect Flow",
    "section": "Learning More",
    "text": "Learning More\nSee the following articles to learn more about using Flow:\n\nUsage: End-to-end guide for developing and running a Flow job.\nReference: Detailed documentation on the Flow Python API and CLI commands.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "usage.html",
    "href": "usage.html",
    "title": "Using Flow",
    "section": "",
    "text": "Inspect Flow mirrors Inspect AI’s object model with corresponding Flow types:\n\nFlowJob — The top-level job definition. Under the hood it translates to an Inspect AI Eval Set. Contains a list of tasks and job settings.\nFlowTask — Configuration for a single evaluation task. Maps to Inspect AI Task parameters.\nFlowModel — Model configuration including API settings and generation settings. Maps to Inspect AI Model.\nFlowGenerateConfig — Model generation parameters. Maps to Inspect AI GenerateConfig.\nFlowSolver and FlowAgent — Solver and agent chain configuration. Map to Inspect AI Solver and Agent.\nFlowOptions — Runtime execution options. Maps to Inspect AI eval_set() parameters.\nFlowDefaults — System for setting default values across tasks, models, solvers, and agents.\n\n\n\n\nFlowJob is the primary interface for defining Flow workflows. All Flow operations—including parameter sweeps and matrix expansions—ultimately produce a list of tasks that FlowJob executes.\nRequired fields:\n\nlog_dir — Output path for logging results. Must be set before running the flow job. Supports S3 paths (e.g., s3://bucket/path)\n\nOptional fields:\n\n\n\n\n\n\n\n\nField\nDescription\nDefault\n\n\n\n\ntasks\nList of FlowTask objects defining the evaluations to run\nNone\n\n\nincludes\nList of other flow configs to include (paths as strings or FlowInclude objects)\nNone\n\n\nlog_dir_create_unique\nIf True, append numeric suffix to log_dir if it exists. If False, reuse existing directory (must be empty or have log_dir_allow_dirty=True)\nFalse\n\n\npython_version\nPython version for the isolated virtual environment (e.g., \"3.11\")\nSame as current environment\n\n\ndependencies\nPyPI packages, Git URLs, or local paths to install\nNone\n\n\nenv\nEnvironment variables to set when running tasks\nNone\n\n\noptions\nRuntime options (see FlowOptions reference)\nNone\n\n\ndefaults\nDefault values applied across tasks, models, and solvers\nNone\n\n\nflow_metadata\nMetadata stored in the flow config (not passed to Inspect AI)\nNone",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#core-concepts",
    "href": "usage.html#core-concepts",
    "title": "Using Flow",
    "section": "",
    "text": "Inspect Flow mirrors Inspect AI’s object model with corresponding Flow types:\n\nFlowJob — The top-level job definition. Under the hood it translates to an Inspect AI Eval Set. Contains a list of tasks and job settings.\nFlowTask — Configuration for a single evaluation task. Maps to Inspect AI Task parameters.\nFlowModel — Model configuration including API settings and generation settings. Maps to Inspect AI Model.\nFlowGenerateConfig — Model generation parameters. Maps to Inspect AI GenerateConfig.\nFlowSolver and FlowAgent — Solver and agent chain configuration. Map to Inspect AI Solver and Agent.\nFlowOptions — Runtime execution options. Maps to Inspect AI eval_set() parameters.\nFlowDefaults — System for setting default values across tasks, models, solvers, and agents.\n\n\n\n\nFlowJob is the primary interface for defining Flow workflows. All Flow operations—including parameter sweeps and matrix expansions—ultimately produce a list of tasks that FlowJob executes.\nRequired fields:\n\nlog_dir — Output path for logging results. Must be set before running the flow job. Supports S3 paths (e.g., s3://bucket/path)\n\nOptional fields:\n\n\n\n\n\n\n\n\nField\nDescription\nDefault\n\n\n\n\ntasks\nList of FlowTask objects defining the evaluations to run\nNone\n\n\nincludes\nList of other flow configs to include (paths as strings or FlowInclude objects)\nNone\n\n\nlog_dir_create_unique\nIf True, append numeric suffix to log_dir if it exists. If False, reuse existing directory (must be empty or have log_dir_allow_dirty=True)\nFalse\n\n\npython_version\nPython version for the isolated virtual environment (e.g., \"3.11\")\nSame as current environment\n\n\ndependencies\nPyPI packages, Git URLs, or local paths to install\nNone\n\n\nenv\nEnvironment variables to set when running tasks\nNone\n\n\noptions\nRuntime options (see FlowOptions reference)\nNone\n\n\ndefaults\nDefault values applied across tasks, models, and solvers\nNone\n\n\nflow_metadata\nMetadata stored in the flow config (not passed to Inspect AI)\nNone",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#creating-flow-configs",
    "href": "usage.html#creating-flow-configs",
    "title": "Using Flow",
    "section": "Creating Flow Configs",
    "text": "Creating Flow Configs\n\nYour First Config\nIn Basic Examples, we showed a simple FlowJob. Let’s break down what’s happening:\n\n\nfirst_config.py\n\nfrom inspect_flow import FlowJob, FlowTask\n\nFlowJob(\n1    log_dir=\"logs\",\n2    dependencies=[\"inspect-evals\"],\n3    tasks=[\n        FlowTask(\n4            name=\"inspect_evals/gpqa_diamond\",\n5            model=\"openai/gpt-5\",\n        ),\n    ],\n)\n\n\n1\n\nSpecify the directory for storing logs\n\n2\n\nInstall inspect-evals Python package\n\n3\n\nList evaluation tasks to run\n\n4\n\nSpecify task from registry by name\n\n5\n\nSpecify model to evaluate by name\n\n\nTo run the task, run the following command in your shell.\nflow run config.py\n\n\n\nProgress bar in terminal\n\n\nWhat happens when you run this?\n\nFlow creates an isolated virtual environment\nInstalls inspect-evals and openai (inferred from model)\nLoads the gpqa_diamond task from the inspect_evals registry\nRuns the evaluation with GPT-5\nStores results in logs/\n\n\n\n\n\n\n\nNoteConfig File Evaluation\n\n\n\nPython config files are evaluated as normal Python code. The last expression in the file is used as the FlowJob. This means you can:\n\nDefine variables and reuse them\nUse loops or comprehensions to generate task lists\nImport helper functions\nAdd comments and documentation\n\nFlow configs are just Python!\n\n\n\n\nTask Specification\nIn the example above, we used a registry name (\"inspect_evals/gpqa_diamond\"). Flow supports multiple ways to reference tasks:\n\nRegistry NameFile PathFile with Function\n\n\n# Tasks from installed packages.\nFlowTask(\n    name=\"inspect_evals/gpqa_diamond\",\n    model=\"openai/gpt-5\"\n)\n\n\n# Auto-discovers `@task` decorated functions in the specified file and creates a task for each of them.\nFlowTask(\n    name=\"./my_task.py\",\n    model=\"openai/gpt-5\"\n)\n\n\n# Explicitly selects a specific function from the file.\nFlowTask(\n    name=\"./my_task.py@custom_eval\",\n    model=\"openai/gpt-5\"\n)\n\n\n\n\n\nTask Configuration\nFlowTask accepts parameters that map to Inspect AI Task fields. The examples below show commonly used fields; see the FlowTask reference documentationfor the complete list of available parameters.\nFlowTask(\n1    name=\"inspect_evals/mmlu_0_shot\",\n2    model=\"openai/gpt-5\",\n3    epochs=3,\n4    config=FlowGenerateConfig(\n        temperature=0.7,\n        max_tokens=1000,\n    ),\n5    solver=\"chain_of_thought\",\n6    args={\"subject\": \"physics\"},\n7    sandbox=\"docker\",\n8    sample_id=[0, 1, 2],\n)\n\n1\n\nTask name — Maps to Inspect AI Task.name. Can be a registry name (\"inspect_evals/mmlu\"), file path (\"./task.py\"), or file with function (\"./task.py@eval_fn\").\n\n2\n\nModel — Maps to Inspect AI Task.model. Optional model for this task. If not specified, uses the model from INSPECT_EVAL_MODEL environment variable. Can be a string (\"openai/gpt-5\") or a FlowModel object for advanced configuration.\n\n3\n\nEpochs — Maps to Inspect AI Task.epochs. Number of times to repeat evaluation over the dataset samples. Can be an integer (epochs=3) or a FlowEpochs object to specify custom reducer functions (FlowEpochs(epochs=3, reducer=\"median\")). By default, scores are combined using the \"mean\" reducer across epochs.\n\n4\n\nGeneration config — Maps to Inspect AI Task.config (GenerateConfig). Model generation parameters like temperature, max_tokens, top_p, reasoning_effort, etc. These settings override config on FlowJob.config but are overridden by settings on FlowModel.config.\n\n5\n\nSolver chain — Maps to Inspect AI Task.solver. The algorithm(s) for solving the task. Can be a string (\"chain_of_thought\"), FlowSolver object, FlowAgent object, or a list of solvers for chaining. Defaults to generate() if not specified.\n\n6\n\nTask arguments — Maps to task function parameters. Dictionary of arguments passed to the task constructor or @task decorated function. Enables parameterization of tasks (e.g., selecting dataset subsets, configuring difficulty levels).\n\n7\n\nSandbox environment — Maps to Inspect AI Task.sandbox. Can be a string (\"docker\", \"local\"), a tuple with additional config, or a SandboxEnvironmentType object.\n\n8\n\nSample selection — Evaluate specific samples from the dataset. Accepts a single ID (sample_id=0), list of IDs (sample_id=[0, 1, 2]), or list of string IDs.\n\n\n\n\nModel Specification\nModels can be specified as simple strings or as FlowModel objects for more control:\n\nSimple stringFlowModelMulti-model tasks\n\n\nFlowTask(name=\"task\", model=\"openai/gpt-5\")\n\n\nFlowTask(\n    name=\"task\",\n    model=FlowModel(\n        name=\"openai/gpt-5\",\n        config=FlowGenerateConfig(\n            reasoning_effort=\"medium\",\n            max_connections=10,\n        ),\n        base_url=\"https://custom-endpoint.com\",\n        api_key=\"${CUSTOM_API_KEY}\",\n    )\n)\n\n\n# For agent evaluations with multiple roles\nFlowTask(\n    name=\"multi_agent_task\",\n    model_roles={\n        \"assistant\": \"openai/gpt-5\",\n        \"critic\": \"anthropic/claude-3-5-sonnet\",\n    }\n)\n\n\n\n\n\n\n\n\n\nTipWhen to use FlowModel\n\n\n\nUse FlowModel when you need to:\n\nSet model-specific generation configs\nUse custom API endpoints\nConfigure API keys per model\nOrganize complex multi-model setups",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#dependency-management",
    "href": "usage.html#dependency-management",
    "title": "Using Flow",
    "section": "Dependency Management",
    "text": "Dependency Management\nInspect Flow automatically creates isolated virtual environments for each workflow run, ensuring repeatability and avoiding dependency conflicts with your system Python environment.\n\nHow Virtual Environments Work\nWhen you run flow run config.py, Flow:\n\nCreates a temporary virtual environment with uv\nInstalls your specified dependencies plus auto-detected model provider packages\nExecutes your evaluations in this isolated environment\nCleans up the temporary environment after completion (logs persist in log_dir)\n\n\n\nSpecifying Dependencies\nThe dependencies field in FlowJob accepts multiple types of package specifiers:\n\nPyPI PackagesGit RepositoriesLocal Packages\n\n\n# Standard PyPI package names with optional version specifiers.\nFlowJob(\n    dependencies=[\n        \"inspect-evals\",\n        \"pandas==2.0.0\",\n    ],\n    tasks=[...]\n)\n\n\n# Install directly from Git repositories. Use `@commit_hash` to pin to specific versions for repeatability.\nFlowJob(\n    dependencies=[\n        \"git+https://github.com/UKGovernmentBEIS/inspect_evals@ef181cd\",\n    ],\n    tasks=[...]\n)\n\n\n# Install local packages using relative or absolute paths.\nFlowJob(\n    dependencies=[\n        \"./my_custom_eval\",\n        \"../shared/utils\",\n    ],\n    tasks=[...]\n)\n\n\n\n\n\nPython Version Control\nSpecify the Python version for your job’s virtual environment:\nFlowJob(\n    python_version=\"3.11\",\n    dependencies=[\"inspect-evals\"],\n    tasks=[...]\n)\n\n\n\n\n\n\nTipChecking Python Version\n\n\n\nTo verify which Python version will be used, run:\nflow config config.py --resolve\nThis shows the resolved configuration including the Python version that will be used.\n\n\n\n\n\n\n\n\nTipRepeatability Best Practices\n\n\n\nFor repeatable workflows:\n\nPin PyPI package versions: \"inspect-evals==0.3.15\"\nPin Git commits: \"git+https://github.com/user/repo@commit_hash\"\nSpecify python_version explicitly",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#defaults-and-overrides",
    "href": "usage.html#defaults-and-overrides",
    "title": "Using Flow",
    "section": "Defaults and Overrides",
    "text": "Defaults and Overrides\nInspect Flow provides a powerful defaults system to avoid repetition when configuring evaluations. The FlowDefaults field lets you set default values that cascade across tasks, models, solvers, and agents—with more specific settings overriding less specific ones.\n\nThe FlowDefaults System\nFlowDefaults supports multiple levels of default configuration:\nfrom inspect_flow import (\n    FlowJob,\n    FlowDefaults,\n    FlowGenerateConfig,\n    FlowModel,\n    FlowSolver,\n    FlowAgent,\n    FlowTask,\n)\n\nFlowJob(\n    defaults=FlowDefaults(\n1        config=FlowGenerateConfig(\n            max_connections=10,\n        ),\n2        model=FlowModel(\n            model_args={\"arg\": \"foo\"},\n        ),\n3        model_prefix={\n            \"openai/\": FlowModel(\n                config=FlowGenerateConfig(\n                    max_connections=20\n                ),\n            ),\n        },\n4        solver=FlowSolver(...),\n5        solver_prefix={\"chain_of_thought\": ...},\n6        agent=FlowAgent(...),\n7        agent_prefix={\"inspect/\": ...},\n8        task=FlowTask(...),\n9        task_prefix={\"inspect_evals/\": ...},\n    ),\n    tasks=[...]\n)\n\n1\n\nDefault model generation options. Will be overridden by settings on FlowTask and FlowModel.\n\n2\n\nField defaults for models.\n\n3\n\nModel defaults for model name prefixes. Overrides FlowDefaults.config and FlowDefaults.model. If multiple prefixes match, longest prefix wins.\n\n4\n\nField defaults for solvers.\n\n5\n\nSolver defaults for solver name prefixes. Overrides FlowDefaults.solver. If multiple prefixes match, longest prefix wins.\n\n6\n\nField defaults for agents.\n\n7\n\nAgent defaults for agent name prefixes. Overrides FlowDefaults.agent. If multiple prefixes match, longest prefix wins.\n\n8\n\nField defaults for tasks.\n\n9\n\nTask defaults for task name prefixes. Overrides FlowDefaults.config and FlowDefaults.task. If multiple prefixes match, longest prefix wins.\n\n\n\n\nMerge Priority and Behavior\nDefaults follow a hierarchy where more specific settings override less specific ones:\nFor Models:\n\nGlobal config defaults (defaults.config)\nGlobal model defaults (defaults.model)\nModel prefix defaults (defaults.model_prefix)\nTask-specific config (task.config)\nModel-specific config (model.config) — highest priority\n\nExample hierarchy in action:\n\n\nhierarchy.py\n\nfrom inspect_flow import FlowDefaults, FlowGenerateConfig, FlowJob, FlowModel, FlowTask\n\nFlowJob(\n    defaults=FlowDefaults(\n        config=FlowGenerateConfig(\n1            temperature=0.0,\n            max_tokens=100,\n        ),\n        model_prefix={\n            \"openai/\": FlowModel(\n2                config=FlowGenerateConfig(temperature=0.5)\n            )\n        },\n    ),\n    tasks=[\n        FlowTask(\n            name=\"task\",\n3            config=FlowGenerateConfig(temperature=0.7),\n            model=FlowModel(\n                name=\"openai/gpt-4o\",\n4                config=FlowGenerateConfig(temperature=1.0),\n            ),\n        )\n    ],\n)\n\n\n1\n\nGlobal defaults: temperature=0.0, max_tokens=100\n\n2\n\nPrefix defaults override: temperature=0.5 (for OpenAI models)\n\n3\n\nTask config overrides: temperature=0.7\n\n4\n\nModel config wins: temperature=1.0, max_tokens=100\n\n\nFinal result: temperature=1.0 (most specific), max_tokens=100 (from global defaults)\n\n\n\n\n\n\nNoteNone Values Don’t Override\n\n\n\nSetting a field to None means “not specified” — it won’t override existing values from defaults. This allows partial configs to merge cleanly:\nFlowTask(\n    config=FlowGenerateConfig(temperature=0.5, max_tokens=None)\n)\nThe max_tokens=None doesn’t override a default max_tokens value; it’s simply not set at this level.\n\n\n\n\nCLI Overrides\nOverride config values at runtime using the --set flag:\n\nBasic usageNested pathsJSON dictsMultiple overrides\n\n\nflow run config.py --set log_dir=./logs\n\n\nflow run config.py --set options.limit=10\nflow run config.py --set defaults.solver.args.tool_calls=none\n\n\nflow run config.py --set 'options.metadata={\"experiment\": \"baseline\", \"version\": \"v1\"}'\n\n\nflow run config.py \\\n  --set log_dir=./logs/experiment1 \\\n  --set options.limit=100 \\\n  --set defaults.config.temperature=0.5\n\n\n\n\n\n\n\n\n\nNoteOverride Behavior\n\n\n\n\nStrings: Replace existing values\nDicts: Replace existing values\nLists:\n\nString values append to existing list\nLists replace existing list\n\n\nExamples:\n# Appends to list\n--set dependencies=new_package\n\n# Replaces list\n--set 'dependencies=[\"pkg1\", \"pkg2\"]'\n\n\n\n\nEnvironment Variables\nSet config values via environment variables:\nexport INSPECT_FLOW_LOG_DIR=./logs/custom\nexport INSPECT_FLOW_LIMIT=50\nexport INSPECT_FLOW_SET=\"options.metadata={\\\"key\\\": \\\"value\\\"}\"\nexport INSPECT_FLOW_VAR=\"task_min_priority=2\"\nflow run config.py\nSupported environment variables:\n\n\n\n\n\n\n\n\nVariable\nEquivalent Flag\nDescription\n\n\n\n\nINSPECT_FLOW_LOG_DIR\n--log-dir\nOverride log directory\n\n\nINSPECT_FLOW_LOG_DIR_CREATE_UNIQUE\n--log-dir-create-unique\nCreate new log directory with numeric suffix if exists\n\n\nINSPECT_FLOW_LIMIT\n--limit\nLimit number of samples\n\n\nINSPECT_FLOW_SET\n--set\nSet config overrides (can be specified multiple times)\n\n\nINSPECT_FLOW_VAR\n--var\nSet variables for __flow_vars__ (can be multiple)\n\n\n\n\n\n\n\n\n\nTipOverride Priority\n\n\n\nSetting defaults via the command line will override the defaults which in turn might be overridden by anything set explicitly.\nDedicated environment variables (see Supported environment variables above) and corresponding CLI flags will override the --set flag.\nDedicated CLI flags have the highest priority.\nPriority order of log-dir, log-dir-create-unique and limit:\n\nFlowJob defaults\nExplicit setting on the FlowJob\nINSPECT_FLOW_SET_ environment variables\nCLI --set flags\nINSPECT_FLOW_LOG_DIR, INSPECT_FLOW_LOG_DIR_CREATE_UNIQUE and INSPECT_FLOW_LIMIT environment variables\nExplicit --log-dir, --log-dir-create-unique and --limit CLI flags\n\nAll other settings follow the priority order:\n\nFlowJob defaults\nExplicit setting on the FlowJob\nINSPECT_FLOW_SET_ environment variables\nCLI --set flags\n\n\n\n\n\nDebugging Defaults Resolution\nTo see the fully resolved configuration with all defaults applied:\nflow config config.py --resolve\nThis shows exactly what settings each task will use after applying all defaults and overrides.",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#configuration-inheritance",
    "href": "usage.html#configuration-inheritance",
    "title": "Using Flow",
    "section": "Configuration Inheritance",
    "text": "Configuration Inheritance\nInspect Flow supports configuration inheritance through two mechanisms:\n\nExplicit Includes\nUse the includes field to explicitly merge other config files into your job:\nFlowJob(\n    includes=[\"defaults_flow.py\"],\n    log_dir=\"./logs/flow_test\",\n    tasks=[\"my_task\"]\n)\nIncluded configs are merged recursively, with the current config’s values taking precedence over included values. For dictionaries, fields are merged deeply. For lists, items are concatenated with duplicates removed.\n\n\nAutomatic Discovery\nInspect Flow automatically discovers and includes files named _flow.py in parent directories. Starting from your config file’s location, it searches upward through the directory tree for _flow.py files and automatically merges them as base configurations.\nThis allows you to define shared defaults (model settings, dependencies, etc.) at a repository root that apply to all configs in subdirectories without explicit includes.",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#parameter-sweeping",
    "href": "usage.html#parameter-sweeping",
    "title": "Using Flow",
    "section": "Parameter Sweeping",
    "text": "Parameter Sweeping\nParameter sweeping lets you systematically explore evaluation configurations by generating Cartesian products of parameters. Instead of manually writing every combination, Flow provides matrix and “with” functions to declaratively generate evaluation grids.\n\nMatrix Functions (Cartesian Products)\nMatrix functions generate all combinations of their parameters using Cartesian products.\n\ntasks_matrix()\nGenerate task configurations by combining tasks with models, configs, solvers, and arguments:\n\n\ntasks_matrix.py\n\nfrom inspect_flow import FlowJob, tasks_matrix\n\nFlowJob(\n    log_dir=\"logs\",\n    dependencies=[\"inspect-evals\"],\n    tasks=tasks_matrix(\n        task=[\"inspect_evals/gpqa_diamond\", \"inspect_evals/mmlu_0_shot\"],\n        model=[\"openai/gpt-4o\", \"anthropic/claude-3-5-sonnet\"],\n    ),\n)\n\nThis creates 4 tasks (2 tasks × 2 models).\n\n\nmodels_matrix()\nGenerate model configurations with different generation settings:\n\n\nmodels_matrix.py\n\nfrom inspect_flow import FlowGenerateConfig, FlowJob, models_matrix, tasks_matrix\n\nFlowJob(\n    log_dir=\"logs\",\n    dependencies=[\"inspect-evals\"],\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=models_matrix(\n            model=[\n                \"openai/gpt-5\",\n                \"openai/gpt-5-mini\",\n            ],\n            config=[\n                FlowGenerateConfig(reasoning_effort=\"minimal\"),\n                FlowGenerateConfig(reasoning_effort=\"low\"),\n                FlowGenerateConfig(reasoning_effort=\"medium\"),\n                FlowGenerateConfig(reasoning_effort=\"high\"),\n            ],\n        ),\n    ),\n)\n\nThis creates 16 tasks (2 task × 2 models × 4 resoning_effort).\n\n\nconfigs_matrix()\nGenerate generation config combinations by specifying individual parameters:\n\n\nconfigs_matrix.py\n\nfrom inspect_flow import FlowJob, configs_matrix, models_matrix, tasks_matrix\n\nFlowJob(\n    log_dir=\"logs\",\n    dependencies=[\"inspect-evals\"],\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=models_matrix(\n            model=[\n                \"openai/gpt-5\",\n                \"openai/gpt-5-mini\",\n            ],\n            config=configs_matrix(\n                reasoning_effort=[\"minimal\", \"low\", \"medium\", \"high\"],\n            ),\n        ),\n    ),\n)\n\nThis creates 16 tasks (2 task × 2 models × 4 resoning_effort).\n\n\nsolvers_matrix() and agents_matrix()\nGenerate solver or agent configurations with different arguments:\n\n\nsolvers_matrix.py\n\nfrom inspect_flow import FlowJob, solvers_matrix, tasks_matrix\n\nFlowJob(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=\"my_task\",\n        solver=solvers_matrix(\n            solver=\"chain_of_thought\",\n            args=[\n                {\"max_iterations\": 3},\n                {\"max_iterations\": 5},\n                {\"max_iterations\": 10},\n            ],\n        ),\n    ),\n)\n\nThis creates 3 tasks (1 task × 3 solver configurations).\n\n\n\nWith Functions (Apply to All)\n“With” functions apply the same value to all items in a list. Use these when you want to sweep over some parameters while keeping others constant.\n\ntasks_with()\nApply common settings to multiple tasks:\n\n\ntasks_with.py\n\nfrom inspect_flow import FlowGenerateConfig, FlowJob, tasks_with\n\nFlowJob(\n    tasks=tasks_with(\n        task=[\"inspect_evals/gpqa_diamond\", \"inspect_evals/mmlu_0_shot\"],\n        model=\"openai/gpt-4o\",  # Same model for both tasks\n        config=FlowGenerateConfig(temperature=0.7),  # Same config for both\n    )\n)\n\nThis creates 2 tasks (2 tasks, each with the same model and config).\n\n\nCombining Matrix and With\nMix parameter sweeps with common settings:\n\n\nmatrix_and_with.py\n\nfrom inspect_flow import (\n    FlowJob,\n    configs_matrix,\n    tasks_matrix,\n    tasks_with,\n)\n\nFlowJob(\n    log_dir=\"logs\",\n    tasks=tasks_with(\n        task=tasks_matrix(\n            task=[\"task1\", \"task2\"], config=configs_matrix(temperature=[0.0, 0.5, 1.0])\n        ),  # Creates 6 tasks (2 × 3)\n        model=\"openai/gpt-4o\",  # Applied to all 6 tasks\n        sandbox=\"docker\",  # Applied to all 6 tasks\n    ),\n)\n\n\n\n\nNested Sweeps\nMatrix functions can be nested to create complex parameter grids. Use the unpacking operator * to expand inner matrix results:\nExample: Tasks with nested model sweep\n\n\nnested_model_sweep.py\n\nfrom inspect_flow import FlowGenerateConfig, FlowJob, models_matrix, tasks_matrix\n\nFlowJob(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=[\"inspect_evals/mmlu_0_shot\", \"inspect_evals/gpqa_diamond\"],\n        model=[\n            \"anthropic/claude-3-5-sonnet\",  # Single model\n            *models_matrix(  # Unpacks list of 4 model configs\n                model=[\"openai/gpt-4o\", \"openai/gpt-4o-mini\"],\n                config=[\n                    FlowGenerateConfig(reasoning_effort=\"low\"),\n                    FlowGenerateConfig(reasoning_effort=\"high\"),\n                ],\n            ),\n        ],  # Total: 1 + 4 = 5 models\n    ),\n)\n\nThis creates 10 tasks (2 tasks × 5 model configurations).\nExample: Tasks with nested task sweep\n\n\nnested_task_sweep.py\n\nfrom inspect_flow import FlowJob, FlowTask, tasks_matrix\n\nFlowJob(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=[\n            FlowTask(name=\"task1\", args={\"subset\": \"test\"}),  # Single task\n            *tasks_matrix(  # Unpacks list of 3 tasks\n                task=\"task2\",\n                args=[\n                    {\"language\": \"en\"},\n                    {\"language\": \"de\"},\n                    {\"language\": \"fr\"},\n                ],\n            ),\n        ],  # Total: 1 + 3 = 4 tasks\n        model=[\"model1\", \"model2\"],\n    ),\n)\n\nThis creates 8 tasks (4 task variants × 2 models).\n\n\n\n\n\n\nWarningWatch Out for Combinatorial Explosion\n\n\n\nParameter sweeps grow multiplicatively. A sweep with: - 3 tasks - 4 models - 5 temperature values - 3 solver configurations\nResults in 3 × 4 × 5 × 3 = 180 evaluations.\nAlways use --dry-run to check the number of evaluations before running expensive grids.\n\n\n\n\nConfig Merging Behavior\nWhen base objects already have values, matrix parameters are merged:\n# Config fields merge recursively\ntasks_matrix(\n    task=FlowTask(\n        name=\"task\",\n        config=FlowGenerateConfig(temperature=0.5)  # Base value\n    ),\n    config=[\n        FlowGenerateConfig(max_tokens=1000),  # Adds max_tokens, keeps temperature=0.5\n        FlowGenerateConfig(max_tokens=2000),  # Adds max_tokens, keeps temperature=0.5\n    ]\n)",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#running-evaluations",
    "href": "usage.html#running-evaluations",
    "title": "Using Flow",
    "section": "Running Evaluations",
    "text": "Running Evaluations\nOnce you’ve defined your Flow configuration, you can execute evaluations using the flow run command. Flow also provides tools for previewing configurations and controlling runtime behavior.\n\nThe flow run Command\nExecute your evaluation workflow:\nflow run config.py\nWhat happens when you run this:\n\nFlow loads your configuration file\nCreates an isolated virtual environment\nInstalls dependencies\nResolves all defaults and matrix expansions\nExecutes evaluations via Inspect AI’s eval_set()\nStores logs in log_dir\nCleans up the temporary environment\n\n\nCommon CLI Flags\nPreview without running:\nflow run config.py --dry-run\nShows how many tasks would be executed without actually running them. Useful for validating large parameter sweeps.\neval_set would be called with 24 tasks\nOverride log directory:\nflow run config.py --log-dir ./experiments/baseline\nChanges where logs and results are stored.\nRuntime overrides:\nflow run config.py \\\n  --set options.limit=100 \\\n  --set defaults.config.temperature=0.5\nOverride any configuration value at runtime. See CLI Overrides for more details.\n\n\n\nThe flow config Command\nPreview your configuration before running:\nBasic usage:\nflow config config.py\nDisplays the parsed configuration as YAML with CLI overrides applied. Does not create a virtual environment or instantiate Python objects.\nFull resolution:\nflow config config.py --resolve\nShows the completely resolved configuration:\n\nCreates virtual environment\nApplies all defaults\nExpands all matrix functions\nInstantiates all Python objects\n\nThis is invaluable for debugging what settings will actually be used in your evaluations.\n\n\n\n\n\n\nTipWhen to Use Each Command\n\n\n\n\nflow config - Quick syntax check, verify overrides\nflow config --resolve - Debug defaults resolution, inspect final settings\nflow run --dry-run - Count tasks in parameter sweeps, validate before expensive runs\nflow run - Execute evaluations\n\n\n\n\n\nResults and Logs\n\nFlow Directory Structure\nEvaluation results are stored in the log_dir:\nlogs/\n├── 2025-11-21T17-38-20+01-00_gpqa-diamond_KvJBGowidXSCLRhkKQbHYA.eval\n├── 2025-11-21T17-38-20+01-00_mmlu-0-shot_Vnu2A3M2wPet5yobLiCQmZ.eval\n├── .eval-set-id\n├── eval-set.json\n├── flow.yaml\n└── ...\nDirectory structure:\n\nFlow passes the log_dir directly to Inspect AI eval_set() for evaluation log storage\nInspect AI handles the actual evaluation log file naming and storage\nLog file naming conventions follow Inspect AI’s standards (see Inspect AI logging docs)\nFlow automatically saves the resolved configuration as flow.yaml in the log directory\nThe .eval-set-id file contains the eval set identifier\nThe eval-set.json file contains eval set metadata\n\nLog formats:\n\n.eval - Binary Inspect AI log format (default, high-performance)\n.json - JSON format (if log_format=\"json\" in FlowOptions)\n\n\n\nViewing Results\nUsing Inspect View:\ninspect view\nOpens the Inspect AI viewer to explore evaluation logs interactively.\n\n\nS3 Support\nStore logs directly to S3:\nFlowJob(\n    log_dir=\"s3://my-bucket/experiments/baseline\",\n    tasks=[...]\n)",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#advanced-features",
    "href": "usage.html#advanced-features",
    "title": "Using Flow",
    "section": "Advanced Features",
    "text": "Advanced Features\n\nMetadata Management\nFlow supports two types of metadata with distinct purposes: metadata and flow_metadata.\n\nmetadata (Inspect AI Metadata)\nThe metadata field in FlowOptions and FlowTask is passed directly to Inspect AI and stored in evaluation logs. Use this for tracking experiment information that should be accessible in Inspect AI’s log viewer and analysis tools.\nExample:\n\n\nmetadata.py\n\nfrom inspect_flow import FlowJob, FlowOptions, FlowTask\n\nFlowJob(\n    log_dir=\"logs\",\n    options=FlowOptions(\n        metadata={\n            \"experiment\": \"baseline_v1\",\n            \"hypothesis\": \"Higher temperature improves creative tasks\",\n            \"hardware\": \"A100-80GB\",\n        }\n    ),\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n            metadata={\n                \"task_variant\": \"chemistry_subset\",\n                \"note\": \"Testing with reduced context\",\n            },\n        )\n    ],\n)\n\nThe metadata from FlowOptions is applied globally to all tasks in the evaluation run, while task-level metadata is specific to each task.\n\n\nflow_metadata (Flow-Only Metadata)\nThe flow_metadata field is available on FlowJob, FlowTask, FlowModel, FlowSolver, and FlowAgent. This metadata is not passed to Inspect AI—it exists only in the Flow configuration and is useful for configuration-time logic and organization.\nUse cases:\n\nFiltering or selecting configurations based on properties\nOrganizing complex configuration generation logic\nDocumenting configuration decisions\nAnnotating configs without polluting Inspect AI logs\n\nExample: Configuration-time filtering\n\n\nflow_metadata.py\n\nfrom inspect_flow import FlowJob, FlowModel, tasks_matrix\n\n# Define models with metadata about capabilities\nmodels = [\n    FlowModel(name=\"openai/gpt-4o\", flow_metadata={\"context_window\": 128000}),\n    FlowModel(name=\"openai/gpt-4o-mini\", flow_metadata={\"context_window\": 128000}),\n    FlowModel(\n        name=\"anthropic/claude-3-5-sonnet\", flow_metadata={\"context_window\": 200000}\n    ),\n]\n\n# Filter to only long-context models\nlong_context_models = [\n    m\n    for m in models\n    if m.flow_metadata and m.flow_metadata.get(\"context_window\", 0) &gt;= 128000\n]\n\nFlowJob(\n    log_dir=\"logs\",\n    tasks=tasks_matrix(\n        task=\"long_context_task\",\n        model=long_context_models,\n    ),\n)\n\n\n\n\nVariable Substitution\n\nFlow Variables (--var)\nPass custom variables to Python config files using --var or the INSPECT_FLOW_VAR environment variable. Use this for dynamic configuration that isn’t available via --set. Variables are accessible via the __flow_vars__ dictionary:\nflow run config.py --var task_min_priority=2\n\n\nflow_vars.py\n\nfrom inspect_flow import FlowJob, FlowTask\n\ntask_min_priority = globals().get(\"__flow_vars__\", {}).get(\"task_min_priority\", 1)\n\nall_tasks = [\n    FlowTask(name=\"task_easy\", flow_metadata={\"priority\": 1}),\n    FlowTask(name=\"task_medium\", flow_metadata={\"priority\": 2}),\n    FlowTask(name=\"task_hard\", flow_metadata={\"priority\": 3}),\n]\n\nFlowJob(\n    log_dir=\"logs\",\n    tasks=[\n        t\n        for t in all_tasks\n        if (t.flow_metadata or {}).get(\"priority\", 0) &gt;= task_min_priority\n    ],\n)\n\n\n\nTemplate Substitution\nUse {field_name} syntax to reference other FlowJob configuration values. Substitutions are applied after the config is loaded:\nFlowJob(\n    log_dir=\"logs/my_eval\",\n    options=FlowOptions(bundle_dir=\"{log_dir}/bundle\"),\n    # Result: bundle_dir=\"logs/my_eval/bundle\"\n)\nFor nested fields, use bracket notation: {options[eval_set_id]} or {flow_metadata[key]}. Substitutions are resolved recursively until no more remain.\n\n\n\nBundle URL Mapping\nConvert local bundle paths to public URLs for sharing evaluation results. The bundle_url_map in FlowOptions applies string replacements to bundle_dir to generate a shareable URL that’s printed to stdout after the evaluation completes.\n\n\nbundle_url_map.py\n\nfrom inspect_flow import FlowJob, FlowOptions, FlowTask\n\nFlowJob(\n    log_dir=\"logs/my_eval\",\n    options=FlowOptions(\n        bundle_dir=\"/local/storage/bundles/my_eval\",\n        bundle_url_map={\"/local/storage\": \"https://example.com/shared\"},\n    ),\n    tasks=[FlowTask(name=\"task\", model=\"openai/gpt-4o\")],\n)\n\nAfter running this prints: Bundle URL: https://example.com/shared/bundles/my_eval\nUse this when storing bundles on servers with public HTTP access or cloud storage with URL mapping. Multiple mappings are applied in order.\n\n\nConfiguration Scripts\nIncluded configuration files can execute validation code to enforce constraints. Files named _flow.py are automatically included from parent directories, and receive __flow_including_jobs__ containing all configs in the include chain.\nTerminology:\n\nIncluding config: The file being loaded (e.g., my_config.py)\nIncluded config: The _flow.py file(s) automatically inherited from parent directories\n\nThe included config can inspect the including configs to validate or enforce constraints.\n\nPrevent Runs with Uncommitted Changes\nPlace a _flow.py file at your repository root to validate that all configs are in clean git repositories. This validation runs automatically for all configs in subdirectories.\n\n\n_flow.py\n\nimport subprocess\nfrom pathlib import Path\n\nfrom inspect_flow import FlowJob\n\n# Get all configs that are including this one\nincluding_jobs: dict[str, FlowJob] = globals().get(\"__flow_including_jobs__\", {})\n\n\ndef check_repo(path: str) -&gt; None:\n    abs_path = Path(path).resolve()\n    check_dir = abs_path if abs_path.is_dir() else abs_path.parent\n\n    result = subprocess.run(\n        [\"git\", \"status\", \"--porcelain\"],\n        cwd=check_dir,\n        capture_output=True,\n        text=True,\n        check=True,\n    )\n\n    if result.stdout.strip():\n        raise RuntimeError(f\"The repository at {check_dir} has uncommitted changes.\")\n\n\n# Check this config and all configs including it\ncheck_repo(__file__)\nfor path in including_jobs.keys():\n    check_repo(path)\n\nFlowJob()  # Return empty job for inheritance\n\n\n\nconfig.py\n\n# Automatically inherits _flow.py\nfrom inspect_flow import FlowJob, FlowTask\n\nFlowJob(\n    log_dir=\"logs\",\n    dependencies=[\"inspect-evals\"],\n    tasks=[FlowTask(name=\"inspect_evals/gpqa_diamond\", model=\"openai/gpt-4o\")],\n)\n# Will fail if uncommitted changes exist in the repository\n\n\n\nLock Configuration Fields\nA _flow.py file can prevent configs from overriding critical settings:\n\n\n_flow.py\n\nfrom inspect_flow import FlowJob, FlowOptions\n\nMAX_SAMPLES = 16\n\n# Get all configs that are including this one\nincluding_jobs: dict[str, FlowJob] = globals().get(\"__flow_including_jobs__\", {})\n\n# Validate that including configs don't override MAX_SAMPLES\nfor file, job in including_jobs.items():\n    if (\n        job.options\n        and job.options.max_samples is not None\n        and job.options.max_samples != MAX_SAMPLES\n    ):\n        raise ValueError(\n            f\"Do not override max_samples! Error in {file} (or its includes)\"\n        )\n\nFlowJob(\n    options=FlowOptions(max_samples=MAX_SAMPLES),\n)\n\n\n\nconfig.py\n\n# Automatically inherits _flow.py\nfrom inspect_flow import FlowJob, FlowOptions, FlowTask\n\nFlowJob(\n    log_dir=\"logs\",\n    options=FlowOptions(max_samples=32),  # Will raise ValueError!\n    tasks=[FlowTask(name=\"inspect_evals/gpqa_diamond\", model=\"openai/gpt-4o\")],\n)\n\nThis pattern is useful for enforcing organizational standards (resource limits, safety constraints, etc.) across all evaluation configs in a repository.",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "reference/inspect_flow.api.html",
    "href": "reference/inspect_flow.api.html",
    "title": "inspect_flow.api",
    "section": "",
    "text": "Options for loading a configuration file.\n\nSource\n\nclass ConfigOptions(TypedDict, total=False)\n\n\n\nPrint the flow job configuration.\n\nSource\n\ndef config(\n    job: FlowJob,\n    resolve: bool = False,\n) -&gt; None\n\njob FlowJob\n\nThe flow job configuration.\n\nresolve bool\n\nIf True, resolve the configuration before printing.\n\n\n\n\n\nLoad a job file and apply any overrides.\n\nSource\n\ndef load_job(file: str, **kwargs: Unpack[ConfigOptions]) -&gt; FlowJob\n\nfile str\n\nThe path to the job configuration file.\n\n**kwargs Unpack[ConfigOptions]\n\nConfiguration options. See ConfigOptions for available parameters.\n\n\n\n\n\nRun an inspect_flow evaluation.\n\nSource\n\ndef run(\n    job: FlowJob,\n    dry_run: bool = False,\n) -&gt; None\n\njob FlowJob\n\nThe flow job configuration.\n\ndry_run bool\n\nIf True, do not run eval, but show a count of tasks that would be run.",
    "crumbs": [
      "Reference",
      "Python API",
      "inspect_flow.api"
    ]
  },
  {
    "objectID": "reference/inspect_flow.api.html#python-api",
    "href": "reference/inspect_flow.api.html#python-api",
    "title": "inspect_flow.api",
    "section": "",
    "text": "Options for loading a configuration file.\n\nSource\n\nclass ConfigOptions(TypedDict, total=False)\n\n\n\nPrint the flow job configuration.\n\nSource\n\ndef config(\n    job: FlowJob,\n    resolve: bool = False,\n) -&gt; None\n\njob FlowJob\n\nThe flow job configuration.\n\nresolve bool\n\nIf True, resolve the configuration before printing.\n\n\n\n\n\nLoad a job file and apply any overrides.\n\nSource\n\ndef load_job(file: str, **kwargs: Unpack[ConfigOptions]) -&gt; FlowJob\n\nfile str\n\nThe path to the job configuration file.\n\n**kwargs Unpack[ConfigOptions]\n\nConfiguration options. See ConfigOptions for available parameters.\n\n\n\n\n\nRun an inspect_flow evaluation.\n\nSource\n\ndef run(\n    job: FlowJob,\n    dry_run: bool = False,\n) -&gt; None\n\njob FlowJob\n\nThe flow job configuration.\n\ndry_run bool\n\nIf True, do not run eval, but show a count of tasks that would be run.",
    "crumbs": [
      "Reference",
      "Python API",
      "inspect_flow.api"
    ]
  },
  {
    "objectID": "reference/inspect_flow.html",
    "href": "reference/inspect_flow.html",
    "title": "inspect_flow",
    "section": "",
    "text": "Configuration for an Agent.\n\nSource\n\nclass FlowAgent(BaseModel, extra=\"forbid\")\n\n\n\nname str | None\n\nName of the agent. Required to be set by the time the agent is created.\n\nargs CreateArgs | None\n\nAdditional args to pass to agent constructor.\n\nflow_metadata dict[str, Any] | None\n\nOptional. Metadata stored in the flow config. Not passed to the agent.\n\ntype Literal['agent']\n\nType needed to differentiated solvers and agents in solver lists.\n\n\n\n\n\n\nConfiguration for a flow job.\n\nSource\n\nclass FlowJob(BaseModel, extra=\"forbid\")\n\n\n\nincludes Sequence[str | FlowInclude] | None\n\nList of other flow configs to include.\n\nlog_dir str | None\n\nOutput path for logging results (required to ensure that a unique storage scope is assigned). Must be set before running the flow job. If a relative path, it will be resolved relative to the most recent config file loaded with ‘load_job’ or the current working directory if ‘load_job’ was not used.\n\nlog_dir_create_unique bool | None\n\nIf True, create a new log directory by appending an _ and numeric suffix if the specified log_dir already exists. If the directory exists and has a _numeric suffix, that suffix will be incremented. If False, use the existing log_dir (which must be empty or have log_dir_allow_dirty=True). Defaults to False.\n\npython_version str | None\n\nPython version to use in the flow virtual environment (e.g. ‘3.11’)\n\noptions FlowOptions | None\n\nArguments for calls to eval_set.\n\ndependencies list[str] | None\n\nDependencies to pip install. E.g. PyPI package specifiers or Git repository URLs.\n\nenv dict[str, str] | None\n\nEnvironment variables to set when running tasks.\n\ndefaults FlowDefaults | None\n\nDefaults values for Inspect objects.\n\nflow_metadata dict[str, Any] | None\n\nOptional. Metadata stored in the flow config. Not passed to the model.\n\ntasks Sequence[str | FlowTask] | None\n\nTasks to run\n\n\n\n\n\n\nDefault field values for Inspect objects. Will be overriden by more specific settings.\n\nSource\n\nclass FlowDefaults(BaseModel, extra=\"forbid\")\n\n\n\nconfig FlowGenerateConfig | None\n\nDefault model generation options. Will be overriden by settings on the FlowModel and FlowTask.\n\nagent FlowAgent | None\n\nField defaults for agents.\n\nagent_prefix dict[str, FlowAgent] | None\n\nAgent defaults for agent name prefixes. E.g. {‘inspect/’: FAgent(…)}\n\nmodel FlowModel | None\n\nField defaults for models.\n\nmodel_prefix dict[str, FlowModel] | None\n\nModel defaults for model name prefixes. E.g. {‘openai/’: FModel(…)}\n\nsolver FlowSolver | None\n\nField defaults for solvers.\n\nsolver_prefix dict[str, FlowSolver] | None\n\nSolver defaults for solver name prefixes. E.g. {‘inspect/’: FSolver(…)}\n\ntask FlowTask | None\n\nField defaults for tasks.\n\ntask_prefix dict[str, FlowTask] | None\n\nTask defaults for task name prefixes. E.g. {‘inspect_evals/’: FTask(…)}\n\n\n\n\n\n\nConfiguration for task epochs.\nNumber of epochs to repeat samples over and optionally one or more reducers used to combine scores from samples across epochs. If not specified the “mean” score reducer is used.\n\nSource\n\nclass FlowEpochs(BaseModel)\n\n\n\nepochs int\n\nNumber of epochs.\n\nreducer str | list[str] | None\n\nOne or more reducers used to combine scores from samples across epochs (defaults to “mean”)\n\n\n\n\n\n\nModel generation options.\n\nSource\n\nclass FlowGenerateConfig(GenerateConfig, extra=\"forbid\")\n\n\n\nConfiguration for a Model.\n\nSource\n\nclass FlowModel(BaseModel, extra=\"forbid\")\n\n\n\nname str | None\n\nName of the model to use. Required to be set by the time the model is created.\n\nrole str | None\n\nOptional named role for model (e.g. for roles specified at the task or eval level). Provide a default as a fallback in the case where the role hasn’t been externally specified.\n\ndefault str | None\n\nOptional. Fallback model in case the specified model or role is not found. Should be a fully qualified model name (e.g. openai/gpt-4o).\n\nconfig FlowGenerateConfig | None\n\nConfiguration for model. Config values will be override settings on the FlowTask and FlowJob.\n\nbase_url str | None\n\nOptional. Alternate base URL for model.\n\napi_key str | None\n\nOptional. API key for model.\n\nmemoize bool | None\n\nUse/store a cached version of the model based on the parameters to get_model(). Defaults to True.\n\nmodel_args CreateArgs | None\n\nAdditional args to pass to model constructor.\n\nflow_metadata dict[str, Any] | None\n\nOptional. Metadata stored in the flow config. Not passed to the model.\n\n\n\n\n\n\nEvaluation options.\n\nSource\n\nclass FlowOptions(BaseModel, extra=\"forbid\")\n\n\n\nretry_attempts int | None\n\nMaximum number of retry attempts before giving up (defaults to 10).\n\nretry_wait float | None\n\nTime to wait between attempts, increased exponentially (defaults to 30, resulting in waits of 30, 60, 120, 240, etc.). Wait time per-retry will in no case be longer than 1 hour.\n\nretry_connections float | None\n\nReduce max_connections at this rate with each retry (defaults to 1.0, which results in no reduction).\n\nretry_cleanup bool | None\n\nCleanup failed log files after retries (defaults to True).\n\nsandbox SandboxEnvironmentType | None\n\nSandbox environment type (or optionally a str or tuple with a shorthand spec).\n\nsandbox_cleanup bool | None\n\nCleanup sandbox environments after task completes (defaults to True).\n\ntags list[str] | None\n\nTags to associate with this evaluation run.\n\nmetadata dict[str, Any] | None\n\nMetadata to associate with this evaluation run.\n\ntrace bool | None\n\nTrace message interactions with evaluated model to terminal.\n\ndisplay DisplayType | None\n\nTask display type (defaults to ‘full’).\n\napproval str | ApprovalPolicyConfig | None\n\nTool use approval policies. Either a path to an approval policy config file or a list of approval policies. Defaults to no approval policy.\n\nscore bool | None\n\nScore output (defaults to True).\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”).\n\nlog_level_transcript str | None\n\nLevel for logging to the log file (defaults to “info”).\n\nlog_format Literal['eval', 'json'] | None\n\nFormat for writing log files (defaults to “eval”, the native high-performance format).\n\nlimit int | None\n\nLimit evaluated samples (defaults to all samples).\n\nsample_shuffle bool | int | None\n\nShuffle order of samples (pass a seed to make the order deterministic).\n\nfail_on_error bool | float | None\n\nTrue to fail on first sample error(default); False to never fail on sample errors; Value between 0 and 1 to fail if a proportion of total samples fails. Value greater than 1 to fail eval if a count of samples fails.\n\ncontinue_on_fail bool | None\n\nTrue to continue running and only fail at the end if the fail_on_error condition is met. False to fail eval immediately when the fail_on_error condition is met (default).\n\nretry_on_error int | None\n\nNumber of times to retry samples if they encounter errors (defaults to 3).\n\ndebug_errors bool | None\n\nRaise task errors (rather than logging them) so they can be debugged (defaults to False).\n\nmax_samples int | None\n\nMaximum number of samples to run in parallel (default is max_connections).\n\nmax_tasks int | None\n\nMaximum number of tasks to run in parallel (defaults is 10).\n\nmax_subprocesses int | None\n\nMaximum number of subprocesses to run in parallel (default is os.cpu_count()).\n\nmax_sandboxes int | None\n\nMaximum number of sandboxes (per-provider) to run in parallel.\n\nlog_samples bool | None\n\nLog detailed samples and scores (defaults to True).\n\nlog_realtime bool | None\n\nLog events in realtime (enables live viewing of samples in inspect view) (defaults to True).\n\nlog_images bool | None\n\nLog base64 encoded version of images, even if specified as a filename or URL (defaults to False).\n\nlog_buffer int | None\n\nNumber of samples to buffer before writing log file. If not specified, an appropriate default for the format and filesystem is chosen (10 for most all cases, 100 for JSON logs on remote filesystems).\n\nlog_shared bool | int | None\n\nSync sample events to log directory so that users on other systems can see log updates in realtime (defaults to no syncing). Specify True to sync every 10 seconds, otherwise an integer to sync every n seconds.\n\nbundle_dir str | None\n\nIf specified, the log viewer and logs generated by this eval set will be bundled into this directory.\n\nbundle_overwrite bool | None\n\nWhether to overwrite files in the bundle_dir. (defaults to False).\n\nlog_dir_allow_dirty bool | None\n\nIf True, allow the log directory to contain unrelated logs. If False, ensure that the log directory only contains logs for tasks in this eval set (defaults to False).\n\neval_set_id str | None\n\nID for the eval set. If not specified, a unique ID will be generated.\n\nbundle_url_map dict[str, str] | None\n\nReplacements applied to bundle_dir to generate a URL. If provided and bundle_dir is set, the mapped URL will be written to stdout.\n\n\n\n\n\n\nConfiguration for a Solver.\n\nSource\n\nclass FlowSolver(BaseModel, extra=\"forbid\")\n\n\n\nname str | None\n\nName of the solver. Required to be set by the time the solver is created.\n\nargs CreateArgs | None\n\nAdditional args to pass to solver constructor.\n\nflow_metadata dict[str, Any] | None\n\nOptional. Metadata stored in the flow config. Not passed to the solver.\n\n\n\n\n\n\nConfiguration for an evaluation task.\nTasks are the basis for defining and running evaluations.\n\nSource\n\nclass FlowTask(BaseModel, extra=\"forbid\")\n\n\n\nname str | None\n\nTask name. Any of registry name (“inspect_evals/mbpp”), file name (“./my_task.py”), or a file name and attr (“./my_task.py@task_name”). Required to be set by the time the task is created.\n\nargs CreateArgs | None\n\nAdditional args to pass to task constructor\n\nsolver str | FlowSolver | list[str | FlowSolver] | FlowAgent | None\n\nSolver or list of solvers. Defaults to generate(), a normal call to the model.\n\nmodel str | FlowModel | None\n\nDefault model for task (Optional, defaults to eval model).\n\nconfig FlowGenerateConfig | None\n\nModel generation config for default model (does not apply to model roles). Will override config settings on the FlowJob. Will be overridden by settings on the FlowModel.\n\nmodel_roles ModelRolesConfig | None\n\nNamed roles for use in get_model().\n\nsandbox SandboxEnvironmentType | None\n\nSandbox environment type (or optionally a str or tuple with a shorthand spec)\n\napproval str | ApprovalPolicyConfig | None\n\nTool use approval policies. Either a path to an approval policy config file or an approval policy config. Defaults to no approval policy.\n\nepochs int | FlowEpochs | None\n\nEpochs to repeat samples for and optional score reducer function(s) used to combine sample scores (defaults to “mean”)\n\nfail_on_error bool | float | None\n\nTrue to fail on first sample error (default); False to never fail on sample errors; Value between 0 and 1 to fail if a proportion of total samples fails. Value greater than 1 to fail eval if a count of samples fails.\n\ncontinue_on_fail bool | None\n\nTrue to continue running and only fail at the end if the fail_on_error condition is met. False to fail eval immediately when the fail_on_error condition is met (default).\n\nmessage_limit int | None\n\nLimit on total messages used for each sample.\n\ntoken_limit int | None\n\nLimit on total tokens used for each sample.\n\ntime_limit int | None\n\nLimit on clock time (in seconds) for samples.\n\nworking_limit int | None\n\nLimit on working time (in seconds) for sample. Working time includes model generation, tool calls, etc. but does not include time spent waiting on retries or shared resources.\n\nversion int | str | None\n\nVersion of task (to distinguish evolutions of the task spec or breaking changes to it)\n\nmetadata dict[str, Any] | None\n\nAdditional metadata to associate with the task.\n\nsample_id str | int | list[str | int] | None\n\nEvaluate specific sample(s) from the dataset.\n\nflow_metadata dict[str, Any] | None\n\nOptional. Metadata stored in the flow config. Not passed to the task.\n\nmodel_name str | None\n\nGet the model name from the model field.\nReturns: The model name if set, otherwise None.",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  },
  {
    "objectID": "reference/inspect_flow.html#types",
    "href": "reference/inspect_flow.html#types",
    "title": "inspect_flow",
    "section": "",
    "text": "Configuration for an Agent.\n\nSource\n\nclass FlowAgent(BaseModel, extra=\"forbid\")\n\n\n\nname str | None\n\nName of the agent. Required to be set by the time the agent is created.\n\nargs CreateArgs | None\n\nAdditional args to pass to agent constructor.\n\nflow_metadata dict[str, Any] | None\n\nOptional. Metadata stored in the flow config. Not passed to the agent.\n\ntype Literal['agent']\n\nType needed to differentiated solvers and agents in solver lists.\n\n\n\n\n\n\nConfiguration for a flow job.\n\nSource\n\nclass FlowJob(BaseModel, extra=\"forbid\")\n\n\n\nincludes Sequence[str | FlowInclude] | None\n\nList of other flow configs to include.\n\nlog_dir str | None\n\nOutput path for logging results (required to ensure that a unique storage scope is assigned). Must be set before running the flow job. If a relative path, it will be resolved relative to the most recent config file loaded with ‘load_job’ or the current working directory if ‘load_job’ was not used.\n\nlog_dir_create_unique bool | None\n\nIf True, create a new log directory by appending an _ and numeric suffix if the specified log_dir already exists. If the directory exists and has a _numeric suffix, that suffix will be incremented. If False, use the existing log_dir (which must be empty or have log_dir_allow_dirty=True). Defaults to False.\n\npython_version str | None\n\nPython version to use in the flow virtual environment (e.g. ‘3.11’)\n\noptions FlowOptions | None\n\nArguments for calls to eval_set.\n\ndependencies list[str] | None\n\nDependencies to pip install. E.g. PyPI package specifiers or Git repository URLs.\n\nenv dict[str, str] | None\n\nEnvironment variables to set when running tasks.\n\ndefaults FlowDefaults | None\n\nDefaults values for Inspect objects.\n\nflow_metadata dict[str, Any] | None\n\nOptional. Metadata stored in the flow config. Not passed to the model.\n\ntasks Sequence[str | FlowTask] | None\n\nTasks to run\n\n\n\n\n\n\nDefault field values for Inspect objects. Will be overriden by more specific settings.\n\nSource\n\nclass FlowDefaults(BaseModel, extra=\"forbid\")\n\n\n\nconfig FlowGenerateConfig | None\n\nDefault model generation options. Will be overriden by settings on the FlowModel and FlowTask.\n\nagent FlowAgent | None\n\nField defaults for agents.\n\nagent_prefix dict[str, FlowAgent] | None\n\nAgent defaults for agent name prefixes. E.g. {‘inspect/’: FAgent(…)}\n\nmodel FlowModel | None\n\nField defaults for models.\n\nmodel_prefix dict[str, FlowModel] | None\n\nModel defaults for model name prefixes. E.g. {‘openai/’: FModel(…)}\n\nsolver FlowSolver | None\n\nField defaults for solvers.\n\nsolver_prefix dict[str, FlowSolver] | None\n\nSolver defaults for solver name prefixes. E.g. {‘inspect/’: FSolver(…)}\n\ntask FlowTask | None\n\nField defaults for tasks.\n\ntask_prefix dict[str, FlowTask] | None\n\nTask defaults for task name prefixes. E.g. {‘inspect_evals/’: FTask(…)}\n\n\n\n\n\n\nConfiguration for task epochs.\nNumber of epochs to repeat samples over and optionally one or more reducers used to combine scores from samples across epochs. If not specified the “mean” score reducer is used.\n\nSource\n\nclass FlowEpochs(BaseModel)\n\n\n\nepochs int\n\nNumber of epochs.\n\nreducer str | list[str] | None\n\nOne or more reducers used to combine scores from samples across epochs (defaults to “mean”)\n\n\n\n\n\n\nModel generation options.\n\nSource\n\nclass FlowGenerateConfig(GenerateConfig, extra=\"forbid\")\n\n\n\nConfiguration for a Model.\n\nSource\n\nclass FlowModel(BaseModel, extra=\"forbid\")\n\n\n\nname str | None\n\nName of the model to use. Required to be set by the time the model is created.\n\nrole str | None\n\nOptional named role for model (e.g. for roles specified at the task or eval level). Provide a default as a fallback in the case where the role hasn’t been externally specified.\n\ndefault str | None\n\nOptional. Fallback model in case the specified model or role is not found. Should be a fully qualified model name (e.g. openai/gpt-4o).\n\nconfig FlowGenerateConfig | None\n\nConfiguration for model. Config values will be override settings on the FlowTask and FlowJob.\n\nbase_url str | None\n\nOptional. Alternate base URL for model.\n\napi_key str | None\n\nOptional. API key for model.\n\nmemoize bool | None\n\nUse/store a cached version of the model based on the parameters to get_model(). Defaults to True.\n\nmodel_args CreateArgs | None\n\nAdditional args to pass to model constructor.\n\nflow_metadata dict[str, Any] | None\n\nOptional. Metadata stored in the flow config. Not passed to the model.\n\n\n\n\n\n\nEvaluation options.\n\nSource\n\nclass FlowOptions(BaseModel, extra=\"forbid\")\n\n\n\nretry_attempts int | None\n\nMaximum number of retry attempts before giving up (defaults to 10).\n\nretry_wait float | None\n\nTime to wait between attempts, increased exponentially (defaults to 30, resulting in waits of 30, 60, 120, 240, etc.). Wait time per-retry will in no case be longer than 1 hour.\n\nretry_connections float | None\n\nReduce max_connections at this rate with each retry (defaults to 1.0, which results in no reduction).\n\nretry_cleanup bool | None\n\nCleanup failed log files after retries (defaults to True).\n\nsandbox SandboxEnvironmentType | None\n\nSandbox environment type (or optionally a str or tuple with a shorthand spec).\n\nsandbox_cleanup bool | None\n\nCleanup sandbox environments after task completes (defaults to True).\n\ntags list[str] | None\n\nTags to associate with this evaluation run.\n\nmetadata dict[str, Any] | None\n\nMetadata to associate with this evaluation run.\n\ntrace bool | None\n\nTrace message interactions with evaluated model to terminal.\n\ndisplay DisplayType | None\n\nTask display type (defaults to ‘full’).\n\napproval str | ApprovalPolicyConfig | None\n\nTool use approval policies. Either a path to an approval policy config file or a list of approval policies. Defaults to no approval policy.\n\nscore bool | None\n\nScore output (defaults to True).\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”).\n\nlog_level_transcript str | None\n\nLevel for logging to the log file (defaults to “info”).\n\nlog_format Literal['eval', 'json'] | None\n\nFormat for writing log files (defaults to “eval”, the native high-performance format).\n\nlimit int | None\n\nLimit evaluated samples (defaults to all samples).\n\nsample_shuffle bool | int | None\n\nShuffle order of samples (pass a seed to make the order deterministic).\n\nfail_on_error bool | float | None\n\nTrue to fail on first sample error(default); False to never fail on sample errors; Value between 0 and 1 to fail if a proportion of total samples fails. Value greater than 1 to fail eval if a count of samples fails.\n\ncontinue_on_fail bool | None\n\nTrue to continue running and only fail at the end if the fail_on_error condition is met. False to fail eval immediately when the fail_on_error condition is met (default).\n\nretry_on_error int | None\n\nNumber of times to retry samples if they encounter errors (defaults to 3).\n\ndebug_errors bool | None\n\nRaise task errors (rather than logging them) so they can be debugged (defaults to False).\n\nmax_samples int | None\n\nMaximum number of samples to run in parallel (default is max_connections).\n\nmax_tasks int | None\n\nMaximum number of tasks to run in parallel (defaults is 10).\n\nmax_subprocesses int | None\n\nMaximum number of subprocesses to run in parallel (default is os.cpu_count()).\n\nmax_sandboxes int | None\n\nMaximum number of sandboxes (per-provider) to run in parallel.\n\nlog_samples bool | None\n\nLog detailed samples and scores (defaults to True).\n\nlog_realtime bool | None\n\nLog events in realtime (enables live viewing of samples in inspect view) (defaults to True).\n\nlog_images bool | None\n\nLog base64 encoded version of images, even if specified as a filename or URL (defaults to False).\n\nlog_buffer int | None\n\nNumber of samples to buffer before writing log file. If not specified, an appropriate default for the format and filesystem is chosen (10 for most all cases, 100 for JSON logs on remote filesystems).\n\nlog_shared bool | int | None\n\nSync sample events to log directory so that users on other systems can see log updates in realtime (defaults to no syncing). Specify True to sync every 10 seconds, otherwise an integer to sync every n seconds.\n\nbundle_dir str | None\n\nIf specified, the log viewer and logs generated by this eval set will be bundled into this directory.\n\nbundle_overwrite bool | None\n\nWhether to overwrite files in the bundle_dir. (defaults to False).\n\nlog_dir_allow_dirty bool | None\n\nIf True, allow the log directory to contain unrelated logs. If False, ensure that the log directory only contains logs for tasks in this eval set (defaults to False).\n\neval_set_id str | None\n\nID for the eval set. If not specified, a unique ID will be generated.\n\nbundle_url_map dict[str, str] | None\n\nReplacements applied to bundle_dir to generate a URL. If provided and bundle_dir is set, the mapped URL will be written to stdout.\n\n\n\n\n\n\nConfiguration for a Solver.\n\nSource\n\nclass FlowSolver(BaseModel, extra=\"forbid\")\n\n\n\nname str | None\n\nName of the solver. Required to be set by the time the solver is created.\n\nargs CreateArgs | None\n\nAdditional args to pass to solver constructor.\n\nflow_metadata dict[str, Any] | None\n\nOptional. Metadata stored in the flow config. Not passed to the solver.\n\n\n\n\n\n\nConfiguration for an evaluation task.\nTasks are the basis for defining and running evaluations.\n\nSource\n\nclass FlowTask(BaseModel, extra=\"forbid\")\n\n\n\nname str | None\n\nTask name. Any of registry name (“inspect_evals/mbpp”), file name (“./my_task.py”), or a file name and attr (“./my_task.py@task_name”). Required to be set by the time the task is created.\n\nargs CreateArgs | None\n\nAdditional args to pass to task constructor\n\nsolver str | FlowSolver | list[str | FlowSolver] | FlowAgent | None\n\nSolver or list of solvers. Defaults to generate(), a normal call to the model.\n\nmodel str | FlowModel | None\n\nDefault model for task (Optional, defaults to eval model).\n\nconfig FlowGenerateConfig | None\n\nModel generation config for default model (does not apply to model roles). Will override config settings on the FlowJob. Will be overridden by settings on the FlowModel.\n\nmodel_roles ModelRolesConfig | None\n\nNamed roles for use in get_model().\n\nsandbox SandboxEnvironmentType | None\n\nSandbox environment type (or optionally a str or tuple with a shorthand spec)\n\napproval str | ApprovalPolicyConfig | None\n\nTool use approval policies. Either a path to an approval policy config file or an approval policy config. Defaults to no approval policy.\n\nepochs int | FlowEpochs | None\n\nEpochs to repeat samples for and optional score reducer function(s) used to combine sample scores (defaults to “mean”)\n\nfail_on_error bool | float | None\n\nTrue to fail on first sample error (default); False to never fail on sample errors; Value between 0 and 1 to fail if a proportion of total samples fails. Value greater than 1 to fail eval if a count of samples fails.\n\ncontinue_on_fail bool | None\n\nTrue to continue running and only fail at the end if the fail_on_error condition is met. False to fail eval immediately when the fail_on_error condition is met (default).\n\nmessage_limit int | None\n\nLimit on total messages used for each sample.\n\ntoken_limit int | None\n\nLimit on total tokens used for each sample.\n\ntime_limit int | None\n\nLimit on clock time (in seconds) for samples.\n\nworking_limit int | None\n\nLimit on working time (in seconds) for sample. Working time includes model generation, tool calls, etc. but does not include time spent waiting on retries or shared resources.\n\nversion int | str | None\n\nVersion of task (to distinguish evolutions of the task spec or breaking changes to it)\n\nmetadata dict[str, Any] | None\n\nAdditional metadata to associate with the task.\n\nsample_id str | int | list[str | int] | None\n\nEvaluate specific sample(s) from the dataset.\n\nflow_metadata dict[str, Any] | None\n\nOptional. Metadata stored in the flow config. Not passed to the task.\n\nmodel_name str | None\n\nGet the model name from the model field.\nReturns: The model name if set, otherwise None.",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  },
  {
    "objectID": "reference/inspect_flow.html#functions",
    "href": "reference/inspect_flow.html#functions",
    "title": "inspect_flow",
    "section": "Functions",
    "text": "Functions\n\nagents_matrix\nCreate a list of agents from the product of lists of field values.\n\nSource\n\ndef agents_matrix(\n    *,\n    agent: str | FlowAgent | Sequence[str | FlowAgent],\n    **kwargs: Unpack[FlowAgentMatrixDict],\n) -&gt; list[FlowAgent]\n\nagent str | FlowAgent | Sequence[str | FlowAgent]\n\nThe agent or list of agents to matrix.\n\n**kwargs Unpack[FlowAgentMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nagents_with\nSet fields on a list of agents.\n\nSource\n\ndef agents_with(\n    *,\n    agent: str | FlowAgent | Sequence[str | FlowAgent],\n    **kwargs: Unpack[FlowAgentDict],\n) -&gt; list[FlowAgent]\n\nagent str | FlowAgent | Sequence[str | FlowAgent]\n\nThe agent or list of agents to set fields on.\n\n**kwargs Unpack[FlowAgentDict]\n\nThe fields to set on each agent.\n\n\n\n\nconfigs_matrix\nCreate a list of generate configs from the product of lists of field values.\n\nSource\n\ndef configs_matrix(\n    *,\n    config: FlowGenerateConfig | Sequence[FlowGenerateConfig] | None = None,\n    **kwargs: Unpack[FlowGenerateConfigMatrixDict],\n) -&gt; list[FlowGenerateConfig]\n\nconfig FlowGenerateConfig | Sequence[FlowGenerateConfig] | None\n\nThe config or list of configs to matrix.\n\n**kwargs Unpack[FlowGenerateConfigMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nconfigs_with\nSet fields on a list of generate configs.\n\nSource\n\ndef configs_with(\n    *,\n    config: FlowGenerateConfig | Sequence[FlowGenerateConfig],\n    **kwargs: Unpack[FlowGenerateConfigDict],\n) -&gt; list[FlowGenerateConfig]\n\nconfig FlowGenerateConfig | Sequence[FlowGenerateConfig]\n\nThe config or list of configs to set fields on.\n\n**kwargs Unpack[FlowGenerateConfigDict]\n\nThe fields to set on each config.\n\n\n\n\nmerge\nMerge two flow objects.\n\nSource\n\ndef merge(base: _T, add: _T) -&gt; _T\n\nbase _T\n\nThe base object.\n\nadd _T\n\nThe object to merge into the base. Values in this object will override those in the base.\n\n\n\n\nmodels_matrix\nCreate a list of models from the product of lists of field values.\n\nSource\n\ndef models_matrix(\n    *,\n    model: str | FlowModel | Sequence[str | FlowModel],\n    **kwargs: Unpack[FlowModelMatrixDict],\n) -&gt; list[FlowModel]\n\nmodel str | FlowModel | Sequence[str | FlowModel]\n\nThe model or list of models to matrix.\n\n**kwargs Unpack[FlowModelMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nmodels_with\nSet fields on a list of models.\n\nSource\n\ndef models_with(\n    *,\n    model: str | FlowModel | Sequence[str | FlowModel],\n    **kwargs: Unpack[FlowModelDict],\n) -&gt; list[FlowModel]\n\nmodel str | FlowModel | Sequence[str | FlowModel]\n\nThe model or list of models to set fields on.\n\n**kwargs Unpack[FlowModelDict]\n\nThe fields to set on each model.\n\n\n\n\nsolvers_matrix\nCreate a list of solvers from the product of lists of field values.\n\nSource\n\ndef solvers_matrix(\n    *,\n    solver: str | FlowSolver | Sequence[str | FlowSolver],\n    **kwargs: Unpack[FlowSolverMatrixDict],\n) -&gt; list[FlowSolver]\n\nsolver str | FlowSolver | Sequence[str | FlowSolver]\n\nThe solver or list of solvers to matrix.\n\n**kwargs Unpack[FlowSolverMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nsolvers_with\nSet fields on a list of solvers.\n\nSource\n\ndef solvers_with(\n    *,\n    solver: str | FlowSolver | Sequence[str | FlowSolver],\n    **kwargs: Unpack[FlowSolverDict],\n) -&gt; list[FlowSolver]\n\nsolver str | FlowSolver | Sequence[str | FlowSolver]\n\nThe solver or list of solvers to set fields on.\n\n**kwargs Unpack[FlowSolverDict]\n\nThe fields to set on each solver.\n\n\n\n\ntasks_matrix\nCreate a list of tasks from the product of lists of field values.\n\nSource\n\ndef tasks_matrix(\n    *,\n    task: str | FlowTask | Sequence[str | FlowTask],\n    **kwargs: Unpack[FlowTaskMatrixDict],\n) -&gt; list[FlowTask]\n\ntask str | FlowTask | Sequence[str | FlowTask]\n\nThe task or list of tasks to matrix.\n\n**kwargs Unpack[FlowTaskMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\ntasks_with\nSet fields on a list of tasks.\n\nSource\n\ndef tasks_with(\n    *,\n    task: str | FlowTask | Sequence[str | FlowTask],\n    **kwargs: Unpack[FlowTaskDict],\n) -&gt; list[FlowTask]\n\ntask str | FlowTask | Sequence[str | FlowTask]\n\nThe task or list of tasks to set fields on.\n\n**kwargs Unpack[FlowTaskDict]\n\nThe fields to set on each task.",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  }
]