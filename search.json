[
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Reference",
    "section": "",
    "text": "Python API\n\n\n\n\n\n\n\ninspect_flow\nCore types and functions.\n\n\ninspect_flow.api\nOutput and run a job.\n\n\n\n\n\nFlow CLI\n\n\n\n\n\n\n\nflow config\nOutput job config.\n\n\nflow run\nRun a job.",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "reference/flow_run.html",
    "href": "reference/flow_run.html",
    "title": "flow run",
    "section": "",
    "text": "Run a job\n\nUsage\nflow run [OPTIONS] CONFIG_FILE\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--dry-run\nboolean\nDo not run job, but show a count of tasks that would be run.\nFalse\n\n\n--new-log-dir\nboolean\nIf set, create a new log directory by appending an _ and numeric suffix if the specified log_dir already exists. If the directory exists and has a _numeric suffix, that suffix will be incremented. If not set, use the existing log_dir (which must be empty or have log_dir_allow_dirty=True).\nFalse\n\n\n--log-dir\ndirectory\nSet the log directory. Will override the log_dir specified in the config.\nNone\n\n\n--limit\ninteger\nLimit the number of samples to run.\nNone\n\n\n--var\ntext\nSet variables accessible to code executing in the config file through the variable __flow_vars__: task_min_priority = __flow_vars__.get(\"task_min_priority\", 1) Examples: --var task_min_priority=2 If the same key is provided multiple times, later values will override earlier ones.\nSentinel.UNSET\n\n\n--set, -s\ntext\nSet config overrides. Examples: --set defaults.solver.args.tool_calls=none --set options.limit=10 --set options.metadata={\"key1\": \"val1\", \"key2\": \"val2\"} The specified value may be a string or json parsable list or dict. If string is provided then it will be appended to existing list values. If json list or dict is provided then it will replace existing values. If the same key is provided multiple times, later values will override earlier ones.\nSentinel.UNSET\n\n\n--help\nboolean\nShow this message and exit.\nFalse",
    "crumbs": [
      "Reference",
      "Inspect CLI",
      "flow run"
    ]
  },
  {
    "objectID": "reference/flow_config.html",
    "href": "reference/flow_config.html",
    "title": "flow config",
    "section": "",
    "text": "Output config\n\nUsage\nflow config [OPTIONS] CONFIG_FILE\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--resolve\nboolean\nFully resolve the config. Will create a venv and create all objects.\nFalse\n\n\n--new-log-dir\nboolean\nIf set, create a new log directory by appending an _ and numeric suffix if the specified log_dir already exists. If the directory exists and has a _numeric suffix, that suffix will be incremented. If not set, use the existing log_dir (which must be empty or have log_dir_allow_dirty=True).\nFalse\n\n\n--log-dir\ndirectory\nSet the log directory. Will override the log_dir specified in the config.\nNone\n\n\n--limit\ninteger\nLimit the number of samples to run.\nNone\n\n\n--var\ntext\nSet variables accessible to code executing in the config file through the variable __flow_vars__: task_min_priority = __flow_vars__.get(\"task_min_priority\", 1) Examples: --var task_min_priority=2 If the same key is provided multiple times, later values will override earlier ones.\nSentinel.UNSET\n\n\n--set, -s\ntext\nSet config overrides. Examples: --set defaults.solver.args.tool_calls=none --set options.limit=10 --set options.metadata={\"key1\": \"val1\", \"key2\": \"val2\"} The specified value may be a string or json parsable list or dict. If string is provided then it will be appended to existing list values. If json list or dict is provided then it will replace existing values. If the same key is provided multiple times, later values will override earlier ones.\nSentinel.UNSET\n\n\n--help\nboolean\nShow this message and exit.\nFalse",
    "crumbs": [
      "Reference",
      "Inspect CLI"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inspect Flow",
    "section": "",
    "text": "Inspect Flow is a workflow orchestration tool for Inspect AI that enables you to run evaluations at scale with repeatability and maintainability.\nWhy Inspect Flow? As evaluation workflows grow in complexity—running multiple tasks across different models with varying parameters—managing these experiments becomes challenging. Inspect Flow addresses this by providing:\n\nDeclarative Configuration: Define your entire evaluation pipeline in type-safe Pydantic schemas\nRepeatability: Encapsulated Python dependencies for each workflow\nParameter Sweeping: Matrix execution patterns for systematic exploration across tasks and models\n\nInspect Flow is designed for researchers and engineers running systematic AI evaluations who need to scale beyond ad-hoc scripts.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Inspect Flow",
    "section": "",
    "text": "Inspect Flow is a workflow orchestration tool for Inspect AI that enables you to run evaluations at scale with repeatability and maintainability.\nWhy Inspect Flow? As evaluation workflows grow in complexity—running multiple tasks across different models with varying parameters—managing these experiments becomes challenging. Inspect Flow addresses this by providing:\n\nDeclarative Configuration: Define your entire evaluation pipeline in type-safe Pydantic schemas\nRepeatability: Encapsulated Python dependencies for each workflow\nParameter Sweeping: Matrix execution patterns for systematic exploration across tasks and models\n\nInspect Flow is designed for researchers and engineers running systematic AI evaluations who need to scale beyond ad-hoc scripts.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Inspect Flow",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\nNotePrerequisites\n\n\n\nBefore using Inspect Flow, you should:\n\nHave familiarity with Inspect AI\nHave an existing Inspect evaluation or use one from inspect-evals\n\n\n\n\nInstallation\nInstall the inspect_flow package from GitHub as follows:\npip install git+https://github.com/meridianlabs-ai/inspect_flow\n\n\nSet up API keys\nYou’ll need API keys for the model providers you want to use. Set the relevant provider API key in your .env file or export it in your shell:\n\nOpenAIAnthropicGoogleGrokMistralHugging Face\n\n\nexport OPENAI_API_KEY=your-openai-api-key\n\n\nexport ANTHROPIC_API_KEY=your-anthropic-api-key\n\n\nexport GOOGLE_API_KEY=your-google-api-key\n\n\nexport GROK_API_KEY=your-grok-api-key\n\n\nexport MISTRAL_API_KEY=your-mistral-api-key\n\n\nexport HF_TOKEN=your-hf-token\n\n\n\n\n\nOptional: VS Code extension\nOptionally install the Inspect AI VS Code Extension which includes features for viewing evaluation log files.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#basic-examples",
    "href": "index.html#basic-examples",
    "title": "Inspect Flow",
    "section": "Basic Examples",
    "text": "Basic Examples\nLet’s walk through creating your first Flow configuration. We’ll use FlowJob (the entrypoint class) and FlowTask to define evaluations.\n\n\n\n\n\n\nTipCore Components Reference\n\n\n\n\n\n\ntypes.FlowJob — Pydantic class that encapsulates the declarative description of a Flow job.\ntypes.FlowTask — Pydantic class abstraction on top of Inspect AI Task.\ntypes.FlowModel — Pydantic class abstraction on top of Inspect AI Model.\ntypes.FlowGenerateConfig — Pydantic class abstraction on top of Inspect AI GenerateConfig.\ntasks_matrix — Helper function for parameter sweeping to generate a list of tasks with all parameter combinations.\nmodels_matrix — Helper function for parameter sweeping to generate a list of models with all parameter combinations.\nconfigs_matrix — Helper function for parameter sweeping to generate a list of GenerateConfig with all parameter combinations.\n\n\n\n\nFlowJob is the main entrypoint for defining evaluation runs. At its core, it takes a list of tasks to run. Here’s a simple example that runs two evaluations:\n\n\nconfig.py\n\nfrom inspect_flow.types import FlowJob, FlowTask\n\n\nFlowJob(\n    dependencies=[\"inspect-evals\"],\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-5\",\n        ),\n        FlowTask(\n            name=\"inspect_evals/mmlu_0_shot\",\n            model=\"openai/gpt-5\",\n        ),\n    ],\n)\n\nTo run the evaluations, run the following command in your shell. This will create a virtual environment for this job run and install the dependencies. Note that model dependencies (like the openai Python package) are inferred and installed automatically.\nflow run config.py\nThis will run both tasks and display progress in your terminal.\n\nMatrix Functions\nOften you’ll want to evaluate multiple tasks across multiple models. Rather than manually defining every combination, use tasks_matrix to generate all task-model pairs:\n\n\nmatrix.py\n\nfrom inspect_flow import FlowJob, tasks_matrix\n\n\nFlowJob(\n    dependencies=[\"inspect-evals\"],\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=[\n            \"openai/gpt-5\",\n            \"openai/gpt-5-mini\",\n        ],\n    ),\n)\n\nTo preview the expanded config before running it, you can run the following command in your shell to ensure the generated config is the one that you intend to run.\nflow config matrix.py\nThis command outputs the expanded configuration showing all 4 task-model combinations (2 tasks × 2 models).\ntasks_matrix and models_matrix are powerful functions that can operate on multiple levels of nested matrixes which enable sophisticated parameter sweeping. Let’s say you want to explore different reasoning efforts across models—you can achieve this with the models_matrix function.\n\n\nmatrix.py\n\nfrom inspect_flow import FlowJob, FlowGenerateConfig, tasks_matrix, models_matrix\n\n\nFlowJob(\n    dependencies=[\"inspect-evals\"],\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=models_matrix(\n            model=[\n                \"openai/gpt-5\",\n                \"openai/gpt-5-mini\",\n            ],\n            config=[\n                FlowGenerateConfig(reasoning_effort=\"minimal\"),\n                FlowGenerateConfig(reasoning_effort=\"low\"),\n                FlowGenerateConfig(reasoning_effort=\"medium\"),\n                FlowGenerateConfig(reasoning_effort=\"high\"),\n            ],\n        ),\n    ),\n)\n\nFor even more concise parameter sweeping, use configs_matrix to generate configuration variants. This produces the same 16 evaluations (2 tasks × 2 models × 4 reasoning levels) as above, but with less boilerplate:\n\n\nmatrix.py\n\nfrom inspect_flow import FlowJob, FlowGenerateConfig, tasks_matrix, models_matrix, configs_matrix\n\n\nFlowJob(\n    dependencies=[\"inspect-evals\"],\n    tasks=tasks_matrix(\n        task=[\n            \"inspect_evals/gpqa_diamond\",\n            \"inspect_evals/mmlu_0_shot\",\n        ],\n        model=models_matrix(\n            model=[\n                \"openai/gpt-5\",\n                \"openai/gpt-5-mini\",\n            ],\n            config=configs_matrix(\n                reasoning_effort=[\"minimal\", \"low\", \"medium\", \"high\"],\n            ),\n        ),\n    ),\n)\n\nTo run the config:\nflow run matrix.py\nThis will run all 16 evaluations (2 tasks × 2 models × 4 reasoning levels). Results are stored in logs/flow/ by default. When complete, you’ll find a link to the logs at the bottom of the task results summary. To view logs interactively, run:\ninspect view",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#learning-more",
    "href": "index.html#learning-more",
    "title": "Inspect Flow",
    "section": "Learning More",
    "text": "Learning More\nSee the following articles to learn more about using Flow:\n\nUsage: End-to-end guide for developing and running a Flow job.\nReference: Detailed documentation on the Flow Python API and CLI commands.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "usage.html",
    "href": "usage.html",
    "title": "Using Flow",
    "section": "",
    "text": "Inspect Flow mirrors Inspect AI’s object model with corresponding Flow types:\n\nFlowJob — The top-level job definition. Under the hood it translates to an Inspect AI Eval Set. Contains a list of tasks and job settings.\nFlowTask — Configuration for a single evaluation task. Maps to Inspect AI Task parameters.\nFlowModel — Model configuration including API settings and generation settings. Maps to Inspect AI Model.\nFlowGenerateConfig — Model generation parameters. Maps to Inspect AI GenerateConfig.\nFlowSolver and FlowAgent — Solver and agent chain configuration. Map to Inspect AI Solver and Agent.\nFlowOptions — Runtime execution options like concurrency limits, error handling, and logging preferences. Maps to Inspect AI eval_set() parameters.\nFlowDefaults — System for setting default values across tasks, models, solvers, and agents.\n\n\n\n\nFlowJob is the primary interface for defining Flow workflows. All Flow operations—including parameter sweeps and matrix expansions—ultimately produce a list of tasks that FlowJob executes.\nRequired fields:\n\ntasks — List of FlowTask objects defining the evaluations to run\n\nOptional fields:\n\n\n\n\n\n\n\n\nField\nDescription\nDefault\n\n\n\n\nflow_dir\nDirectory for logs and resolved config. Supports S3 paths (e.g., s3://bucket/path)\n\"logs/flow\"\n\n\npython_version\nPython version for the isolated virtual environment (e.g., \"3.11\")\nSame as current environment\n\n\ndependencies\nPyPI packages, Git URLs, or local paths to install\n[]\n\n\nenv\nEnvironment variables to set in the workflow execution context\n{}\n\n\noptions\nRuntime options (see FlowOptions reference)\nNone\n\n\ndefaults\nDefault values applied across tasks, models, and solvers\nNone",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#core-concepts",
    "href": "usage.html#core-concepts",
    "title": "Using Flow",
    "section": "",
    "text": "Inspect Flow mirrors Inspect AI’s object model with corresponding Flow types:\n\nFlowJob — The top-level job definition. Under the hood it translates to an Inspect AI Eval Set. Contains a list of tasks and job settings.\nFlowTask — Configuration for a single evaluation task. Maps to Inspect AI Task parameters.\nFlowModel — Model configuration including API settings and generation settings. Maps to Inspect AI Model.\nFlowGenerateConfig — Model generation parameters. Maps to Inspect AI GenerateConfig.\nFlowSolver and FlowAgent — Solver and agent chain configuration. Map to Inspect AI Solver and Agent.\nFlowOptions — Runtime execution options like concurrency limits, error handling, and logging preferences. Maps to Inspect AI eval_set() parameters.\nFlowDefaults — System for setting default values across tasks, models, solvers, and agents.\n\n\n\n\nFlowJob is the primary interface for defining Flow workflows. All Flow operations—including parameter sweeps and matrix expansions—ultimately produce a list of tasks that FlowJob executes.\nRequired fields:\n\ntasks — List of FlowTask objects defining the evaluations to run\n\nOptional fields:\n\n\n\n\n\n\n\n\nField\nDescription\nDefault\n\n\n\n\nflow_dir\nDirectory for logs and resolved config. Supports S3 paths (e.g., s3://bucket/path)\n\"logs/flow\"\n\n\npython_version\nPython version for the isolated virtual environment (e.g., \"3.11\")\nSame as current environment\n\n\ndependencies\nPyPI packages, Git URLs, or local paths to install\n[]\n\n\nenv\nEnvironment variables to set in the workflow execution context\n{}\n\n\noptions\nRuntime options (see FlowOptions reference)\nNone\n\n\ndefaults\nDefault values applied across tasks, models, and solvers\nNone",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#creating-flow-configs",
    "href": "usage.html#creating-flow-configs",
    "title": "Using Flow",
    "section": "Creating Flow Configs",
    "text": "Creating Flow Configs\n\nYour First Config\nIn Basic Examples, we showed a simple FlowJob. Let’s break down what’s happening:\n\n\nconfig.py\n\nfrom inspect_flow import FlowJob, FlowTask\n\n\nFlowJob(\n1    dependencies=[\"inspect-evals\"],\n2    tasks=[\n        FlowTask(\n3            name=\"inspect_evals/gpqa_diamond\",\n4            model=\"openai/gpt-5\",\n        ),\n    ],\n)\n\n\n1\n\nInstall inspect-evals Python package\n\n2\n\nList evaluation tasks to run\n\n3\n\nSpecify task from registry by name\n\n4\n\nSpecify model to evaluate by name\n\n\nTo run the task, run the following command in your shell.\nflow run config.py\nWhat happens when you run this?\n\nFlow creates an isolated virtual environment\nInstalls inspect-evals and openai (inferred from model)\nLoads the gpqa_diamond task from the inspect_evals registry\nRuns the evaluation with GPT-5\nStores results in logs/flow/ (default log directory)\n\n\n\n\n\n\n\nNoteConfig File Evaluation\n\n\n\nPython config files are evaluated as normal Python code. The last expression in the file is used as the FlowJob. This means you can:\n\nDefine variables and reuse them\nUse loops or comprehensions to generate task lists\nImport helper functions\nAdd comments and documentation\n\nFlow configs are just Python!\n\n\n\n\nTask Specification\nIn the example above, we used a registry name (\"inspect_evals/gpqa_diamond\"). Flow supports multiple ways to reference tasks:\n\nRegistry NameFile PathFile with Function\n\n\nFlowTask(\n    name=\"inspect_evals/gpqa_diamond\",\n    model=\"openai/gpt-5\"\n)\nTasks from installed packages.\n\n\nFlowTask(\n    name=\"./my_task.py\",\n    model=\"openai/gpt-5\"\n)\nAuto-discovers @task decorated functions in the specified file and creates a task for each of them.\n\n\nFlowTask(\n    name=\"./my_task.py@custom_eval\",\n    model=\"openai/gpt-5\"\n)\nExplicitly selects a specific function from the file.\n\n\n\n\n\nTask Configuration\nFlowTask accepts parameters that map to Inspect AI Task fields:\nFlowTask(\n1    name=\"inspect_evals/mmlu_0_shot\",\n2    model=\"openai/gpt-5\",\n3    epochs=3,\n4    config=FlowGenerateConfig(\n        temperature=0.7,\n        max_tokens=1000,\n    ),\n5    solver=\"chain_of_thought\",\n6    args={\"subject\": \"physics\"},\n7    sandbox=\"docker\",\n8    sample_id=[0, 1, 2],\n)\n\n1\n\nTask name — Maps to Inspect AI Task.name. Can be a registry name (\"inspect_evals/mmlu\"), file path (\"./task.py\"), or file with function (\"./task.py@eval_fn\").\n\n2\n\nModel — Maps to Inspect AI Task.model. Optional model for this task. If not specified, uses the model from INSPECT_EVAL_MODEL environment variable. Can be a string (\"openai/gpt-5\") or a FlowModel object for advanced configuration.\n\n3\n\nEpochs — Maps to Inspect AI Task.epochs. Number of times to repeat evaluation over the dataset samples. Can be an integer (epochs=3) or a FlowEpochs object to specify custom reducer functions (FlowEpochs(epochs=3, reducer=\"median\")). By default, scores are combined using the \"mean\" reducer across epochs.\n\n4\n\nGeneration config — Maps to Inspect AI Task.config (GenerateConfig). Model generation parameters like temperature, max_tokens, top_p, reasoning_effort, etc. These settings override config on FlowJob.config but are overridden by settings on FlowModel.config.\n\n5\n\nSolver chain — Maps to Inspect AI Task.solver. The algorithm(s) for solving the task. Can be a string (\"chain_of_thought\"), FlowSolver object, FlowAgent object, or a list of solvers for chaining. Defaults to generate() if not specified.\n\n6\n\nTask arguments — Maps to task function parameters. Dictionary of arguments passed to the task constructor or @task decorated function. Enables parameterization of tasks (e.g., selecting dataset subsets, configuring difficulty levels).\n\n7\n\nSandbox environment — Maps to Inspect AI Task.sandbox. Can be a string (\"docker\", \"local\"), a tuple with additional config, or a SandboxEnvironmentType object.\n\n8\n\nSample selection — Evaluate specific samples from the dataset. Accepts a single ID (sample_id=0), list of IDs (sample_id=[0, 1, 2]), or list of string IDs.\n\n\n\n\nModel Specification\nModels can be specified as simple strings or as FlowModel objects for more control:\n\nSimple stringFlowModelMulti-model tasks\n\n\nFor simple use cases:\nFlowTask(name=\"task\", model=\"openai/gpt-5\")\n\n\nFor advanced config:\nfrom inspect_flow.types import FlowModel, FlowGenerateConfig\n\nFlowTask(\n    name=\"task\",\n    model=FlowModel(\n        name=\"openai/gpt-5\",\n        config=FlowGenerateConfig(\n            reasoning_effort=\"medium\",\n            max_connections=10,\n        ),\n        base_url=\"https://custom-endpoint.com\",\n        api_key=\"${CUSTOM_API_KEY}\",\n    )\n)\n\n\nFor agent evaluations with multiple roles:\nFlowTask(\n    name=\"multi_agent_task\",\n    model_roles={\n        \"assistant\": \"openai/gpt-5\",\n        \"critic\": \"anthropic/claude-3-5-sonnet\",\n    }\n)\n\n\n\n\n\n\n\n\n\nTipWhen to use FlowModel\n\n\n\nUse FlowModel when you need to:\n\nSet model-specific generation configs\nUse custom API endpoints\nConfigure API keys per model\nOrganize complex multi-model setups",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#dependency-management",
    "href": "usage.html#dependency-management",
    "title": "Using Flow",
    "section": "Dependency Management",
    "text": "Dependency Management\nInspect Flow automatically creates isolated virtual environments for each workflow run, ensuring repeatability and avoiding dependency conflicts with your system Python environment.\n\nHow Virtual Environments Work\nWhen you run flow run config.py, Flow:\n\nCreates a temporary virtual environment in ~/.cache/inspect-flow/ with uv\nInstalls your specified dependencies plus auto-detected model provider packages\nExecutes your evaluations in this isolated environment\nCleans up the temporary environment after completion (logs persist in flow_dir)\n\n\n\nSpecifying Dependencies\nThe dependencies field in FlowJob accepts multiple types of package specifiers:\n\nPyPI PackagesGit RepositoriesLocal Packages\n\n\nFlowJob(\n    dependencies=[\n        \"inspect-evals\",\n        \"pandas==2.0.0\",\n    ],\n    tasks=[...]\n)\nStandard PyPI package names with optional version specifiers.\n\n\nFlowJob(\n    dependencies=[\n        \"git+https://github.com/UKGovernmentBEIS/inspect_evals@ef181cd\",\n    ],\n    tasks=[...]\n)\nInstall directly from Git repositories. Use @commit_hash to pin to specific versions for repeatability.\n\n\nFlowJob(\n    dependencies=[\n        \"./my_custom_eval\",\n        \"../shared/utils\",\n    ],\n    tasks=[...]\n)\nInstall local packages using relative or absolute paths.\n\n\n\n\n\nPython Version Control\nSpecify the Python version for your job’s virtual environment:\nFlowJob(\n    python_version=\"3.11\",\n    dependencies=[\"inspect-evals\"],\n    tasks=[...]\n)\n\n\n\n\n\n\nTipChecking Python Version\n\n\n\nTo verify which Python version will be used, run:\nflow config config.py --resolve\nThis shows the resolved configuration including the Python version that will be used.\n\n\n\n\n\n\n\n\nTipRepeatability Best Practices\n\n\n\nFor repeatable workflows:\n\nPin PyPI package versions: \"inspect-evals==0.3.15\"\nPin Git commits: \"git+https://github.com/user/repo@commit_hash\"\nSpecify python_version explicitly",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#defaults-and-overrides",
    "href": "usage.html#defaults-and-overrides",
    "title": "Using Flow",
    "section": "Defaults and Overrides",
    "text": "Defaults and Overrides\nInspect Flow provides a powerful defaults system to avoid repetition when configuring evaluations. The FlowDefaults field lets you set default values that cascade across tasks, models, solvers, and agents—with more specific settings overriding less specific ones.\n\nThe FlowDefaults System\nFlowDefaults supports multiple levels of default configuration:\nfrom inspect_flow import FlowJob, FlowDefaults, FlowGenerateConfig, FlowModel\n\nFlowJob(\n    defaults=FlowDefaults(\n1        config=FlowGenerateConfig(\n            max_connections=10,\n        ),\n2        model=FlowModel(\n            model_args={\"arg\": \"foo\"},\n        ),\n3        model_prefix={\n            \"openai/\": FlowModel(\n                config=FlowGenerateConfig(\n                    max_connections=20\n                ),\n            ),\n        },\n4        solver=FlowSolver(...),\n5        solver_prefix={\"chain_of_thought\": ...},\n6        agent=FlowAgent(...),\n7        agent_prefix={\"inspect/\": ...},\n        task=FlowTask(...),\n        task_prefix={\"inspect_evals/\": ...},\n    ),\n    tasks=[...]\n)\n\n1\n\nDefault model generation options. Will be overriden by settings on the FlowModel and FlowTask.\n\n2\n\nField defaults for models.\n\n3\n\nModel defaults for model name prefixes. Overrides FlowDefaults.config and FlowDefaults.model. If multiple prefixes match, longest prefix wins.\n\n4\n\nField defaults for solvers.\n\n5\n\nSolver defaults for solver name prefixes. Overrides FlowDefaults.solver.\n\n6\n\nField defaults for tasks.\n\n7\n\nTask defaults for task name prefixes. Overrides FlowDefaults.config and FlowDefaults.task. If multiple prefixes match, longest prefix wins.\n\n\n\n\nMerge Priority and Behavior\nDefaults follow a clear hierarchy where more specific settings override less specific ones:\nFor Models:\n\nGlobal config defaults (defaults.config)\nGlobal model defaults (defaults.model)\nModel prefix defaults (defaults.model_prefix)\nTask-specific config (task.config)\nModel-specific config (model.config) — highest priority\n\nExample hierarchy in action:\nFlowJob(\n    defaults=FlowDefaults(\n        config=FlowGenerateConfig(\n1            temperature=0.0,\n            max_tokens=100,\n        ),\n        model_prefix={\n            \"openai/\": FlowModel(\n2                config=FlowGenerateConfig(temperature=0.5)\n            )\n        },\n    ),\n    tasks=[\n        FlowTask(\n            name=\"task\",\n3            config=FlowGenerateConfig(temperature=0.7),\n            model=FlowModel(\n                name=\"openai/gpt-4o\",\n4                config=FlowGenerateConfig(temperature=1.0)\n            ),\n        )\n    ]\n)\n\n1\n\nGlobal defaults: temperature=0.0, max_tokens=100\n\n2\n\nPrefix defaults override: temperature=0.5 (for OpenAI models)\n\n3\n\nTask config overrides: temperature=0.7\n\n4\n\nModel config wins: temperature=1.0, max_tokens=100\n\n\nFinal result: temperature=1.0 (most specific), max_tokens=100 (from global defaults)\n\n\n\n\n\n\nNoteNone Values Don’t Override\n\n\n\nSetting a field to None means “not specified” — it won’t override existing values from defaults. This allows partial configs to merge cleanly:\nFlowTask(\n    config=FlowGenerateConfig(temperature=0.5, max_tokens=None)\n)\nThe max_tokens=None doesn’t override a default max_tokens value; it’s simply not set at this level.\n\n\n\n\nCLI Overrides\nOverride config values at runtime using the --set flag:\nBasic usage:\nflow run config.py --set flow_dir=./custom_logs\nNested paths:\nflow run config.py --set options.limit=10\nflow run config.py --set defaults.solver.args.tool_calls=none\nJSON dicts:\nflow run config.py --set 'options.metadata={\"experiment\": \"baseline\", \"version\": \"v1\"}'\nMultiple overrides:\nflow run config.py \\\n  --set flow_dir=./logs/experiment1 \\\n  --set options.limit=100 \\\n  --set defaults.config.temperature=0.5\n\n\n\n\n\n\nNoteOverride Behavior\n\n\n\n\nStrings: Replace existing values\nDicts: Replace existing values\nLists:\n\nString values append to existing list\nLists replace existing list\n\n\nExamples:\n# Appends to list\n--set dependencies=new_package\n\n# Replaces list\n--set 'dependencies=[\"pkg1\", \"pkg2\"]'\n\n\n\n\nEnvironment Variables\nSet config values via environment variables:\nexport INSPECT_FLOW_DIR=./logs/custom\nexport INSPECT_FLOW_LIMIT=50\nflow run config.py\nSupported environment variables:\n\n\n\n\n\n\n\n\nVariable\nEquivalent Flag\nDescription\n\n\n\n\nINSPECT_FLOW_DIR\n--flow-dir\nOverride log directory\n\n\nINSPECT_FLOW_LIMIT\n--limit\nLimit number of samples\n\n\n\n\n\n\n\n\n\nTipOverride Priority\n\n\n\nSetting defaults via the command line will override the defaults which in turn might be overridden by anything set explicitly.\nDedicated environment variables (see Supported environment variables above) and corresponding CLI flags will override the --set flag.\nDedicated CLI flags have the highest priority.\nPriority order of flow-dir and limit:\n\nFlowJob defaults\nExplicit setting on the FlowJob\nCLI --set flags\nEnvironment variables\nExplitic --flow-dir and --limit CLI flags\n\nAll other settings follow the priority order:\n\nFlowJob defaults\nExplicit setting on the FlowJob\nCLI --set flags\n\n\n\n\n\nDebugging Defaults Resolution\nTo see the fully resolved configuration with all defaults applied:\nflow config config.py --resolve\nThis shows exactly what settings each task will use after applying all defaults and overrides.",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#parameter-sweeping",
    "href": "usage.html#parameter-sweeping",
    "title": "Using Flow",
    "section": "Parameter Sweeping",
    "text": "Parameter Sweeping\nParameter sweeping lets you systematically explore evaluation configurations by generating Cartesian products of parameters. Instead of manually writing every combination, Flow provides matrix and “with” functions to declaratively generate evaluation grids.\n\nMatrix Functions (Cartesian Products)\nMatrix functions generate all combinations of their parameters using Cartesian products.\n\ntasks_matrix()\nGenerate task configurations by combining tasks with models, configs, solvers, and arguments:\nfrom inspect_flow import FlowJob, tasks_matrix\n\n\nFlowJob(\n    tasks=tasks_matrix(\n        task=[\"inspect_evals/gpqa_diamond\", \"inspect_evals/mmlu_0_shot\"],\n        model=[\"openai/gpt-4o\", \"anthropic/claude-3-5-sonnet\"],\n    )\n)\nThis creates 4 tasks (2 tasks × 2 models).\nAvailable parameters:\n\ntask: Single task or list of tasks (required)\nmodel: Single model or list of models\nconfig: List of FlowGenerateConfig objects\nsolver: List of solvers\nargs: List of argument dictionaries for task constructors\nmodel_roles: List of role-to-model mappings (for multi-model tasks)\n\n\n\nmodels_matrix()\nGenerate model configurations with different generation settings:\nfrom inspect_flow import FlowJob, FlowGenerateConfig, tasks_matrix, models_matrix\n\n\nFlowJob(\n    tasks=tasks_matrix(\n        task=\"inspect_evals/gpqa_diamond\",\n        model=models_matrix(\n            model=[\"openai/gpt-4o\", \"openai/gpt-4o-mini\"],\n            config=[\n                FlowGenerateConfig(temperature=0.0),\n                FlowGenerateConfig(temperature=0.5),\n                FlowGenerateConfig(temperature=1.0),\n            ]\n        )\n    )\n)\nThis creates 6 tasks (1 task × 2 models × 3 temperatures).\n\n\nconfigs_matrix()\nGenerate generation config combinations by specifying individual parameters:\nfrom inspect_flow import FlowJob, tasks_matrix, models_matrix, configs_matrix\n\n\nFlowJob(\n    tasks=tasks_matrix(\n        task=\"inspect_evals/gpqa_diamond\",\n        model=models_matrix(\n            model=[\"openai/gpt-4o\", \"openai/gpt-4o-mini\"],\n            config=configs_matrix(\n                temperature=[0.0, 0.5, 1.0],\n                max_tokens=[1000, 2000],\n            )\n        )\n    )\n)\nThis creates 12 tasks (1 task × 2 models × 3 temperatures × 2 max_tokens).\n\n\nsolvers_matrix() and agents_matrix()\nGenerate solver or agent configurations with different arguments:\nfrom inspect_flow import FlowJob, tasks_matrix, solvers_matrix\n\nFlowJob(\n    tasks=tasks_matrix(\n        task=\"my_task\",\n        solver=solvers_matrix(\n            solver=\"chain_of_thought\",\n            args=[\n                {\"max_iterations\": 3},\n                {\"max_iterations\": 5},\n                {\"max_iterations\": 10},\n            ]\n        )\n    )\n)\nThis creates 3 tasks (1 task × 3 solver configurations).\n\n\n\nWith Functions (Apply to All)\n“With” functions apply the same value to all items in a list. Use these when you want to sweep over some parameters while keeping others constant.\n\ntasks_with()\nApply common settings to multiple tasks:\nfrom inspect_flow import FlowJob, FlowGenerateConfig, tasks_with\n\n\nFlowJob(\n    tasks=tasks_with(\n        task=[\"inspect_evals/gpqa_diamond\", \"inspect_evals/mmlu_0_shot\"],\n        model=\"openai/gpt-4o\",  # Same model for both tasks\n        config=FlowGenerateConfig(temperature=0.7),  # Same config for both\n    )\n)\nThis creates 2 tasks (2 tasks, each with the same model and config).\n\n\nCombining Matrix and With\nMix parameter sweeps with common settings:\nfrom inspect_flow import FlowJob, FlowGenerateConfig, tasks_with, tasks_matrix, configs_matrix\n\n\nFlowJob(\n    tasks=tasks_with(\n        task=tasks_matrix(\n            task=[\"task1\", \"task2\"],\n            config=configs_matrix(\n                temperature=[0.0, 0.5, 1.0]\n            )\n        ),  # Creates 6 tasks (2 × 3)\n        model=\"openai/gpt-4o\",  # Applied to all 6 tasks\n        sandbox=\"docker\",  # Applied to all 6 tasks\n    )\n)\n\n\n\nNested Sweeps\nMatrix functions can be nested to create complex parameter grids. Use the unpacking operator * to expand inner matrix results:\nExample: Tasks with nested model sweep\nfrom inspect_flow import FlowJob, FlowGenerateConfig, tasks_matrix, models_matrix\n\n\nFlowJob(\n    tasks=tasks_matrix(\n        task=[\"inspect_evals/mmlu_0_shot\", \"inspect_evals/gpqa_diamond\"],\n        model=[\n            \"anthropic/claude-3-5-sonnet\",  # Single model\n            *models_matrix(  # Unpacks list of 4 model configs\n                model=[\"openai/gpt-4o\", \"openai/gpt-4o-mini\"],\n                config=[\n                    FlowGenerateConfig(reasoning_effort=\"low\"),\n                    FlowGenerateConfig(reasoning_effort=\"high\"),\n                ]\n            ),\n        ],  # Total: 1 + 4 = 5 models\n    )\n)\nThis creates 10 tasks (2 tasks × 5 model configurations).\nExample: Tasks with nested task sweep\nfrom inspect_flow import FlowJob, FlowTask, tasks_matrix\n\n\nFlowJob(\n    tasks=tasks_matrix(\n        task=[\n            FlowTask(name=\"task1\", args={\"subset\": \"test\"}),  # Single task\n            *tasks_matrix(  # Unpacks list of 3 tasks\n                task=\"task2\",\n                args=[\n                    {\"language\": \"en\"},\n                    {\"language\": \"de\"},\n                    {\"language\": \"fr\"},\n                ]\n            ),\n        ],  # Total: 1 + 3 = 4 tasks\n        model=[\"model1\", \"model2\"],\n    )\n)\nThis creates 8 tasks (4 task variants × 2 models).\n\n\n\n\n\n\nWarningWatch Out for Combinatorial Explosion\n\n\n\nParameter sweeps grow multiplicatively. A sweep with: - 3 tasks - 4 models - 5 temperature values - 3 solver configurations\nResults in 3 × 4 × 5 × 3 = 180 evaluations.\nAlways use --dry-run to check the number of evaluations before running expensive grids.\n\n\n\n\nConfig Merging Behavior\nWhen base objects already have values, matrix parameters are merged:\n# Config fields merge recursively\ntasks_matrix(\n    task=FlowTask(\n        name=\"task\",\n        config=FlowGenerateConfig(temperature=0.5)  # Base value\n    ),\n    config=[\n        FlowGenerateConfig(max_tokens=1000),  # Adds max_tokens, keeps temperature=0.5\n        FlowGenerateConfig(max_tokens=2000),  # Adds max_tokens, keeps temperature=0.5\n    ]\n)",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#running-evaluations",
    "href": "usage.html#running-evaluations",
    "title": "Using Flow",
    "section": "Running Evaluations",
    "text": "Running Evaluations\nOnce you’ve defined your Flow configuration, you can execute evaluations using the flow run command. Flow also provides tools for previewing configurations and controlling runtime behavior.\n\nThe flow run Command\nExecute your evaluation workflow:\nflow run config.py\nWhat happens when you run this:\n\nFlow loads your configuration file\nCreates an isolated virtual environment in ~/.cache/inspect-flow/\nInstalls dependencies\nResolves all defaults and matrix expansions\nExecutes evaluations via Inspect AI’s eval_set()\nStores logs in flow_dir (default: logs/flow/)\nCleans up the temporary environment\n\n\nCommon CLI Flags\nPreview without running:\nflow run config.py --dry-run\nShows how many tasks would be executed without actually running them. Useful for validating large parameter sweeps.\neval_set would be called with 24 tasks\nLimit samples for testing:\nflow run config.py --limit 10\nOnly evaluates the first 10 samples per task. Perfect for quick testing before running expensive evaluations.\nOverride log directory:\nflow run config.py --flow-dir ./experiments/baseline\nChanges where logs and results are stored.\nRuntime overrides:\nflow run config.py \\\n  --set options.limit=100 \\\n  --set defaults.config.temperature=0.5 \\\n  --set flow_dir=./logs/experiment1\nOverride any configuration value at runtime. See CLI Overrides for more details.\n\n\n\nThe flow config Command\nPreview your configuration before running:\nBasic usage:\nflow config config.py\nDisplays the parsed configuration as YAML with CLI overrides applied. Does not create a virtual environment or instantiate Python objects.\nFull resolution:\nflow config config.py --resolve\nShows the completely resolved configuration: - Creates virtual environment - Applies all defaults - Expands all matrix functions - Instantiates all Python objects\nThis is invaluable for debugging what settings will actually be used in your evaluations.\n\n\n\n\n\n\nTipWhen to Use Each Command\n\n\n\n\nflow config - Quick syntax check, verify overrides\nflow config --resolve - Debug defaults resolution, inspect final settings\nflow run --dry-run - Count tasks in parameter sweeps, validate before expensive runs\nflow run - Execute evaluations\n\n\n\n\n\nResults and Logs\n\nFlow Directory Structure\nEvaluation results are stored in the flow_dir (default: logs/flow/):\nlogs/flow/\n├── logs/\n│   ├── 2025-11-11T10-44-39+01-00_gpqa-diamond_XuAMHBzFSPgCE5M6mDzuBM.eval\n│   ├── 2025-11-11T10-44-39+01-00_mmlu-0-shot_Vnu2A3M2wPet5yobLiCQmZ.eval\n│   └── ...\n└── 2025-11-11T10-44-39+01-00_flow.yaml\nDirectory structure:\n\nFlow controls the flow_dir path and passes {flow_dir}/logs/ to Inspect AI\nInspect AI handles the actual evaluation log file naming and storage\nLog file naming conventions follow Inspect AI’s standards (see Inspect AI logging docs)\nFlow automatically saves the resolved configuration before execution as {timestamp}_flow.yaml\n\nLog formats:\n\n.eval - Binary Inspect AI log format (default, high-performance)\n.json - JSON format (if log_format=\"json\" in FlowOptions)\n\n\n\nViewing Results\nUsing Inspect View:\ninspect view\nOpens the Inspect AI viewer to explore evaluation logs interactively.\n\n\nS3 Support\nStore logs directly to S3:\nFlowJob(\n    flow_dir=\"s3://my-bucket/experiments/baseline\",\n    tasks=[...]\n)",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "usage.html#advanced-features",
    "href": "usage.html#advanced-features",
    "title": "Using Flow",
    "section": "Advanced Features",
    "text": "Advanced Features\n\nMetadata Management\nFlow supports two types of metadata with distinct purposes: metadata and flow_metadata.\n\nmetadata (Inspect AI Metadata)\nThe metadata field in FlowOptions and FlowTask is passed directly to Inspect AI and stored in evaluation logs. Use this for tracking experiment information that should be accessible in Inspect AI’s log viewer and analysis tools.\nExample:\nfrom inspect_flow.types import FlowJob, FlowTask, FlowOptions\n\nFlowJob(\n    options=FlowOptions(\n        metadata={\n            \"experiment\": \"baseline_v1\",\n            \"hypothesis\": \"Higher temperature improves creative tasks\",\n            \"hardware\": \"A100-80GB\",\n        }\n    ),\n    tasks=[\n        FlowTask(\n            name=\"inspect_evals/gpqa_diamond\",\n            model=\"openai/gpt-4o\",\n            metadata={\n                \"task_variant\": \"chemistry_subset\",\n                \"note\": \"Testing with reduced context\",\n            }\n        )\n    ]\n)\nThe metadata from FlowOptions is applied globally to all tasks in the evaluation run, while task-level metadata is specific to each task.\n\n\nflow_metadata (Flow-Only Metadata)\nThe flow_metadata field is available on FlowJob, FlowTask, FlowModel, FlowSolver, and FlowAgent. This metadata is not passed to Inspect AI—it exists only in the Flow configuration and is useful for configuration-time logic and organization.\nUse cases:\n\nFiltering or selecting configurations based on properties\nOrganizing complex configuration generation logic\nDocumenting configuration decisions\nAnnotating configs without polluting Inspect AI logs\n\nExample: Configuration-time filtering\nfrom inspect_flow import FlowJob, FlowModel, tasks_matrix\n\n\n# Define models with metadata about capabilities\nmodels = [\n    FlowModel(\"openai/gpt-4o\", flow_metadata={\"context_window\": 128000}),\n    FlowModel(\"openai/gpt-4o-mini\", flow_metadata={\"context_window\": 128000}),\n    FlowModel(\"anthropic/claude-3-5-sonnet\", flow_metadata={\"context_window\": 200000}),\n]\n\n# Filter to only long-context models\nlong_context_models = [\n    m for m in models\n    if m.flow_metadata and m.flow_metadata.get(\"context_window\", 0) &gt;= 128000\n]\n\nFlowJob(\n    tasks=tasks_matrix(\n        task=\"long_context_task\",\n        model=long_context_models,\n    )\n)",
    "crumbs": [
      "Welcome",
      "Using Flow"
    ]
  },
  {
    "objectID": "reference/inspect_flow.api.html",
    "href": "reference/inspect_flow.api.html",
    "title": "inspect_flow.api",
    "section": "",
    "text": "Options for loading a configuration file.\n\nSource\n\nclass ConfigOptions(TypedDict, total=False)\n\n\n\nPrint the flow configuration.\n\nSource\n\ndef config(\n    config: FlowJob,\n    resolve: bool = False,\n) -&gt; None\n\nconfig FlowJob\n\nThe flow configuration.\n\nresolve bool\n\nIf True, resolve the configuration before printing.\n\n\n\n\n\nLoad a configuration file and apply any overrides.\n\nSource\n\ndef load_config(config_file: str, **kwargs: Unpack[ConfigOptions]) -&gt; FlowJob\n\nconfig_file str\n\nThe path to the configuration file.\n\n**kwargs Unpack[ConfigOptions]\n\nConfiguration options. See ConfigOptions for available parameters.\n\n\n\n\n\nRun an inspect_flow evaluation.\n\nSource\n\ndef run(\n    config: FlowJob,\n    dry_run: bool = False,\n) -&gt; None\n\nconfig FlowJob\n\nThe flow configuration.\n\ndry_run bool\n\nIf True, do not run eval, but show a count of tasks that would be run.",
    "crumbs": [
      "Reference",
      "Python API",
      "inspect_flow.api"
    ]
  },
  {
    "objectID": "reference/inspect_flow.api.html#python-api",
    "href": "reference/inspect_flow.api.html#python-api",
    "title": "inspect_flow.api",
    "section": "",
    "text": "Options for loading a configuration file.\n\nSource\n\nclass ConfigOptions(TypedDict, total=False)\n\n\n\nPrint the flow configuration.\n\nSource\n\ndef config(\n    config: FlowJob,\n    resolve: bool = False,\n) -&gt; None\n\nconfig FlowJob\n\nThe flow configuration.\n\nresolve bool\n\nIf True, resolve the configuration before printing.\n\n\n\n\n\nLoad a configuration file and apply any overrides.\n\nSource\n\ndef load_config(config_file: str, **kwargs: Unpack[ConfigOptions]) -&gt; FlowJob\n\nconfig_file str\n\nThe path to the configuration file.\n\n**kwargs Unpack[ConfigOptions]\n\nConfiguration options. See ConfigOptions for available parameters.\n\n\n\n\n\nRun an inspect_flow evaluation.\n\nSource\n\ndef run(\n    config: FlowJob,\n    dry_run: bool = False,\n) -&gt; None\n\nconfig FlowJob\n\nThe flow configuration.\n\ndry_run bool\n\nIf True, do not run eval, but show a count of tasks that would be run.",
    "crumbs": [
      "Reference",
      "Python API",
      "inspect_flow.api"
    ]
  },
  {
    "objectID": "reference/inspect_flow.html",
    "href": "reference/inspect_flow.html",
    "title": "inspect_flow",
    "section": "",
    "text": "Configuration for an Agent.\n\nSource\n\nclass FlowAgent(BaseModel, extra=\"forbid\")\n\n\n\nConfiguration for a flow job.\n\nSource\n\nclass FlowJob(BaseModel, extra=\"forbid\")\n\n\n\nDefault field values for Inspect objects. Will be overriden by more specific settings.\n\nSource\n\nclass FlowDefaults(BaseModel, extra=\"forbid\")\n\n\n\nConfiguration for task epochs.\nNumber of epochs to repeat samples over and optionally one or more reducers used to combine scores from samples across epochs. If not specified the “mean” score reducer is used.\n\nSource\n\nclass FlowEpochs(BaseModel)\n\n\n\nModel generation options.\n\nSource\n\nclass FlowGenerateConfig(GenerateConfig, extra=\"forbid\")\n\n\n\nConfiguration for a Model.\n\nSource\n\nclass FlowModel(BaseModel, extra=\"forbid\")\n\n\n\nEvaluation options.\n\nSource\n\nclass FlowOptions(BaseModel, extra=\"forbid\")\n\n\n\nConfiguration for a Solver.\n\nSource\n\nclass FlowSolver(BaseModel, extra=\"forbid\")\n\n\n\nConfiguration for an evaluation task.\nTasks are the basis for defining and running evaluations.\n\nSource\n\nclass FlowTask(BaseModel, extra=\"forbid\")\n\n\n\nmodel_name str | None\n\nGet the model name from the model field.\nReturns: The model name if set, otherwise None.",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  },
  {
    "objectID": "reference/inspect_flow.html#types",
    "href": "reference/inspect_flow.html#types",
    "title": "inspect_flow",
    "section": "",
    "text": "Configuration for an Agent.\n\nSource\n\nclass FlowAgent(BaseModel, extra=\"forbid\")\n\n\n\nConfiguration for a flow job.\n\nSource\n\nclass FlowJob(BaseModel, extra=\"forbid\")\n\n\n\nDefault field values for Inspect objects. Will be overriden by more specific settings.\n\nSource\n\nclass FlowDefaults(BaseModel, extra=\"forbid\")\n\n\n\nConfiguration for task epochs.\nNumber of epochs to repeat samples over and optionally one or more reducers used to combine scores from samples across epochs. If not specified the “mean” score reducer is used.\n\nSource\n\nclass FlowEpochs(BaseModel)\n\n\n\nModel generation options.\n\nSource\n\nclass FlowGenerateConfig(GenerateConfig, extra=\"forbid\")\n\n\n\nConfiguration for a Model.\n\nSource\n\nclass FlowModel(BaseModel, extra=\"forbid\")\n\n\n\nEvaluation options.\n\nSource\n\nclass FlowOptions(BaseModel, extra=\"forbid\")\n\n\n\nConfiguration for a Solver.\n\nSource\n\nclass FlowSolver(BaseModel, extra=\"forbid\")\n\n\n\nConfiguration for an evaluation task.\nTasks are the basis for defining and running evaluations.\n\nSource\n\nclass FlowTask(BaseModel, extra=\"forbid\")\n\n\n\nmodel_name str | None\n\nGet the model name from the model field.\nReturns: The model name if set, otherwise None.",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  },
  {
    "objectID": "reference/inspect_flow.html#functions",
    "href": "reference/inspect_flow.html#functions",
    "title": "inspect_flow",
    "section": "Functions",
    "text": "Functions\n\nagents_matrix\nCreate a list of agents from the product of lists of field values.\n\nSource\n\ndef agents_matrix(\n    *,\n    agent: str | FlowAgent | Sequence[str | FlowAgent],\n    **kwargs: Unpack[FlowAgentMatrixDict],\n) -&gt; list[FlowAgent]\n\nagent str | FlowAgent | Sequence[str | FlowAgent]\n\nThe agent or list of agents to matrix.\n\n**kwargs Unpack[FlowAgentMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nagents_with\nSet fields on a list of agents.\n\nSource\n\ndef agents_with(\n    *,\n    agent: str | FlowAgent | Sequence[str | FlowAgent],\n    **kwargs: Unpack[FlowAgentDict],\n) -&gt; list[FlowAgent]\n\nagent str | FlowAgent | Sequence[str | FlowAgent]\n\nThe agent or list of agents to set fields on.\n\n**kwargs Unpack[FlowAgentDict]\n\nThe fields to set on each agent.\n\n\n\n\nconfigs_matrix\nCreate a list of generate configs from the product of lists of field values.\n\nSource\n\ndef configs_matrix(\n    *,\n    config: FlowGenerateConfig | Sequence[FlowGenerateConfig] | None = None,\n    **kwargs: Unpack[FlowGenerateConfigMatrixDict],\n) -&gt; list[FlowGenerateConfig]\n\nconfig FlowGenerateConfig | Sequence[FlowGenerateConfig] | None\n\nThe config or list of configs to matrix.\n\n**kwargs Unpack[FlowGenerateConfigMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nconfigs_with\nSet fields on a list of generate configs.\n\nSource\n\ndef configs_with(\n    *,\n    config: FlowGenerateConfig | Sequence[FlowGenerateConfig],\n    **kwargs: Unpack[FlowGenerateConfigDict],\n) -&gt; list[FlowGenerateConfig]\n\nconfig FlowGenerateConfig | Sequence[FlowGenerateConfig]\n\nThe config or list of configs to set fields on.\n\n**kwargs Unpack[FlowGenerateConfigDict]\n\nThe fields to set on each config.\n\n\n\n\nmerge\nMerge two flow objects.\n\nSource\n\ndef merge(base: _T, add: _T) -&gt; _T\n\nbase _T\n\nThe base object.\n\nadd _T\n\nThe object to merge into the base. Values in this object will override those in the base.\n\n\n\n\nmodels_matrix\nCreate a list of models from the product of lists of field values.\n\nSource\n\ndef models_matrix(\n    *,\n    model: str | FlowModel | Sequence[str | FlowModel],\n    **kwargs: Unpack[FlowModelMatrixDict],\n) -&gt; list[FlowModel]\n\nmodel str | FlowModel | Sequence[str | FlowModel]\n\nThe model or list of models to matrix.\n\n**kwargs Unpack[FlowModelMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nmodels_with\nSet fields on a list of models.\n\nSource\n\ndef models_with(\n    *,\n    model: str | FlowModel | Sequence[str | FlowModel],\n    **kwargs: Unpack[FlowModelDict],\n) -&gt; list[FlowModel]\n\nmodel str | FlowModel | Sequence[str | FlowModel]\n\nThe model or list of models to set fields on.\n\n**kwargs Unpack[FlowModelDict]\n\nThe fields to set on each model.\n\n\n\n\nsolvers_matrix\nCreate a list of solvers from the product of lists of field values.\n\nSource\n\ndef solvers_matrix(\n    *,\n    solver: str | FlowSolver | Sequence[str | FlowSolver],\n    **kwargs: Unpack[FlowSolverMatrixDict],\n) -&gt; list[FlowSolver]\n\nsolver str | FlowSolver | Sequence[str | FlowSolver]\n\nThe solver or list of solvers to matrix.\n\n**kwargs Unpack[FlowSolverMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\nsolvers_with\nSet fields on a list of solvers.\n\nSource\n\ndef solvers_with(\n    *,\n    solver: str | FlowSolver | Sequence[str | FlowSolver],\n    **kwargs: Unpack[FlowSolverDict],\n) -&gt; list[FlowSolver]\n\nsolver str | FlowSolver | Sequence[str | FlowSolver]\n\nThe solver or list of solvers to set fields on.\n\n**kwargs Unpack[FlowSolverDict]\n\nThe fields to set on each solver.\n\n\n\n\ntasks_matrix\nCreate a list of tasks from the product of lists of field values.\n\nSource\n\ndef tasks_matrix(\n    *,\n    task: str | FlowTask | Sequence[str | FlowTask],\n    **kwargs: Unpack[FlowTaskMatrixDict],\n) -&gt; list[FlowTask]\n\ntask str | FlowTask | Sequence[str | FlowTask]\n\nThe task or list of tasks to matrix.\n\n**kwargs Unpack[FlowTaskMatrixDict]\n\nThe lists of field values to matrix.\n\n\n\n\ntasks_with\nSet fields on a list of tasks.\n\nSource\n\ndef tasks_with(\n    *,\n    task: str | FlowTask | Sequence[str | FlowTask],\n    **kwargs: Unpack[FlowTaskDict],\n) -&gt; list[FlowTask]\n\ntask str | FlowTask | Sequence[str | FlowTask]\n\nThe task or list of tasks to set fields on.\n\n**kwargs Unpack[FlowTaskDict]\n\nThe fields to set on each task.",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  }
]