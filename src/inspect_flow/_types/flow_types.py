from typing import (
    Any,
    Dict,
    List,
    Literal,
    Mapping,
    NotRequired,
    Optional,
    TypeAlias,
    TypedDict,
    TypeVar,
    Union,
    overload,
)

from inspect_ai.approval._policy import (
    ApprovalPolicyConfig,
    ApproverPolicyConfig,
)  # TODO:ransom private import
from inspect_ai.model import BatchConfig, GenerateConfig, ResponseSchema
from inspect_ai.util import (
    DisplayType,
    JSONSchema,
    SandboxEnvironmentSpec,
    SandboxEnvironmentType,
)
from pydantic import BaseModel, Field, field_validator, model_validator
from typing_extensions import Unpack

from inspect_flow._util.list_util import ensure_list_or_none

CreateArgs: TypeAlias = Mapping[str, Any]
ModelRolesConfig: TypeAlias = Mapping[str, Union["ModelConfig", str]]

# BEGIN GENERATED CODE
# generated by datamodel-codegen:
#   filename:  <stdin>
#   timestamp: 2025-10-22T18:30:15+00:00


class AgentConfigDict(TypedDict):
    name: str
    args: NotRequired[Optional[List[Dict[str, Any]]]]


class ApproverPolicyConfigDict(TypedDict):
    name: str
    tools: Union[str, List[str]]
    params: NotRequired[Dict[str, Any]]


class BatchConfigDict(TypedDict):
    size: NotRequired[Optional[int]]
    max_size: NotRequired[Optional[int]]
    send_delay: NotRequired[Optional[float]]
    tick: NotRequired[Optional[float]]
    max_batches: NotRequired[Optional[int]]
    max_consecutive_check_failures: NotRequired[Optional[int]]


class EpochsConfigDict(TypedDict):
    epochs: int
    reducer: NotRequired[Optional[Union[str, List[str]]]]


class FlowOptionsDict(TypedDict):
    log_dir: str


class JSONSchemaDict(TypedDict):
    type: NotRequired[
        Optional[
            Literal["string", "integer", "number", "boolean", "array", "object", "null"]
        ]
    ]
    format: NotRequired[Optional[str]]
    description: NotRequired[Optional[str]]
    default: NotRequired[Any]
    enum: NotRequired[Optional[List]]
    items: NotRequired[Optional[Union["JSONSchema", "JSONSchemaDict"]]]
    properties: NotRequired[Optional[Dict[str, Union["JSONSchema", "JSONSchemaDict"]]]]
    additionalProperties: NotRequired[
        Optional[Union[Union["JSONSchema", "JSONSchemaDict"], bool]]
    ]
    anyOf: NotRequired[Optional[List[Union["JSONSchema", "JSONSchemaDict"]]]]
    required: NotRequired[Optional[List[str]]]


class ResponseSchemaDict(TypedDict):
    name: str
    json_schema: Union["JSONSchema", "JSONSchemaDict"]
    description: NotRequired[Optional[str]]
    strict: NotRequired[Optional[bool]]


class SandboxEnvironmentSpecDict(TypedDict):
    type: str
    config: NotRequired[Any]


class SolverConfigDict(TypedDict):
    name: str
    args: NotRequired[Optional[List[Dict[str, Any]]]]


class ApprovalPolicyConfigDict(TypedDict):
    approvers: List[Union["ApproverPolicyConfig", "ApproverPolicyConfigDict"]]


class GenerateConfigDict(TypedDict):
    max_retries: NotRequired[Optional[int]]
    timeout: NotRequired[Optional[int]]
    max_connections: NotRequired[Optional[int]]
    system_message: NotRequired[Optional[str]]
    max_tokens: NotRequired[Optional[int]]
    top_p: NotRequired[Optional[float]]
    temperature: NotRequired[Optional[float]]
    stop_seqs: NotRequired[Optional[List[str]]]
    best_of: NotRequired[Optional[int]]
    frequency_penalty: NotRequired[Optional[float]]
    presence_penalty: NotRequired[Optional[float]]
    logit_bias: NotRequired[Optional[Dict[str, float]]]
    seed: NotRequired[Optional[int]]
    top_k: NotRequired[Optional[int]]
    num_choices: NotRequired[Optional[int]]
    logprobs: NotRequired[Optional[bool]]
    top_logprobs: NotRequired[Optional[int]]
    parallel_tool_calls: NotRequired[Optional[bool]]
    internal_tools: NotRequired[Optional[bool]]
    max_tool_output: NotRequired[Optional[int]]
    cache_prompt: NotRequired[Optional[Union[str, bool]]]
    reasoning_effort: NotRequired[Optional[Literal["minimal", "low", "medium", "high"]]]
    reasoning_tokens: NotRequired[Optional[int]]
    reasoning_summary: NotRequired[Optional[Literal["concise", "detailed", "auto"]]]
    reasoning_history: NotRequired[Optional[Literal["none", "all", "last", "auto"]]]
    response_schema: NotRequired[
        Optional[Union["ResponseSchema", "ResponseSchemaDict"]]
    ]
    extra_body: NotRequired[Optional[Dict[str, Any]]]
    batch: NotRequired[
        Optional[Union[bool, int, Union["BatchConfig", "BatchConfigDict"]]]
    ]


class ModelConfigDict(TypedDict):
    name: str
    role: NotRequired[Optional[str]]
    default: NotRequired[Optional[str]]
    config: NotRequired[Optional[List[Union["GenerateConfig", "GenerateConfigDict"]]]]
    base_url: NotRequired[Optional[str]]
    api_key: NotRequired[Optional[str]]
    memoize: NotRequired[bool]
    model_args: NotRequired[Optional[Dict[str, Any]]]


class TaskConfigDict(TypedDict):
    name: NotRequired[Optional[str]]
    file: NotRequired[Optional[str]]
    file_attr: NotRequired[Optional[str]]
    registry_name: NotRequired[Optional[str]]
    args: NotRequired[Optional[List[Dict[str, Any]]]]
    solvers: NotRequired[
        Optional[
            List[
                Union[
                    Union[str, "SolverConfig", "SolverConfigDict"],
                    List[Union[str, "SolverConfig", "SolverConfigDict"]],
                    Union["AgentConfig", "AgentConfigDict"],
                ]
            ]
        ]
    ]
    models: NotRequired[Optional[List[Union[str, "ModelConfig", "ModelConfigDict"]]]]
    config: NotRequired[Optional[Union["GenerateConfig", "GenerateConfigDict"]]]
    model_roles: NotRequired[
        Optional[
            List[Dict[str, Union[Union[str, "ModelConfig", "ModelConfigDict"], str]]]
        ]
    ]
    sandbox: NotRequired[
        Optional[
            Union[
                str, List, Union["SandboxEnvironmentSpec", "SandboxEnvironmentSpecDict"]
            ]
        ]
    ]
    approval: NotRequired[
        Optional[Union[str, Union["ApprovalPolicyConfig", "ApprovalPolicyConfigDict"]]]
    ]
    epochs: NotRequired[Optional[Union[int, Union["EpochsConfig", "EpochsConfigDict"]]]]
    fail_on_error: NotRequired[Optional[Union[bool, float]]]
    continue_on_fail: NotRequired[Optional[bool]]
    message_limit: NotRequired[Optional[int]]
    token_limit: NotRequired[Optional[int]]
    time_limit: NotRequired[Optional[int]]
    working_limit: NotRequired[Optional[int]]
    version: NotRequired[Optional[int]]
    metadata: NotRequired[Optional[Dict[str, Any]]]
    sample_id: NotRequired[Optional[Union[str, int, List[Union[str, int]]]]]


class EvalSetOptionsDict(TypedDict):
    retry_attempts: NotRequired[Optional[int]]
    retry_wait: NotRequired[Optional[float]]
    retry_connections: NotRequired[Optional[float]]
    retry_cleanup: NotRequired[Optional[bool]]
    sandbox: NotRequired[
        Optional[
            Union[
                str, List, Union["SandboxEnvironmentSpec", "SandboxEnvironmentSpecDict"]
            ]
        ]
    ]
    sandbox_cleanup: NotRequired[Optional[bool]]
    tags: NotRequired[Optional[List[str]]]
    metadata: NotRequired[Optional[Dict[str, Any]]]
    trace: NotRequired[Optional[bool]]
    display: NotRequired[
        Optional[Literal["full", "conversation", "rich", "plain", "log", "none"]]
    ]
    approval: NotRequired[
        Optional[Union[str, Union["ApprovalPolicyConfig", "ApprovalPolicyConfigDict"]]]
    ]
    score: NotRequired[bool]
    log_level: NotRequired[Optional[str]]
    log_level_transcript: NotRequired[Optional[str]]
    log_format: NotRequired[Optional[Literal["eval", "json"]]]
    limit: NotRequired[Optional[int]]
    sample_shuffle: NotRequired[Optional[Union[bool, int]]]
    fail_on_error: NotRequired[Optional[Union[bool, float]]]
    continue_on_fail: NotRequired[Optional[bool]]
    retry_on_error: NotRequired[Optional[int]]
    debug_errors: NotRequired[Optional[bool]]
    max_samples: NotRequired[Optional[int]]
    max_tasks: NotRequired[Optional[int]]
    max_subprocesses: NotRequired[Optional[int]]
    max_sandboxes: NotRequired[Optional[int]]
    log_samples: NotRequired[Optional[bool]]
    log_realtime: NotRequired[Optional[bool]]
    log_images: NotRequired[Optional[bool]]
    log_buffer: NotRequired[Optional[int]]
    log_shared: NotRequired[Optional[bool]]
    log_dir_allow_dirty: NotRequired[Optional[bool]]
    config: NotRequired[Optional[Union["GenerateConfig", "GenerateConfigDict"]]]


class MatrixDict(TypedDict):
    tasks: List[Union[str, "TaskConfig", "TaskConfigDict"]]
    args: NotRequired[Optional[List[Dict[str, Any]]]]
    models: NotRequired[Optional[List[Union[str, "ModelConfig", "ModelConfigDict"]]]]
    model_roles: NotRequired[
        Optional[
            List[Dict[str, Union[Union[str, "ModelConfig", "ModelConfigDict"], str]]]
        ]
    ]
    solvers: NotRequired[
        Optional[
            List[
                Union[
                    Union[str, "SolverConfig", "SolverConfigDict"],
                    List[Union[str, "SolverConfig", "SolverConfigDict"]],
                    Union["AgentConfig", "AgentConfigDict"],
                ]
            ]
        ]
    ]


class FlowConfigDict(TypedDict):
    options: NotRequired[Optional[Union["FlowOptions", "FlowOptionsDict"]]]
    eval_set_options: NotRequired[
        Optional[Union["EvalSetOptions", "EvalSetOptionsDict"]]
    ]
    dependencies: NotRequired[Optional[List[str]]]
    matrix: List[Union["Matrix", "MatrixDict"]]


# END GENERATED CODE


class ModelConfig(BaseModel, extra="forbid"):
    name: str = Field(description="Name of the model to use.")

    role: str | None = Field(
        default=None,
        description="Optional named role for model (e.g. for roles specified at the task or eval level). Provide a default as a fallback in the case where the role hasn't been externally specified.",
    )

    default: str | None = Field(
        default=None,
        description="Optional. Fallback model in case the specified model or role is not found. Should be a fully qualified model name (e.g. openai/gpt-4o).",
    )

    # TODO:ransom should we forbid extra on GenerateConfig?
    config: list[GenerateConfig] | None = Field(
        default=None,
        description="Configuration for model. One model is created for each value in the list. Config values will be overridden if set on the task or eval_set_config.",
    )

    base_url: str | None = Field(
        default=None,
        description="Optional. Alternate base URL for model.",
    )

    api_key: str | None = Field(
        default=None,
        description="Optional. API key for model.",
    )

    memoize: bool = Field(
        default=True,
        description="Use/store a cached version of the model based on the parameters to get_model().",
    )

    model_args: CreateArgs | None = Field(
        default=None, description="Additional args to pass to model constructor."
    )

    # Convert single items to lists
    @field_validator("config", mode="before")
    @classmethod
    def convert_to_list(cls, v):
        return ensure_list_or_none(v)


class SolverConfig(BaseModel, extra="forbid"):
    name: str = Field(description="Name of the solver.")

    args: list[CreateArgs] | None = Field(
        default=None,
        description="Solver arguments. One solver is create for each value in the list. Only a single value may be set for solvers in a chain.",
    )

    # Convert single items to lists
    @field_validator("args", mode="before")
    @classmethod
    def convert_to_list(cls, v):
        return ensure_list_or_none(v)


class AgentConfig(BaseModel, extra="forbid"):
    name: str = Field(description="Name of the solver.")

    args: list[CreateArgs] | None = Field(
        default=None,
        description="Agent arguments. One Agent is created for each value in the list.",
    )

    # Convert single items to lists
    @field_validator("args", mode="before")
    @classmethod
    def convert_to_list(cls, v):
        return ensure_list_or_none(v)


class EpochsConfig(BaseModel):
    epochs: int = Field(description="Number of epochs.")

    reducer: str | list[str] | None = Field(
        default=None,
        description='One or more reducers used to combine scores from samples across epochs (defaults to "mean")',
    )


class TaskConfig(BaseModel, extra="forbid"):
    name: str | None = Field(
        default=None,
        description='Task name. If not specified is automatically determined based on the name of the task directory (or "task") if its anonymous task (e.g. created by a function exported from a file.',
    )

    file: str | None = Field(
        default=None,
        description="Python file containing the task implementation. If not provided, the task will be loaded from the registry.",
    )

    file_attr: str | None = Field(
        default=None,
        description="Name of the task factory attr within file. Only used if file is specified. Defaults to 'name'.",
    )

    registry_name: str | None = Field(
        default=None,
        description="Name of the task within the registry. Only used if file is not specified. Defaults to 'name'.",
    )

    args: list[CreateArgs] | None = Field(
        default=None,
        description="Task factory arguments.",
    )

    solvers: list[SolverConfig | list[SolverConfig] | AgentConfig] | None = Field(
        default=None,
        description="List of solver or list of list of solvers. Defaults to generate(), a normal call to the model. Will matrix over items in the top level list.",
    )

    models: list[ModelConfig] | None = Field(
        default=None,
        description="Default model for task (Optional, defaults to eval model). Will matrix over items in the list.",
    )

    config: GenerateConfig | None = Field(
        default=None,
        description="Model generation config for default model (does not apply to model roles). Will override config settings on the ModelConfig. Config values will be overridden if also set on the eval_set_config.",
    )

    model_roles: list[ModelRolesConfig] | None = Field(
        default=None,
        description="Named roles for use in `get_model()`. Will matrix over items in the list.",
    )

    sandbox: SandboxEnvironmentType | None = Field(
        default=None,
        description="Sandbox environment type (or optionally a str or tuple with a shorthand spec)",
    )

    approval: str | ApprovalPolicyConfig | None = Field(
        default=None,
        description="Tool use approval policies. Either a path to an approval policy config file or an approval policy config. Defaults to no approval policy.",
    )

    epochs: int | EpochsConfig | None = Field(
        default=None,
        description='Epochs to repeat samples for and optional score reducer function(s) used to combine sample scores (defaults to "mean")',
    )

    fail_on_error: bool | float | None = Field(
        default=None,
        description="`True` to fail on first sample error(default); `False` to never fail on sample errors; Value between 0 and 1 to fail if a proportion of total samples fails. Value greater than 1 to fail eval if a count of samples fails.",
    )

    continue_on_fail: bool | None = Field(
        default=None,
        description="`True` to continue running and only fail at the end if the `fail_on_error` condition is met. `False` to fail eval immediately when the `fail_on_error` condition is met (default).",
    )

    message_limit: int | None = Field(
        default=None, description="Limit on total messages used for each sample."
    )

    token_limit: int | None = Field(
        default=None, description="Limit on total tokens used for each sample."
    )

    time_limit: int | None = Field(
        default=None, description="Limit on clock time (in seconds) for samples."
    )

    working_limit: int | None = Field(
        default=None,
        description="Limit on working time (in seconds) for sample. Working time includes model generation, tool calls, etc. but does not include time spent waiting on retries or shared resources.",
    )

    version: int | None = Field(
        default=None,
        description="Version of task (to distinguish evolutions of the task spec or breaking changes to it)",
    )

    metadata: dict[str, Any] | None = Field(
        default=None, description="Additional metadata to associate with the task."
    )

    sample_id: str | int | list[str | int] | None = Field(
        default=None,
        description="Evaluate specific sample(s) from the dataset.",
    )

    # Convert single items to lists
    @field_validator("args", "model_roles", mode="before")
    @classmethod
    def convert_to_list(cls, v):
        return ensure_list_or_none(v)

    @field_validator("models", mode="before")
    @classmethod
    def convert_string_models(cls, v):
        return convert_to_class_list(ModelConfig, v)

    @field_validator("solvers", mode="before")
    @classmethod
    def convert_string_solvers(cls, v):
        return convert_to_solver_config_list(v)

    @model_validator(mode="after")
    def validate_field_combinations(self):
        if self.file is not None:
            if self.registry_name is not None:
                raise ValueError(
                    "registry_name cannot be specified when file is specified"
                )
            if self.file_attr is None and self.name is None:
                raise ValueError(
                    "Either file_attr or name must be specified when file is specified"
                )
        elif self.file_attr is not None:
            raise ValueError("file_attr cannot be specified when file is not specified")
        elif self.registry_name is None and self.name is None:
            raise ValueError(
                "Either registry_name or name must be specified when file is not specified"
            )

        return self


class Matrix(BaseModel, extra="forbid"):
    tasks: list[TaskConfig] = Field(
        description="List of tasks to evaluate in this eval set."
    )

    args: list[CreateArgs] | None = Field(
        default=None,
        description="Task arguments or list of task arguments to use for evaluation.",
    )

    models: list[ModelConfig] | None = Field(
        default=None,
        description="Model or list of models to use for evaluation. If not specified, the default model for each task will be used.",
    )

    model_roles: list[ModelRolesConfig] | None = Field(
        default=None,
        description="Model roles to use for evaluation.",
    )

    solvers: list[SolverConfig | list[SolverConfig] | AgentConfig] | None = Field(
        default=None,
        description="Solvers.",
    )

    # Convert single items to lists
    @field_validator("args", "model_roles", mode="before")
    @classmethod
    def convert_to_list(cls, v):
        return ensure_list_or_none(v)

    @field_validator("tasks", mode="before")
    @classmethod
    def convert_string_tasks(cls, v):
        return convert_to_class_list(TaskConfig, v)

    @field_validator("models", mode="before")
    @classmethod
    def convert_string_models(cls, v):
        return convert_to_class_list(ModelConfig, v)

    @field_validator("solvers", mode="before")
    @classmethod
    def convert_string_solvers(cls, v):
        return convert_to_solver_config_list(v)

    @overload
    def __init__(
        self,
        __config_dict: MatrixDict,
        /,
    ) -> None: ...

    @overload
    def __init__(
        self,
        /,
        **kwargs: Unpack[MatrixDict],
    ) -> None: ...

    def __init__(
        self,
        __config_dict: MatrixDict | None = None,
        /,
        **kwargs: Any,
    ) -> None:
        if __config_dict is not None:
            super().__init__(**__config_dict)
        else:
            super().__init__(**kwargs)


class FlowOptions(BaseModel, extra="forbid"):
    log_dir: str = Field(
        description="Output path for logging results (required to ensure that a unique storage scope is assigned for the set)."
    )


class EvalSetOptions(BaseModel, extra="forbid"):
    retry_attempts: int | None = Field(
        default=None,
        description="Maximum number of retry attempts before giving up (defaults to 10).",
    )

    retry_wait: float | None = Field(
        default=None,
        description="Time to wait between attempts, increased exponentially (defaults to 30, resulting in waits of 30, 60, 120, 240, etc.). Wait time per-retry will in no case be longer than 1 hour.",
    )

    retry_connections: float | None = Field(
        default=None,
        description="Reduce max_connections at this rate with each retry (defaults to 1.0, which results in no reduction).",
    )

    retry_cleanup: bool | None = Field(
        default=None,
        description="Cleanup failed log files after retries (defaults to True).",
    )

    sandbox: SandboxEnvironmentType | None = Field(
        default=None,
        description="Sandbox environment type (or optionally a str or tuple with a shorthand spec)",
    )

    sandbox_cleanup: bool | None = Field(
        default=None,
        description="Cleanup sandbox environments after task completes (defaults to True)",
    )

    tags: list[str] | None = Field(
        default=None, description="Tags to associate with this evaluation run."
    )

    metadata: dict[str, Any] | None = Field(
        default=None, description="Metadata to associate with this evaluation run."
    )

    trace: bool | None = Field(
        default=None,
        description="Trace message interactions with evaluated model to terminal.",
    )

    display: DisplayType | None = Field(
        default=None, description="Task display type (defaults to 'full')."
    )

    approval: str | ApprovalPolicyConfig | None = Field(
        default=None,
        description="Tool use approval policies. Either a path to an approval policy config file or a list of approval policies. Defaults to no approval policy.",
    )

    score: bool = Field(default=True, description="Score output (defaults to True)")

    log_level: str | None = Field(
        default=None,
        description='Level for logging to the console: "debug", "http", "sandbox", "info", "warning", "error", "critical", or "notset" (defaults to "warning")',
    )

    log_level_transcript: str | None = Field(
        default=None,
        description='Level for logging to the log file (defaults to "info")',
    )

    log_format: Literal["eval", "json"] | None = Field(
        default=None,
        description='Format for writing log files (defaults to "eval", the native high-performance format).',
    )

    limit: int | None = Field(
        default=None, description="Limit evaluated samples (defaults to all samples)."
    )

    sample_shuffle: bool | int | None = Field(
        default=None,
        description="Shuffle order of samples (pass a seed to make the order deterministic).",
    )

    fail_on_error: bool | float | None = Field(
        default=None,
        description="`True` to fail on first sample error(default); `False` to never fail on sample errors; Value between 0 and 1 to fail if a proportion of total samples fails. Value greater than 1 to fail eval if a count of samples fails.",
    )

    continue_on_fail: bool | None = Field(
        default=None,
        description="`True` to continue running and only fail at the end if the `fail_on_error` condition is met. `False` to fail eval immediately when the `fail_on_error` condition is met (default).",
    )

    retry_on_error: int | None = Field(
        default=None,
        description="Number of times to retry samples if they encounter errors (by default, no retries occur).",
    )

    debug_errors: bool | None = Field(
        default=None,
        description="Raise task errors (rather than logging them) so they can be debugged (defaults to False).",
    )

    max_samples: int | None = Field(
        default=None,
        description="Maximum number of samples to run in parallel (default is max_connections)",
    )

    max_tasks: int | None = Field(
        default=None,
        description="Maximum number of tasks to run in parallel(defaults to the greater of 4 and the number of models being evaluated)",
    )

    max_subprocesses: int | None = Field(
        default=None,
        description="Maximum number of subprocesses to run in parallel (default is os.cpu_count())",
    )

    max_sandboxes: int | None = Field(
        default=None,
        description="Maximum number of sandboxes (per-provider) to run in parallel.",
    )

    log_samples: bool | None = Field(
        default=None, description="Log detailed samples and scores (defaults to True)"
    )

    log_realtime: bool | None = Field(
        default=None,
        description="Log events in realtime (enables live viewing of samples in inspect view). Defaults to True.",
    )

    log_images: bool | None = Field(
        default=None,
        description="Log base64 encoded version of images, even if specified as a filename or URL (defaults to False)",
    )

    log_buffer: int | None = Field(
        default=None,
        description="Number of samples to buffer before writing log file. If not specified, an appropriate default for the format and filesystem is chosen (10 for most all cases, 100 for JSON logs on remote filesystems).",
    )

    log_shared: bool | None = Field(
        default=None,
        description="Sync sample events to log directory so that users on other systems can see log updates in realtime (defaults to no syncing). Specify `True` to sync every 10 seconds, otherwise an integer to sync every `n` seconds.",
    )

    log_dir_allow_dirty: bool | None = Field(
        default=None,
        description="If True, allow the log directory to contain unrelated logs. If False, ensure that the log directory only contains logs for tasks in this eval set (defaults to False).",
    )

    config: GenerateConfig | None = Field(
        default=None,
        description="Model generation options. Will override settings on the ModelConfig and TaskConfig.",
    )


class FlowConfig(BaseModel, extra="forbid"):
    options: FlowOptions | None = Field(
        default=None, description="Global options for flow"
    )

    eval_set_options: EvalSetOptions | None = Field(
        default=None, description="Arguments for calls to eval_set."
    )

    dependencies: list[str] | None = Field(
        # TODO:ransom support requirements.txt/pyproj.toml for specifying dependencies
        default=None,
        description="Dependencies to pip install. E.g. PyPI package specifiers or Git repository URLs.",
    )

    matrix: list[Matrix] = Field(description="Matrix of tasks to run")

    # Convert single items to lists
    @field_validator("dependencies", "matrix", mode="before")
    @classmethod
    def convert_to_list(cls, v):
        return ensure_list_or_none(v)

    @overload
    def __init__(
        self,
        __config_dict: FlowConfigDict,
        /,
    ) -> None: ...

    @overload
    def __init__(
        self,
        /,
        **kwargs: Unpack[FlowConfigDict],
    ) -> None: ...

    def __init__(
        self,
        __config_dict: FlowConfigDict | None = None,
        /,
        **kwargs: Any,
    ) -> None:
        """Initialize FlowConfig from either a FlowConfigDict or keyword arguments."""
        if __config_dict is not None:
            super().__init__(**__config_dict)
        else:
            super().__init__(**kwargs)


T = TypeVar("T", TaskConfig, ModelConfig, SolverConfig, AgentConfig)


@overload
def convert_to_class_list(cls: type[T], v: None) -> None: ...


@overload
def convert_to_class_list(cls: type[T], v: str | T | list[str | T]) -> list[T]: ...


def convert_to_class_list(
    cls: type[T], v: str | T | list[str | T] | None
) -> list[T] | None:
    if v is None:
        return v
    if not isinstance(v, list):
        v = [v]
    return [cls(name=task) if isinstance(task, str) else task for task in v]


def convert_to_solver_config(
    v: str | SolverConfig | list[str | SolverConfig],
) -> SolverConfig | list[SolverConfig]:
    if isinstance(v, str):
        return SolverConfig(name=v)
    if isinstance(v, list):
        return convert_to_class_list(SolverConfig, v)
    return v


# SolverConfig supports chained solvers specified as nested lists
def convert_to_solver_config_list(
    v: str | SolverConfig | list[str | SolverConfig | list[str | SolverConfig]] | None,
) -> list[SolverConfig | list[SolverConfig]] | None:
    if v is None:
        return v
    if not isinstance(v, list):
        v = [v]
    return [convert_to_solver_config(task) for task in v]
